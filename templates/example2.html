<html><head></head>
    <title>dehazing | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of papers for dehazing category on arXiv, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.4.9.1">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">@font-face {
  font-family: "xm-iconfont";
  src: url('//at.alicdn.com/t/font_792691_ptvyboo0bno.eot?t=1574048839056');
  /* IE9 */
  src: url('//at.alicdn.com/t/font_792691_ptvyboo0bno.eot?t=1574048839056#iefix') format('embedded-opentype'), /* IE6-IE8 */ url('data:application/x-font-woff2;charset=utf-8;base64,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') format('woff2'), url('//at.alicdn.com/t/font_792691_ptvyboo0bno.woff?t=1574048839056') format('woff'), url('//at.alicdn.com/t/font_792691_ptvyboo0bno.ttf?t=1574048839056') format('truetype'), /* chrome, firefox, opera, Safari, Android, iOS 4.2+ */ url('//at.alicdn.com/t/font_792691_ptvyboo0bno.svg?t=1574048839056#iconfont') format('svg');
  /* iOS 4.1- */
}
.xm-iconfont {
  font-family: "xm-iconfont" !important;
  font-size: 16px;
  font-style: normal;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.xm-icon-quanxuan:before {
  content: "\e62c";
}
.xm-icon-caidan:before {
  content: "\e610";
}
.xm-icon-fanxuan:before {
  content: "\e837";
}
.xm-icon-pifu:before {
  content: "\e668";
}
.xm-icon-qingkong:before {
  content: "\e63e";
}
.xm-icon-sousuo:before {
  content: "\e600";
}
.xm-icon-danx:before {
  content: "\e62b";
}
.xm-icon-duox:before {
  content: "\e613";
}
.xm-icon-close:before {
  content: "\e601";
}
.xm-icon-expand:before {
  content: "\e641";
}
.xm-icon-banxuan:before {
  content: "\e60d";
}
</style><style type="text/css">@-webkit-keyframes xm-upbit {
  from {
    -webkit-transform: translate3d(0, 30px, 0);
    opacity: 0.3;
  }
  to {
    -webkit-transform: translate3d(0, 0, 0);
    opacity: 1;
  }
}
@keyframes xm-upbit {
  from {
    transform: translate3d(0, 30px, 0);
    opacity: 0.3;
  }
  to {
    transform: translate3d(0, 0, 0);
    opacity: 1;
  }
}
@-webkit-keyframes loader {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}
@keyframes loader {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}
xm-select {
  background-color: #FFF;
  position: relative;
  border: 1px solid #E6E6E6;
  border-radius: 2px;
  display: block;
  width: 100%;
  cursor: pointer;
  outline: none;
}
xm-select * {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
  font-size: 14px;
  font-weight: 400;
  text-overflow: ellipsis;
  user-select: none;
  -ms-user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
}
xm-select:hover,
xm-select:focus {
  border-color: #C0C4CC;
}
xm-select > .xm-tips {
  color: #999999;
  padding: 0 10px;
  position: absolute;
  display: flex;
  height: 100%;
  align-items: center;
}
xm-select > .xm-icon {
  display: inline-block;
  overflow: hidden;
  position: absolute;
  width: 0;
  height: 0;
  right: 10px;
  top: 50%;
  margin-top: -3px;
  cursor: pointer;
  border: 6px dashed transparent;
  border-top-color: #C2C2C2;
  border-top-style: solid;
  transition: all 0.3s;
  -webkit-transition: all 0.3s;
}
xm-select > .xm-icon-expand {
  margin-top: -9px;
  transform: rotate(180deg);
}
xm-select > .xm-label.single-row {
  position: absolute;
  top: 0;
  bottom: 0px;
  left: 0px;
  right: 30px;
  overflow: auto hidden;
}
xm-select > .xm-label.single-row .scroll {
  overflow-y: hidden;
}
xm-select > .xm-label.single-row .label-content {
  flex-wrap: nowrap;
  white-space: nowrap;
}
xm-select > .xm-label.auto-row .label-content {
  flex-wrap: wrap;
  padding-right: 30px !important;
}
xm-select > .xm-label.auto-row .xm-label-block > span {
  white-space: unset;
  height: 100%;
}
xm-select > .xm-label .scroll .label-content {
  display: flex;
  padding: 3px 10px;
}
xm-select > .xm-label .xm-label-block {
  display: flex;
  position: relative;
  padding: 0px 5px;
  margin: 2px 5px 2px 0;
  border-radius: 3px;
  align-items: baseline;
  color: #FFF;
}
xm-select > .xm-label .xm-label-block > span {
  display: flex;
  color: #FFF;
  white-space: nowrap;
}
xm-select > .xm-label .xm-label-block > i {
  color: #FFF;
  margin-left: 8px;
  font-size: 12px;
  cursor: pointer;
  display: flex;
}
xm-select > .xm-label .xm-label-block.disabled {
  background-color: #C2C2C2 !important;
  cursor: no-drop !important;
}
xm-select > .xm-label .xm-label-block.disabled > i {
  cursor: no-drop !important;
}
xm-select > .xm-body {
  position: absolute;
  left: 0;
  top: 42px;
  padding: 5px 0;
  z-index: 999;
  width: 100%;
  min-width: fit-content;
  border: 1px solid #E6E6E6;
  background-color: #fff;
  border-radius: 2px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.12);
  animation-name: xm-upbit;
  animation-duration: 0.3s;
  animation-fill-mode: both;
}
xm-select > .xm-body .scroll-body {
  overflow-x: hidden;
  overflow-y: auto;
}
xm-select > .xm-body .scroll-body::-webkit-scrollbar {
  width: 8px;
}
xm-select > .xm-body .scroll-body::-webkit-scrollbar-track {
  -webkit-border-radius: 2em;
  -moz-border-radius: 2em;
  -ms-border-radius: 2em;
  border-radius: 2em;
  background-color: #FFF;
}
xm-select > .xm-body .scroll-body::-webkit-scrollbar-thumb {
  -webkit-border-radius: 2em;
  -moz-border-radius: 2em;
  -ms-border-radius: 2em;
  border-radius: 2em;
  background-color: #C2C2C2;
}
xm-select > .xm-body.up {
  top: auto;
  bottom: 42px;
}
xm-select > .xm-body.relative {
  position: relative;
  display: block !important;
  top: 0;
  box-shadow: none;
  border: none;
  animation-name: none;
  animation-duration: 0;
  min-width: 100%;
}
xm-select > .xm-body .xm-group {
  cursor: default;
}
xm-select > .xm-body .xm-group-item {
  display: inline-block;
  cursor: pointer;
  padding: 0 10px;
  color: #999;
  font-size: 12px;
}
xm-select > .xm-body .xm-option {
  display: flex;
  align-items: center;
  position: relative;
  padding: 0 10px;
  cursor: pointer;
}
xm-select > .xm-body .xm-option-icon {
  color: transparent;
  display: flex;
  border: 1px solid #E6E6E6;
  border-radius: 3px;
  justify-content: center;
  align-items: center;
}
xm-select > .xm-body .xm-option-icon.xm-custom-icon {
  color: unset;
  border: unset;
}
xm-select > .xm-body .xm-option-icon-hidden {
  margin-right: -10px;
}
xm-select > .xm-body .xm-option-icon.xm-icon-danx {
  border-radius: 100%;
}
xm-select > .xm-body .xm-option-content {
  display: flex;
  position: relative;
  padding-left: 15px;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
  ;
  width: calc(100% - 20px);
}
xm-select > .xm-body .xm-option.hide-icon .xm-option-content {
  padding-left: 0;
}
xm-select > .xm-body .xm-option.selected.hide-icon .xm-option-content {
  color: #FFF !important;
}
xm-select > .xm-body .xm-option .loader {
  width: 0.8em;
  height: 0.8em;
  margin-right: 6px;
  color: #C2C2C2;
}
xm-select > .xm-body .xm-select-empty {
  text-align: center;
  color: #999;
}
xm-select > .xm-body .disabled {
  cursor: no-drop;
}
xm-select > .xm-body .disabled:hover {
  background-color: #FFF;
}
xm-select > .xm-body .disabled .xm-option-icon {
  border-color: #C2C2C2 !important;
}
xm-select > .xm-body .disabled .xm-option-content {
  color: #C2C2C2 !important;
}
xm-select > .xm-body .disabled.selected > .xm-option-icon {
  color: #C2C2C2 !important;
}
xm-select > .xm-body .xm-search {
  background-color: #FFF !important;
  position: relative;
  padding: 0 10px;
  margin-bottom: 5px;
  cursor: pointer;
}
xm-select > .xm-body .xm-search > i {
  position: absolute;
  color: ;
}
xm-select > .xm-body .xm-search-input {
  border: none;
  border-bottom: 1px solid #E6E6E6;
  padding-left: 27px;
  cursor: text;
}
xm-select > .xm-body .xm-paging {
  padding: 0 10px;
  display: flex;
  margin-top: 5px;
}
xm-select > .xm-body .xm-paging > span:first-child {
  border-radius: 2px 0 0 2px;
}
xm-select > .xm-body .xm-paging > span:last-child {
  border-radius: 0 2px 2px 0;
}
xm-select > .xm-body .xm-paging > span {
  display: flex;
  flex: auto;
  justify-content: center;
  vertical-align: middle;
  margin: 0 -1px 0 0;
  background-color: #fff;
  color: #333;
  font-size: 12px;
  border: 1px solid #e2e2e2;
  flex-wrap: nowrap;
  width: 100%;
  overflow: hidden;
  min-width: 50px;
}
xm-select > .xm-body .xm-toolbar {
  padding: 0 10px;
  display: flex;
  margin: -3px 0;
  cursor: default;
}
xm-select > .xm-body .xm-toolbar .toolbar-tag {
  cursor: pointer;
  display: flex;
  margin-right: 20px;
  color: ;
  align-items: baseline;
}
xm-select > .xm-body .xm-toolbar .toolbar-tag:hover {
  opacity: 0.8;
}
xm-select > .xm-body .xm-toolbar .toolbar-tag:active {
  opacity: 1;
}
xm-select > .xm-body .xm-toolbar .toolbar-tag > i {
  margin-right: 2px;
  font-size: 14px;
}
xm-select > .xm-body .xm-toolbar .toolbar-tag:last-child {
  margin-right: 0;
}
xm-select > .xm-body .xm-body-custom {
  line-height: initial;
  cursor: default;
}
xm-select > .xm-body .xm-body-custom * {
  box-sizing: initial;
}
xm-select > .xm-body .xm-tree {
  position: relative;
}
xm-select > .xm-body .xm-tree-icon {
  display: inline-block;
  margin-right: 3px;
  cursor: pointer;
  border: 6px dashed transparent;
  border-left-color: #C2C2C2;
  border-left-style: solid;
  transition: all 0.3s;
  -webkit-transition: all 0.3s;
  z-index: 2;
  visibility: hidden;
}
xm-select > .xm-body .xm-tree-icon.expand {
  margin-top: 3px;
  margin-right: 5px;
  margin-left: -2px;
  transform: rotate(90deg);
}
xm-select > .xm-body .xm-tree-icon.xm-visible {
  visibility: visible;
}
xm-select > .xm-body .xm-tree .left-line {
  position: absolute;
  left: 13px;
  width: 0;
  z-index: 1;
  border-left: 1px dotted #c0c4cc !important;
}
xm-select > .xm-body .xm-tree .top-line {
  position: absolute;
  left: 13px;
  height: 0;
  z-index: 1;
  border-top: 1px dotted #c0c4cc !important;
}
xm-select > .xm-body .xm-tree .xm-tree-icon + .top-line {
  margin-left: 1px;
}
xm-select > .xm-body .scroll-body > .xm-tree > .xm-option > .top-line,
xm-select > .xm-body .scroll-body > .xm-option > .top-line {
  width: 0 !important;
}
xm-select > .xm-body .xm-cascader-box {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  padding: 5px 0;
  border: 1px solid #E6E6E6;
  background-color: #fff;
  border-radius: 2px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.12);
  margin: -1px;
}
xm-select > .xm-body .xm-cascader-box::before {
  content: ' ';
  position: absolute;
  width: 0;
  height: 0;
  border: 6px solid transparent;
  border-right-color: #E6E6E6;
  top: 10px;
  left: -12px;
}
xm-select > .xm-body .xm-cascader-box::after {
  content: ' ';
  position: absolute;
  width: 0;
  height: 0;
  border: 6px solid transparent;
  border-right-color: #fff;
  top: 10px;
  left: -11px;
}
xm-select > .xm-body .xm-cascader-scroll {
  height: 100%;
  overflow-x: hidden;
  overflow-y: auto;
}
xm-select > .xm-body.cascader {
  width: unset;
  min-width: unset;
}
xm-select > .xm-body.cascader .xm-option-content {
  padding-left: 8px;
}
xm-select > .xm-body.cascader .disabled .xm-right-arrow {
  color: #C2C2C2 !important;
}
xm-select > .xm-body.cascader .hide-icon.disabled .xm-right-arrow {
  color: #999 !important;
}
xm-select .xm-input {
  cursor: pointer;
  border-radius: 2px;
  border-width: 1px;
  border-style: solid;
  border-color: #E6E6E6;
  display: block;
  width: 100%;
  box-sizing: border-box;
  background-color: #FFF;
  line-height: 1.3;
  padding-left: 10px;
  outline: 0;
  user-select: text;
  -ms-user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
}
xm-select .dis {
  display: none;
}
xm-select .loading {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(255, 255, 255, 0.6);
  display: flex;
  align-items: center;
  justify-content: center;
}
xm-select .loader {
  border: 0.2em dotted currentcolor;
  border-radius: 50%;
  -webkit-animation: 1s loader linear infinite;
  animation: 1s loader linear infinite;
  display: inline-block;
  width: 1em;
  height: 1em;
  color: inherit;
  vertical-align: middle;
  pointer-events: none;
}
xm-select .xm-select-default {
  position: absolute;
  width: 100%;
  height: 100%;
  border: none;
  visibility: hidden;
}
xm-select .xm-select-disabled {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  cursor: no-drop;
  z-index: 2;
  opacity: 0.3;
  background-color: #FFF;
}
xm-select .item--divided {
  border-top: 1px solid #ebeef5;
  width: calc(100% - 20px);
  cursor: initial;
}
xm-select .xm-right-arrow {
  position: absolute;
  color: ;
  right: 5px;
  top: -1px;
  font-weight: 700;
  transform: scale(0.6, 1);
}
xm-select .xm-right-arrow::after {
  content: '>';
}
xm-select[size='large'] {
  min-height: 40px;
  line-height: 40px;
}
xm-select[size='large'] .xm-input {
  height: 40px;
}
xm-select[size='large'] .xm-label .scroll .label-content {
  line-height: 34px;
}
xm-select[size='large'] .xm-label .xm-label-block {
  height: 30px;
  line-height: 30px;
}
xm-select[size='large'] .xm-body .xm-option .xm-option-icon {
  height: 20px;
  width: 20px;
  font-size: 20px;
}
xm-select[size='large'] .xm-paging > span {
  height: 34px;
  line-height: 34px;
}
xm-select[size='large'] .xm-tree .left-line {
  height: 100%;
  bottom: 20px;
}
xm-select[size='large'] .xm-tree .left-line-group {
  height: calc(100% - 40px);
}
xm-select[size='large'] .xm-tree .xm-tree-icon.xm-hidden + .top-line {
  top: 19px;
}
xm-select[size='large'] .item--divided {
  margin: 10px;
}
xm-select {
  min-height: 36px;
  line-height: 36px;
}
xm-select .xm-input {
  height: 36px;
}
xm-select .xm-label .scroll .label-content {
  line-height: 30px;
}
xm-select .xm-label .xm-label-block {
  height: 26px;
  line-height: 26px;
}
xm-select .xm-body .xm-option .xm-option-icon {
  height: 18px;
  width: 18px;
  font-size: 18px;
}
xm-select .xm-paging > span {
  height: 30px;
  line-height: 30px;
}
xm-select .xm-tree .left-line {
  height: 100%;
  bottom: 18px;
}
xm-select .xm-tree .left-line-group {
  height: calc(100% - 36px);
}
xm-select .xm-tree .xm-tree-icon.xm-hidden + .top-line {
  top: 17px;
}
xm-select .item--divided {
  margin: 9px;
}
xm-select[size='small'] {
  min-height: 32px;
  line-height: 32px;
}
xm-select[size='small'] .xm-input {
  height: 32px;
}
xm-select[size='small'] .xm-label .scroll .label-content {
  line-height: 26px;
}
xm-select[size='small'] .xm-label .xm-label-block {
  height: 22px;
  line-height: 22px;
}
xm-select[size='small'] .xm-body .xm-option .xm-option-icon {
  height: 16px;
  width: 16px;
  font-size: 16px;
}
xm-select[size='small'] .xm-paging > span {
  height: 26px;
  line-height: 26px;
}
xm-select[size='small'] .xm-tree .left-line {
  height: 100%;
  bottom: 16px;
}
xm-select[size='small'] .xm-tree .left-line-group {
  height: calc(100% - 32px);
}
xm-select[size='small'] .xm-tree .xm-tree-icon.xm-hidden + .top-line {
  top: 15px;
}
xm-select[size='small'] .item--divided {
  margin: 8px;
}
xm-select[size='mini'] {
  min-height: 28px;
  line-height: 28px;
}
xm-select[size='mini'] .xm-input {
  height: 28px;
}
xm-select[size='mini'] .xm-label .scroll .label-content {
  line-height: 22px;
}
xm-select[size='mini'] .xm-label .xm-label-block {
  height: 18px;
  line-height: 18px;
}
xm-select[size='mini'] .xm-body .xm-option .xm-option-icon {
  height: 14px;
  width: 14px;
  font-size: 14px;
}
xm-select[size='mini'] .xm-paging > span {
  height: 22px;
  line-height: 22px;
}
xm-select[size='mini'] .xm-tree .left-line {
  height: 100%;
  bottom: 14px;
}
xm-select[size='mini'] .xm-tree .left-line-group {
  height: calc(100% - 28px);
}
xm-select[size='mini'] .xm-tree .xm-tree-icon.xm-hidden + .top-line {
  top: 13px;
}
xm-select[size='mini'] .item--divided {
  margin: 7px;
}
.layui-form-pane xm-select {
  margin: -1px -1px -1px 0;
}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype'); font-weight: bold}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype'); font-style: italic}
@font-face {font-family: MathJax_Math; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype'); font-style: italic}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body id="arxiv"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <div class="search-container-top notranslate">
        <div class="search-main">
            <i class="fa fa-search search-icon"></i>
            <input id="search-query" class="search-box search-page-box" type="text" placeholder="Cool Papers" value="dehazing" onkeyup="paperSearch(event)">
            <button class="search-query-btn" onclick="paperSearch()">Go</button>
        </div>
    </div>
    <h1 class="notranslate" style="margin-top:80px">dehazing</h1>
    <p class="info notranslate">
        <span class="sort-it" title="sort by published time" onclick="paperSort('date')" style="color: gold;"><i class="fa fa-clock-o"></i></span>
        <span class="sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span> |
        Total: 298
    </p>
    <div class="papers">
        <div id="2410.16095" class="panel paper" keywords="haze,dehazing,lmhaze,intensity,dataset,intensities,hazy,images,real,mamba">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.16095" target="_blank" title="1/298"><span class="index notranslate">#1</span></a>
                <a id="title-2410.16095" class="title-link" href="/arxiv/2410.16095" target="_blank">LMHaze: Intensity-aware Image <mark data-markjs="true">Dehazing</mark> with a Large-scale Multi-intensity Real Haze Dataset</a>
                <a id="pdf-2410.16095" class="title-pdf notranslate" onclick="togglePdf('2410.16095', 'https://arxiv.org/pdf/2410.16095', this)">[PDF<sup id="pdf-stars-2410.16095">1</sup>]</a>
                <a id="copy-2410.16095" class="title-copy notranslate" onclick="copyToClipboard('2410.16095')">[Copy]</a>
                <a id="kimi-2410.16095" class="title-kimi notranslate" onclick="toggleKimi('2410.16095', this)">[Kimi<sup id="kimi-stars-2410.16095"></sup>]</a>
                <a id="rel-2410.16095" class="title-rel notranslate" onclick="openRelatedPapers('2410.16095')">[REL]</a>
            </h2>
            <p id="authors-2410.16095" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruikun Zhang" target="_blank"><span class="author notranslate">Ruikun Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao Yang" target="_blank"><span class="author notranslate">Hao Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yan Yang" target="_blank"><span class="author notranslate">Yan Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ying Fu" target="_blank"><span class="author notranslate">Ying Fu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liyuan Pan" target="_blank"><span class="author notranslate">Liyuan Pan</span></a>
            </p>
            <p id="summary-2410.16095" class="summary">Image <mark data-markjs="true">dehazing</mark> has drawn a significant attention in recent years. Learning-based methods usually require paired hazy and corresponding ground truth (haze-free) images for training. However, it is difficult to collect real-world image pairs, which prevents developments of existing methods. Although several works partially alleviate this issue by using synthetic datasets or small-scale real datasets. The haze intensity distribution bias and scene homogeneity in existing datasets limit the generalization ability of these methods, particularly when encountering images with previously unseen haze intensities. In this work, we present LMHaze, a large-scale, high-quality real-world dataset. LMHaze comprises paired hazy and haze-free images captured in diverse indoor and outdoor environments, spanning multiple scenarios and haze intensities. It contains over 5K high-resolution image pairs, surpassing the size of the biggest existing real-world <mark data-markjs="true">dehazing</mark> dataset by over 25 times. Meanwhile, to better handle images with different haze intensities, we propose a mixture-of-experts model based on Mamba (MoE-Mamba) for <mark data-markjs="true">dehazing</mark>, which dynamically adjusts the model parameters according to the haze intensity. Moreover, with our proposed dataset, we conduct a new large multimodal model (LMM)-based benchmark study to simulate human perception for evaluating dehazed images. Experiments demonstrate that LMHaze dataset improves the <mark data-markjs="true">dehazing</mark> performance in real scenarios and our <mark data-markjs="true">dehazing</mark> method provides better results compared to state-of-the-art methods.</p>
            <p id="subjects-2410.16095" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2410.16095" class="metainfo date"><strong>Publish</strong>: 2024-10-21 15:20:02 UTC</p>
            <div id="pdf-container-2410.16095" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.16095" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.16095" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2410.14595" class="panel paper" keywords="dehazing,dehazenet,haze,dehazed,recovery,contrastive,detail,draco,paradigm,image">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.14595" target="_blank" title="2/298"><span class="index notranslate">#2</span></a>
                <a id="title-2410.14595" class="title-link" href="/arxiv/2410.14595" target="_blank">DRACO-DehazeNet: An Efficient Image <mark data-markjs="true">Dehazing</mark> Network Combining Detail Recovery and a Novel Contrastive Learning Paradigm</a>
                <a id="pdf-2410.14595" class="title-pdf notranslate" onclick="togglePdf('2410.14595', 'https://arxiv.org/pdf/2410.14595', this)">[PDF<sup id="pdf-stars-2410.14595">2</sup>]</a>
                <a id="copy-2410.14595" class="title-copy notranslate" onclick="copyToClipboard('2410.14595')">[Copy]</a>
                <a id="kimi-2410.14595" class="title-kimi notranslate" onclick="toggleKimi('2410.14595', this)">[Kimi<sup id="kimi-stars-2410.14595"></sup>]</a>
                <a id="rel-2410.14595" class="title-rel notranslate" onclick="openRelatedPapers('2410.14595')">[REL]</a>
            </h2>
            <p id="authors-2410.14595" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gao Yu Lee" target="_blank"><span class="author notranslate">Gao Yu Lee</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tanmoy Dam" target="_blank"><span class="author notranslate">Tanmoy Dam</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Md Meftahul Ferdaus" target="_blank"><span class="author notranslate">Md Meftahul Ferdaus</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel Puiu Poenar" target="_blank"><span class="author notranslate">Daniel Puiu Poenar</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vu Duong" target="_blank"><span class="author notranslate">Vu Duong</span></a>
            </p>
            <p id="summary-2410.14595" class="summary">Image <mark data-markjs="true">dehazing</mark> is crucial for clarifying images obscured by haze or fog, but current learning-based approaches is dependent on large volumes of training data and hence consumed significant computational power. Additionally, their performance is often inadequate under non-uniform or heavy haze. To address these challenges, we developed the Detail Recovery And Contrastive DehazeNet, which facilitates efficient and effective <mark data-markjs="true">dehazing</mark> via a dense dilated inverted residual block and an attention-based detail recovery network that tailors enhancements to specific dehazed scene contexts. A major innovation is its ability to train effectively with limited data, achieved through a novel quadruplet loss-based contrastive <mark data-markjs="true">dehazing</mark> paradigm. This approach distinctly separates hazy and clear image features while also distinguish lower-quality and higher-quality dehazed images obtained from each sub-modules of our network, thereby refining the <mark data-markjs="true">dehazing</mark> process to a larger extent. Extensive tests on a variety of benchmarked haze datasets demonstrated the superiority of our approach. The code repository for this work will be available soon.</p>
            <p id="subjects-2410.14595" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2410.14595" class="metainfo date"><strong>Publish</strong>: 2024-10-18 16:48:31 UTC</p>
            <div id="pdf-container-2410.14595" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.14595" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.14595" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2410.10121" class="panel paper" keywords="dehazing,branch,global,feecuin,image,cnns,transformers,feature,positions,hazy">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.10121" target="_blank" title="3/298"><span class="index notranslate">#3</span></a>
                <a id="title-2410.10121" class="title-link" href="/arxiv/2410.10121" target="_blank">Interaction-Guided Two-Branch Image <mark data-markjs="true">Dehazing</mark> Network</a>
                <a id="pdf-2410.10121" class="title-pdf notranslate" onclick="togglePdf('2410.10121', 'https://arxiv.org/pdf/2410.10121', this)" style="color: purple;">[PDF<sup id="pdf-stars-2410.10121">3</sup>]</a>
                <a id="copy-2410.10121" class="title-copy notranslate" onclick="copyToClipboard('2410.10121')">[Copy]</a>
                <a id="kimi-2410.10121" class="title-kimi notranslate" onclick="toggleKimi('2410.10121', this)">[Kimi<sup id="kimi-stars-2410.10121">1</sup>]</a>
                <a id="rel-2410.10121" class="title-rel notranslate" onclick="openRelatedPapers('2410.10121')">[REL]</a>
            </h2>
            <p id="authors-2410.10121" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huichun Liu" target="_blank"><span class="author notranslate">Huichun Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaosong Li" target="_blank"><span class="author notranslate">Xiaosong Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tianshu Tan" target="_blank"><span class="author notranslate">Tianshu Tan</span></a>
            </p>
            <p id="summary-2410.10121" class="summary">Image <mark data-markjs="true">dehazing</mark> aims to restore clean images from hazy ones. Convolutional Neural Networks (CNNs) and Transformers have demonstrated exceptional performance in local and global feature extraction, respectively, and currently represent the two mainstream frameworks in image <mark data-markjs="true">dehazing</mark>. In this paper, we propose a novel dual-branch image <mark data-markjs="true">dehazing</mark> framework that guides CNN and Transformer components interactively. We reconsider the complementary characteristics of CNNs and Transformers by leveraging the differential relationships between global and local features for interactive guidance. This approach enables the capture of local feature positions through global attention maps, allowing the CNN to focus solely on feature information at effective positions. The single-branch Transformer design ensures the network's global information recovery capability. Extensive experiments demonstrate that our proposed method yields competitive qualitative and quantitative evaluation performance on both synthetic and real public datasets. Codes are available at https://github.com/Feecuin/Two-Branch-<mark data-markjs="true">Dehazing</mark></p>
            <p id="subjects-2410.10121" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2410.10121" class="metainfo date"><strong>Publish</strong>: 2024-10-14 03:21:56 UTC</p>
            <div id="pdf-container-2410.10121" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.10121" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.10121" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2410.04762" class="panel paper" keywords="dehazing,wtcl,dehaze,contrastive,hazy,dwt,wavelet,image,world,transform">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.04762" target="_blank" title="4/298"><span class="index notranslate">#4</span></a>
                <a id="title-2410.04762" class="title-link" href="/arxiv/2410.04762" target="_blank">WTCL-Dehaze: Rethinking Real-world Image <mark data-markjs="true">Dehazing</mark> via Wavelet Transform and Contrastive Learning</a>
                <a id="pdf-2410.04762" class="title-pdf notranslate" onclick="togglePdf('2410.04762', 'https://arxiv.org/pdf/2410.04762', this)" style="color: purple;">[PDF<sup id="pdf-stars-2410.04762">3</sup>]</a>
                <a id="copy-2410.04762" class="title-copy notranslate" onclick="copyToClipboard('2410.04762')">[Copy]</a>
                <a id="kimi-2410.04762" class="title-kimi notranslate" onclick="toggleKimi('2410.04762', this)">[Kimi<sup id="kimi-stars-2410.04762">1</sup>]</a>
                <a id="rel-2410.04762" class="title-rel notranslate" onclick="openRelatedPapers('2410.04762')">[REL]</a>
            </h2>
            <p id="authors-2410.04762" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Divine Joseph Appiah" target="_blank"><span class="author notranslate">Divine Joseph Appiah</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Donghai Guan" target="_blank"><span class="author notranslate">Donghai Guan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Abdul Nasser Kasule" target="_blank"><span class="author notranslate">Abdul Nasser Kasule</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingqiang Wei" target="_blank"><span class="author notranslate">Mingqiang Wei</span></a>
            </p>
            <p id="summary-2410.04762" class="summary">Images captured in hazy outdoor conditions often suffer from colour distortion, low contrast, and loss of detail, which impair high-level vision tasks. Single image <mark data-markjs="true">dehazing</mark> is essential for applications such as autonomous driving and surveillance, with the aim of restoring image clarity. In this work, we propose WTCL-Dehaze an enhanced semi-supervised <mark data-markjs="true">dehazing</mark> network that integrates Contrastive Loss and Discrete Wavelet Transform (DWT). We incorporate contrastive regularization to enhance feature representation by contrasting hazy and clear image pairs. Additionally, we utilize DWT for multi-scale feature extraction, effectively capturing high-frequency details and global structures. Our approach leverages both labelled and unlabelled data to mitigate the domain gap and improve generalization. The model is trained on a combination of synthetic and real-world datasets, ensuring robust performance across different scenarios. Extensive experiments demonstrate that our proposed algorithm achieves superior performance and improved robustness compared to state-of-the-art single image <mark data-markjs="true">dehazing</mark> methods on both benchmark datasets and real-world images.</p>
            <p id="subjects-2410.04762" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2410.04762" class="metainfo date"><strong>Publish</strong>: 2024-10-07 05:36:11 UTC</p>
            <div id="pdf-container-2410.04762" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.04762" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.04762" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2410.01395" class="panel paper" keywords="urological,dehaze,surgical,rsf,surgery,robot,dehazing,atomization,vision,dataset">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.01395" target="_blank" title="5/298"><span class="index notranslate">#5</span></a>
                <a id="title-2410.01395" class="title-link" href="/arxiv/2410.01395" target="_blank">Toward Zero-Shot Learning for Visual <mark data-markjs="true">Dehazing</mark> of Urological Surgical Robots</a>
                <a id="pdf-2410.01395" class="title-pdf notranslate" onclick="togglePdf('2410.01395', 'https://arxiv.org/pdf/2410.01395', this)" style="color: purple;">[PDF<sup id="pdf-stars-2410.01395">1</sup>]</a>
                <a id="copy-2410.01395" class="title-copy notranslate" onclick="copyToClipboard('2410.01395')">[Copy]</a>
                <a id="kimi-2410.01395" class="title-kimi notranslate" onclick="toggleKimi('2410.01395', this)">[Kimi<sup id="kimi-stars-2410.01395"></sup>]</a>
                <a id="rel-2410.01395" class="title-rel notranslate" onclick="openRelatedPapers('2410.01395')">[REL]</a>
            </h2>
            <p id="authors-2410.01395" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Renkai Wu" target="_blank"><span class="author notranslate">Renkai Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xianjin Wang" target="_blank"><span class="author notranslate">Xianjin Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pengchen Liang" target="_blank"><span class="author notranslate">Pengchen Liang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhenyu Zhang" target="_blank"><span class="author notranslate">Zhenyu Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qing Chang" target="_blank"><span class="author notranslate">Qing Chang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao Tang" target="_blank"><span class="author notranslate">Hao Tang</span></a>
            </p>
            <p id="summary-2410.01395" class="summary">Robot-assisted surgery has profoundly influenced current forms of minimally invasive surgery. However, in transurethral suburethral urological surgical robots, they need to work in a liquid environment. This causes vaporization of the liquid when shearing and heating is performed, resulting in bubble atomization that affects the visual perception of the robot. This can lead to the need for uninterrupted pauses in the surgical procedure, which makes the surgery take longer. To address the atomization characteristics of liquids under urological surgical robotic vision, we propose an unsupervised zero-shot dehaze method (RSF-Dehaze) for urological surgical robotic vision. Specifically, the proposed Region Similarity Filling Module (RSFM) of RSF-Dehaze significantly improves the recovery of blurred region tissues. In addition, we organize and propose a dehaze dataset for robotic vision in urological surgery (USRobot-Dehaze dataset). In particular, this dataset contains the three most common urological surgical robot operation scenarios. To the best of our knowledge, we are the first to organize and propose a publicly available dehaze dataset for urological surgical robot vision. The proposed RSF-Dehaze proves the effectiveness of our method in three urological surgical robot operation scenarios with extensive comparative experiments with 20 most classical and advanced <mark data-markjs="true">dehazing</mark> and image recovery algorithms. The proposed source code and dataset are available at https://github.com/wurenkai/RSF-Dehaze .</p>
            <p id="subjects-2410.01395" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/eess.IV" target="_blank"><span class="subject"><strong>Image and Video Processing</strong></span></a>
                ; <a href="/arxiv/cs.CV" target="_blank"><span class="subject">Computer Vision and Pattern Recognition</span></a>
            </p>
            <p id="date-2410.01395" class="metainfo date"><strong>Publish</strong>: 2024-10-02 10:16:42 UTC</p>
            <div id="pdf-container-2410.01395" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.01395" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.01395" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2410.01225" class="panel paper" keywords="detection,visibility,visual,object,human,piercing,cue,environmental,rtts,ots">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2410.01225" target="_blank" title="6/298"><span class="index notranslate">#6</span></a>
                <a id="title-2410.01225" class="title-link" href="/arxiv/2410.01225" target="_blank">Perceptual Piercing: Human Visual Cue-based Object Detection in Low Visibility Conditions</a>
                <a id="pdf-2410.01225" class="title-pdf notranslate" onclick="togglePdf('2410.01225', 'https://arxiv.org/pdf/2410.01225', this)" style="color: purple;">[PDF<sup id="pdf-stars-2410.01225"></sup>]</a>
                <a id="copy-2410.01225" class="title-copy notranslate" onclick="copyToClipboard('2410.01225')">[Copy]</a>
                <a id="kimi-2410.01225" class="title-kimi notranslate" onclick="toggleKimi('2410.01225', this)">[Kimi<sup id="kimi-stars-2410.01225"></sup>]</a>
                <a id="rel-2410.01225" class="title-rel notranslate" onclick="openRelatedPapers('2410.01225')">[REL]</a>
            </h2>
            <p id="authors-2410.01225" class="metainfo authors notranslate"><strong>Author</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ashutosh Kumar" target="_blank"><span class="author notranslate">Ashutosh Kumar</span></a>
            </p>
            <p id="summary-2410.01225" class="summary">This study proposes a novel deep learning framework inspired by atmospheric scattering and human visual cortex mechanisms to enhance object detection under poor visibility scenarios such as fog, smoke, and haze. These conditions pose significant challenges for object recognition, impacting various sectors, including autonomous driving, aviation management, and security systems. The objective is to enhance the precision and reliability of detection systems under adverse environmental conditions. The research investigates the integration of human-like visual cues, particularly focusing on selective attention and environmental adaptability, to ascertain their impact on object detection's computational efficiency and accuracy. This paper proposes a multi-tiered strategy that integrates an initial quick detection process, followed by targeted region-specific <mark data-markjs="true">dehazing</mark>, and concludes with an in-depth detection phase. The approach is validated using the Foggy Cityscapes, RESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance standards in detection accuracy while significantly optimizing computational efficiency. The findings offer a viable solution for enhancing object detection in poor visibility and contribute to the broader understanding of integrating human visual principles into deep learning algorithms for intricate visual recognition challenges.</p>
            <p id="subjects-2410.01225" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2410.01225" class="metainfo date"><strong>Publish</strong>: 2024-10-02 04:03:07 UTC</p>
            <div id="pdf-container-2410.01225" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2410.01225" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2410.01225" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2409.17432" class="panel paper" keywords="haze,hazespace2m,dehazing,hazy,dehazers,dataset,clarity,images,classification,type">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2409.17432" target="_blank" title="7/298"><span class="index notranslate">#7</span></a>
                <a id="title-2409.17432" class="title-link" href="/arxiv/2409.17432" target="_blank">HazeSpace2M: A Dataset for Haze Aware Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2409.17432" class="title-pdf notranslate" onclick="togglePdf('2409.17432', 'https://arxiv.org/pdf/2409.17432', this)" style="color: purple;">[PDF<sup id="pdf-stars-2409.17432">1</sup>]</a>
                <a id="copy-2409.17432" class="title-copy notranslate" onclick="copyToClipboard('2409.17432')">[Copy]</a>
                <a id="kimi-2409.17432" class="title-kimi notranslate" onclick="toggleKimi('2409.17432', this)">[Kimi<sup id="kimi-stars-2409.17432"></sup>]</a>
                <a id="rel-2409.17432" class="title-rel notranslate" onclick="openRelatedPapers('2409.17432')">[REL]</a>
            </h2>
            <p id="authors-2409.17432" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Md Tanvir Islam" target="_blank"><span class="author notranslate">Md Tanvir Islam</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nasir Rahim" target="_blank"><span class="author notranslate">Nasir Rahim</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Saeed Anwar" target="_blank"><span class="author notranslate">Saeed Anwar</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Muhammad Saqib" target="_blank"><span class="author notranslate">Muhammad Saqib</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sambit Bakshi" target="_blank"><span class="author notranslate">Sambit Bakshi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Khan Muhammad" target="_blank"><span class="author notranslate">Khan Muhammad</span></a>
            </p>
            <p id="summary-2409.17432" class="summary">Reducing the atmospheric haze and enhancing image clarity is crucial for computer vision applications. The lack of real-life hazy ground truth images necessitates synthetic datasets, which often lack diverse haze types, impeding effective haze type classification and <mark data-markjs="true">dehazing</mark> algorithm selection. This research introduces the HazeSpace2M dataset, a collection of over 2 million images designed to enhance <mark data-markjs="true">dehazing</mark> through haze type classification. HazeSpace2M includes diverse scenes with 10 haze intensity levels, featuring Fog, Cloud, and Environmental Haze (EH). Using the dataset, we introduce a technique of haze type classification followed by specialized dehazers to clear hazy images. Unlike conventional methods, our approach classifies haze types before applying type-specific <mark data-markjs="true">dehazing</mark>, improving clarity in real-life hazy images. Benchmarking with state-of-the-art (SOTA) models, ResNet50 and AlexNet achieve 92.75\% and 92.50\% accuracy, respectively, against existing synthetic datasets. However, these models achieve only 80% and 70% accuracy, respectively, against our Real Hazy Testset (RHT), highlighting the challenging nature of our HazeSpace2M dataset. Additional experiments show that haze type classification followed by specialized <mark data-markjs="true">dehazing</mark> improves results by 2.41% in PSNR, 17.14% in SSIM, and 10.2\% in MSE over general dehazers. Moreover, when testing with SOTA <mark data-markjs="true">dehazing</mark> models, we found that applying our proposed framework significantly improves their performance. These results underscore the significance of HazeSpace2M and our proposed framework in addressing atmospheric haze in multimedia processing. Complete code and dataset is available on \href{https://github.com/tanvirnwu/HazeSpace2M} {\textcolor{blue}{\textbf{GitHub}}}.</p>
            <p id="subjects-2409.17432" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2409.17432" class="metainfo date"><strong>Publish</strong>: 2024-09-25 23:47:25 UTC</p>
            <div id="pdf-container-2409.17432" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2409.17432" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2409.17432" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2409.10353" class="panel paper" keywords="diffusion,restoration,taming,image,review,models,dehazing,conform,tasks,deblurring">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2409.10353" target="_blank" title="8/298"><span class="index notranslate">#8</span></a>
                <a id="title-2409.10353" class="title-link" href="/arxiv/2409.10353" target="_blank">Taming Diffusion Models for Image Restoration: A Review</a>
                <a id="pdf-2409.10353" class="title-pdf notranslate" onclick="togglePdf('2409.10353', 'https://arxiv.org/pdf/2409.10353', this)">[PDF<sup id="pdf-stars-2409.10353">3</sup>]</a>
                <a id="copy-2409.10353" class="title-copy notranslate" onclick="copyToClipboard('2409.10353')">[Copy]</a>
                <a id="kimi-2409.10353" class="title-kimi notranslate" onclick="toggleKimi('2409.10353', this)">[Kimi<sup id="kimi-stars-2409.10353">4</sup>]</a>
                <a id="rel-2409.10353" class="title-rel notranslate" onclick="openRelatedPapers('2409.10353')">[REL]</a>
            </h2>
            <p id="authors-2409.10353" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziwei Luo" target="_blank"><span class="author notranslate">Ziwei Luo</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fredrik K. Gustafsson" target="_blank"><span class="author notranslate">Fredrik K. Gustafsson</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zheng Zhao" target="_blank"><span class="author notranslate">Zheng Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jens Sjlund" target="_blank"><span class="author notranslate">Jens Sjlund</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Thomas B. Schn" target="_blank"><span class="author notranslate">Thomas B. Schn</span></a>
            </p>
            <p id="summary-2409.10353" class="summary">Diffusion models have achieved remarkable progress in generative modelling, particularly in enhancing image quality to conform to human preferences. Recently, these models have also been applied to low-level computer vision for photo-realistic image restoration (IR) in tasks such as image denoising, deblurring, <mark data-markjs="true">dehazing</mark>, etc. In this review paper, we introduce key constructions in diffusion models and survey contemporary techniques that make use of diffusion models in solving general IR tasks. Furthermore, we point out the main challenges and limitations of existing diffusion-based IR frameworks and provide potential directions for future work.</p>
            <p id="subjects-2409.10353" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2409.10353" class="metainfo date"><strong>Publish</strong>: 2024-09-16 15:04:14 UTC</p>
            <div id="pdf-container-2409.10353" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2409.10353" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2409.10353" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2409.09779" class="panel paper" keywords="underwater,waterformer,color,dehazing,block,haze,restoration,marine,capture,cast">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2409.09779" target="_blank" title="9/298"><span class="index notranslate">#9</span></a>
                <a id="title-2409.09779" class="title-link" href="/arxiv/2409.09779" target="_blank">Underwater Image Enhancement via <mark data-markjs="true">Dehazing</mark> and Color Restoration</a>
                <a id="pdf-2409.09779" class="title-pdf notranslate" onclick="togglePdf('2409.09779', 'https://arxiv.org/pdf/2409.09779', this)">[PDF<sup id="pdf-stars-2409.09779">1</sup>]</a>
                <a id="copy-2409.09779" class="title-copy notranslate" onclick="copyToClipboard('2409.09779')">[Copy]</a>
                <a id="kimi-2409.09779" class="title-kimi notranslate" onclick="toggleKimi('2409.09779', this)">[Kimi<sup id="kimi-stars-2409.09779"></sup>]</a>
                <a id="rel-2409.09779" class="title-rel notranslate" onclick="openRelatedPapers('2409.09779')">[REL]</a>
            </h2>
            <p id="authors-2409.09779" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chengqin Wu" target="_blank"><span class="author notranslate">Chengqin Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuai Yu" target="_blank"><span class="author notranslate">Shuai Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qingson Hu" target="_blank"><span class="author notranslate">Qingson Hu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingxiang Xu" target="_blank"><span class="author notranslate">Jingxiang Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lijun Zhang" target="_blank"><span class="author notranslate">Lijun Zhang</span></a>
            </p>
            <p id="summary-2409.09779" class="summary">With the rapid development of marine engineering projects such as marine resource extraction and oceanic surveys, underwater visual imaging and analysis has become a critical technology. Unfortunately, due to the inevitable non-linear attenuation of light in underwater environments, underwater images and videos often suffer from low contrast, blurriness, and color degradation, which significantly complicate the subsequent research. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process and disregard their independence and interdependence, which limits the performance improvement. Here, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve the underwater image quality. WaterFormer contains three major components: a <mark data-markjs="true">dehazing</mark> block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) to capture fusion features within the network. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. To improve the quality of the enhanced images, we introduce the Chromatic Consistency Loss and Sobel Color Loss to train the network. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.</p>
            <p id="subjects-2409.09779" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2409.09779" class="metainfo date"><strong>Publish</strong>: 2024-09-15 15:58:20 UTC</p>
            <div id="pdf-container-2409.09779" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2409.09779" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2409.09779" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2409.08510" class="panel paper" keywords="casdyf,dehazing,dynamic,cascaded,filters,dauing,receptive,reside,haze4k,21db">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2409.08510" target="_blank" title="10/298"><span class="index notranslate">#10</span></a>
                <a id="title-2409.08510" class="title-link" href="/arxiv/2409.08510" target="_blank">CasDyF-Net: Image <mark data-markjs="true">Dehazing</mark> via Cascaded Dynamic Filters</a>
                <a id="pdf-2409.08510" class="title-pdf notranslate" onclick="togglePdf('2409.08510', 'https://arxiv.org/pdf/2409.08510', this)" style="color: purple;">[PDF<sup id="pdf-stars-2409.08510"></sup>]</a>
                <a id="copy-2409.08510" class="title-copy notranslate" onclick="copyToClipboard('2409.08510')">[Copy]</a>
                <a id="kimi-2409.08510" class="title-kimi notranslate" onclick="toggleKimi('2409.08510', this)">[Kimi<sup id="kimi-stars-2409.08510"></sup>]</a>
                <a id="rel-2409.08510" class="title-rel notranslate" onclick="openRelatedPapers('2409.08510')">[REL]</a>
            </h2>
            <p id="authors-2409.08510" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wang Yinglong" target="_blank"><span class="author notranslate">Wang Yinglong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=He Bin" target="_blank"><span class="author notranslate">He Bin</span></a>
            </p>
            <p id="summary-2409.08510" class="summary">Image <mark data-markjs="true">dehazing</mark> aims to restore image clarity and visual quality by reducing atmospheric scattering and absorption effects. While deep learning has made significant strides in this area, more and more methods are constrained by network depth. Consequently, lots of approaches have adopted parallel branching strategies. however, they often prioritize aspects such as resolution, receptive field, or frequency domain segmentation without dynamically partitioning branches based on the distribution of input features. Inspired by dynamic filtering, we propose using cascaded dynamic filters to create a multi-branch network by dynamically generating filter kernels based on feature map distribution. To better handle branch features, we propose a residual multiscale block (RMB), combining different receptive fields. Furthermore, we also introduce a dynamic convolution-based local fusion method to merge features from adjacent branches. Experiments on RESIDE, Haze4K, and O-Haze datasets validate our method's effectiveness, with our model achieving a PSNR of 43.21dB on the RESIDE-Indoor dataset. The code is available at https://github.com/dauing/CasDyF-Net.</p>
            <p id="subjects-2409.08510" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2409.08510" class="metainfo date"><strong>Publish</strong>: 2024-09-13 03:20:38 UTC</p>
            <div id="pdf-container-2409.08510" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2409.08510" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2409.08510" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2409.04812" class="panel paper" keywords="aerial,plair,plai,line,weather,restoration,power,image,dverse,datasets">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2409.04812" target="_blank" title="11/298"><span class="index notranslate">#11</span></a>
                <a id="title-2409.04812" class="title-link" href="/arxiv/2409.04812" target="_blank">Power Line Aerial Image Restoration under dverse Weather: Datasets and Baselines</a>
                <a id="pdf-2409.04812" class="title-pdf notranslate" onclick="togglePdf('2409.04812', 'https://arxiv.org/pdf/2409.04812', this)">[PDF<sup id="pdf-stars-2409.04812"></sup>]</a>
                <a id="copy-2409.04812" class="title-copy notranslate" onclick="copyToClipboard('2409.04812')">[Copy]</a>
                <a id="kimi-2409.04812" class="title-kimi notranslate" onclick="toggleKimi('2409.04812', this)">[Kimi<sup id="kimi-stars-2409.04812"></sup>]</a>
                <a id="rel-2409.04812" class="title-rel notranslate" onclick="openRelatedPapers('2409.04812')">[REL]</a>
            </h2>
            <p id="authors-2409.04812" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sai Yang" target="_blank"><span class="author notranslate">Sai Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bin Hu" target="_blank"><span class="author notranslate">Bin Hu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bojun Zhou" target="_blank"><span class="author notranslate">Bojun Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fan Liu" target="_blank"><span class="author notranslate">Fan Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaoxin Wu" target="_blank"><span class="author notranslate">Xiaoxin Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinsong Zhang" target="_blank"><span class="author notranslate">Xinsong Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Juping Gu" target="_blank"><span class="author notranslate">Juping Gu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Zhou" target="_blank"><span class="author notranslate">Jun Zhou</span></a>
            </p>
            <p id="summary-2409.04812" class="summary">Power Line Autonomous Inspection (PLAI) plays a crucial role in the construction of smart grids due to its great advantages of low cost, high efficiency, and safe operation. PLAI is completed by accurately detecting the electrical components and defects in the aerial images captured by Unmanned Aerial Vehicles (UAVs). However, the visible quality of aerial images is inevitably degraded by adverse weather like haze, rain, or snow, which are found to drastically decrease the detection accuracy in our research. To circumvent this problem, we propose a new task of Power Line Aerial Image Restoration under Adverse Weather (PLAIR-AW), which aims to recover clean and high-quality images from degraded images with bad weather thus improving detection performance for PLAI. In this context, we are the first to release numerous corresponding datasets, namely, HazeCPLID, HazeTTPLA, HazeInsPLAD for power line aerial image <mark data-markjs="true">dehazing</mark>, RainCPLID, RainTTPLA, RainInsPLAD for power line aerial image deraining, SnowCPLID, SnowInsPLAD for power line aerial image desnowing, which are synthesized upon the public power line aerial image datasets of CPLID, TTPLA, InsPLAD following the mathematical models. Meanwhile, we select numerous state-of-the-art methods from image restoration community as the baseline methods for PLAIR-AW. At last, we conduct large-scale empirical experiments to evaluate the performance of baseline methods on the proposed datasets. The proposed datasets and trained models are available at https://github.com/ntuhubin/PLAIR-AW.</p>
            <p id="subjects-2409.04812" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2409.04812" class="metainfo date"><strong>Publish</strong>: 2024-09-07 12:53:05 UTC</p>
            <div id="pdf-container-2409.04812" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2409.04812" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2409.04812" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2408.12317" class="panel paper" keywords="cliphaze,haze,clip,dehazing,cam,aggregation,homogeneous,receptive,instructor,synergizes">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2408.12317" target="_blank" title="12/298"><span class="index notranslate">#12</span></a>
                <a id="title-2408.12317" class="title-link" href="/arxiv/2408.12317" target="_blank">Adapt CLIP as Aggregation Instructor for Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2408.12317" class="title-pdf notranslate" onclick="togglePdf('2408.12317', 'https://arxiv.org/pdf/2408.12317', this)">[PDF<sup id="pdf-stars-2408.12317"></sup>]</a>
                <a id="copy-2408.12317" class="title-copy notranslate" onclick="copyToClipboard('2408.12317')">[Copy]</a>
                <a id="kimi-2408.12317" class="title-kimi notranslate" onclick="toggleKimi('2408.12317', this)">[Kimi<sup id="kimi-stars-2408.12317">3</sup>]</a>
                <a id="rel-2408.12317" class="title-rel notranslate" onclick="openRelatedPapers('2408.12317')">[REL]</a>
            </h2>
            <p id="authors-2408.12317" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaozhe Zhang" target="_blank"><span class="author notranslate">Xiaozhe Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fengying Xie" target="_blank"><span class="author notranslate">Fengying Xie</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haidong Ding" target="_blank"><span class="author notranslate">Haidong Ding</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Linpeng Pan" target="_blank"><span class="author notranslate">Linpeng Pan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhenwei Shi" target="_blank"><span class="author notranslate">Zhenwei Shi</span></a>
            </p>
            <p id="summary-2408.12317" class="summary">Most <mark data-markjs="true">dehazing</mark> methods suffer from limited receptive field and do not explore the rich semantic prior encapsulated in vision-language models, which have proven effective in downstream tasks. In this paper, we introduce CLIPHaze, a pioneering hybrid framework that synergizes the efficient global modeling of Mamba with the prior knowledge and zero-shot capabilities of CLIP to address both issues simultaneously. Specifically, our method employs parallel state space model and window-based self-attention to obtain global contextual dependency and local fine-grained perception, respectively. To seamlessly aggregate information from both paths, we introduce CLIP-instructed Aggregation Module (CAM). For non-homogeneous and homogeneous haze, CAM leverages zero-shot estimated haze density map and high-quality image embedding without degradation information to explicitly and implicitly determine the optimal neural operation range for each pixel, thereby adaptively fusing two paths with different receptive fields. Extensive experiments on various benchmarks demonstrate that CLIPHaze achieves state-of-the-art (SOTA) performance, particularly in non-homogeneous haze. Code will be publicly after acceptance.</p>
            <p id="subjects-2408.12317" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2408.12317" class="metainfo date"><strong>Publish</strong>: 2024-08-22 11:51:50 UTC</p>
            <div id="pdf-container-2408.12317" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2408.12317" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2408.12317" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2408.10145" class="panel paper" keywords="restoration,image,multi,scale,rfb,representation,detail,details,cnn,block">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2408.10145" target="_blank" title="13/298"><span class="index notranslate">#13</span></a>
                <a id="title-2408.10145" class="title-link" href="/arxiv/2408.10145" target="_blank">Multi-Scale Representation Learning for Image Restoration with State-Space Model</a>
                <a id="pdf-2408.10145" class="title-pdf notranslate" onclick="togglePdf('2408.10145', 'https://arxiv.org/pdf/2408.10145', this)">[PDF<sup id="pdf-stars-2408.10145">5</sup>]</a>
                <a id="copy-2408.10145" class="title-copy notranslate" onclick="copyToClipboard('2408.10145')">[Copy]</a>
                <a id="kimi-2408.10145" class="title-kimi notranslate" onclick="toggleKimi('2408.10145', this)">[Kimi<sup id="kimi-stars-2408.10145">2</sup>]</a>
                <a id="rel-2408.10145" class="title-rel notranslate" onclick="openRelatedPapers('2408.10145')">[REL]</a>
            </h2>
            <p id="authors-2408.10145" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuhong He" target="_blank"><span class="author notranslate">Yuhong He</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Long Peng" target="_blank"><span class="author notranslate">Long Peng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qiaosi Yi" target="_blank"><span class="author notranslate">Qiaosi Yi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen Wu" target="_blank"><span class="author notranslate">Chen Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lu Wang" target="_blank"><span class="author notranslate">Lu Wang</span></a>
            </p>
            <p id="summary-2408.10145" class="summary">Image restoration endeavors to reconstruct a high-quality, detail-rich image from a degraded counterpart, which is a pivotal process in photography and various computer vision systems. In real-world scenarios, different types of degradation can cause the loss of image details at various scales and degrade image contrast. Existing methods predominantly rely on CNN and Transformer to capture multi-scale representations. However, these methods are often limited by the high computational complexity of Transformers and the constrained receptive field of CNN, which hinder them from achieving superior performance and efficiency in image restoration. To address these challenges, we propose a novel Multi-Scale State-Space Model-based (MS-Mamba) for efficient image restoration that enhances the capacity for multi-scale representation learning through our proposed global and regional SSM modules. Additionally, an Adaptive Gradient Block (AGB) and a Residual Fourier Block (RFB) are proposed to improve the network's detail extraction capabilities by capturing gradients in various directions and facilitating learning details in the frequency domain. Extensive experiments on nine public benchmarks across four classic image restoration tasks, image deraining, <mark data-markjs="true">dehazing</mark>, denoising, and low-light enhancement, demonstrate that our proposed method achieves new state-of-the-art performance while maintaining low computational complexity. The source code will be publicly available.</p>
            <p id="subjects-2408.10145" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2408.10145" class="metainfo date"><strong>Publish</strong>: 2024-08-19 16:42:58 UTC</p>
            <div id="pdf-container-2408.10145" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2408.10145" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2408.10145" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2408.08149" class="panel paper" keywords="restoration,vision,vat,level,retraining,tasks,unsupervised,high,riational,ranslator">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2408.08149" target="_blank" title="14/298"><span class="index notranslate">#14</span></a>
                <a id="title-2408.08149" class="title-link" href="/arxiv/2408.08149" target="_blank">Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks</a>
                <a id="pdf-2408.08149" class="title-pdf notranslate" onclick="togglePdf('2408.08149', 'https://arxiv.org/pdf/2408.08149', this)">[PDF<sup id="pdf-stars-2408.08149">5</sup>]</a>
                <a id="copy-2408.08149" class="title-copy notranslate" onclick="copyToClipboard('2408.08149')">[Copy]</a>
                <a id="kimi-2408.08149" class="title-kimi notranslate" onclick="toggleKimi('2408.08149', this)">[Kimi<sup id="kimi-stars-2408.08149">4</sup>]</a>
                <a id="rel-2408.08149" class="title-rel notranslate" onclick="openRelatedPapers('2408.08149')">[REL]</a>
            </h2>
            <p id="authors-2408.08149" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiawei Wu" target="_blank"><span class="author notranslate">Jiawei Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhi Jin" target="_blank"><span class="author notranslate">Zhi Jin</span></a>
            </p>
            <p id="summary-2408.08149" class="summary">Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments. These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks. However, collecting paired data in real-world scenarios and retraining large-scale models are challenge. To this end, we propose an unsupervised learning method called \textbf{Va}riational \textbf{T}ranslator (VaT), which does not require retraining existing restoration and high-level vision networks. Instead, it establishes a lightweight network that serves as an intermediate bridge between them. By variational inference, VaT approximates the joint distribution of restoration output and high-level vision input, dividing the optimization objective into preserving content and maximizing marginal likelihood associated with high-level vision tasks. By cleverly leveraging self-training paradigms, VaT achieves the above optimization objective without requiring labels. As a result, the translated images maintain a close resemblance to their original content while also demonstrating exceptional performance on high-level vision tasks. Extensive experiments in <mark data-markjs="true">dehazing</mark> and low-light enhancement for detection and classification show the superiority of our method over other state-of-the-art unsupervised counterparts, even significantly surpassing supervised methods in some complex real-world scenarios.</p>
            <p id="subjects-2408.08149" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2408.08149" class="metainfo date"><strong>Publish</strong>: 2024-08-15 13:35:59 UTC</p>
            <div id="pdf-container-2408.08149" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2408.08149" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2408.08149" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2408.05683" class="panel paper" keywords="dehazing,depth,hazy,order,perception,farther,images,weather,closer,single">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2408.05683" target="_blank" title="15/298"><span class="index notranslate">#15</span></a>
                <a id="title-2408.05683" class="title-link" href="/arxiv/2408.05683" target="_blank">Single Image <mark data-markjs="true">Dehazing</mark> Using Scene Depth Ordering</a>
                <a id="pdf-2408.05683" class="title-pdf notranslate" onclick="togglePdf('2408.05683', 'https://arxiv.org/pdf/2408.05683', this)" style="color: purple;">[PDF<sup id="pdf-stars-2408.05683">5</sup>]</a>
                <a id="copy-2408.05683" class="title-copy notranslate" onclick="copyToClipboard('2408.05683')">[Copy]</a>
                <a id="kimi-2408.05683" class="title-kimi notranslate" onclick="toggleKimi('2408.05683', this)">[Kimi<sup id="kimi-stars-2408.05683">2</sup>]</a>
                <a id="rel-2408.05683" class="title-rel notranslate" onclick="openRelatedPapers('2408.05683')">[REL]</a>
            </h2>
            <p id="authors-2408.05683" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pengyang Ling" target="_blank"><span class="author notranslate">Pengyang Ling</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huaian Chen" target="_blank"><span class="author notranslate">Huaian Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiao Tan" target="_blank"><span class="author notranslate">Xiao Tan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yimeng Shan" target="_blank"><span class="author notranslate">Yimeng Shan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yi Jin" target="_blank"><span class="author notranslate">Yi Jin</span></a>
            </p>
            <p id="summary-2408.05683" class="summary">Images captured in hazy weather generally suffer from quality degradation, and many <mark data-markjs="true">dehazing</mark> methods have been developed to solve this problem. However, single image <mark data-markjs="true">dehazing</mark> problem is still challenging due to its ill-posed nature. In this paper, we propose a depth order guided single image <mark data-markjs="true">dehazing</mark> method, which utilizes depth order in hazy images to guide the <mark data-markjs="true">dehazing</mark> process to achieve a similar depth perception in corresponding <mark data-markjs="true">dehazing</mark> results. The consistency of depth perception ensures that the regions that look farther or closer in hazy images also appear farther or closer in the corresponding <mark data-markjs="true">dehazing</mark> results, and thus effectively avoid the undesired visual effects. To achieve this goal, a simple yet effective strategy is proposed to extract the depth order in hazy images, which offers a reference for depth perception in hazy weather. Additionally, a depth order embedded transformation model is devised, which performs transmission estimation under the guidance of depth order to realize an unchanged depth order in the <mark data-markjs="true">dehazing</mark> results. The extracted depth order provides a powerful global constraint for the <mark data-markjs="true">dehazing</mark> process, which contributes to the efficient utilization of global information, thereby bringing an overall improvement in restoration quality. Extensive experiments demonstrate that the proposed method can better recover potential structure and vivid color with higher computational efficiency than the state-of-the-art <mark data-markjs="true">dehazing</mark> methods.</p>
            <p id="subjects-2408.05683" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.MM" target="_blank"><span class="subject">Multimedia</span></a>
            </p>
            <p id="date-2408.05683" class="metainfo date"><strong>Publish</strong>: 2024-08-11 03:29:27 UTC</p>
            <div id="pdf-container-2408.05683" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2408.05683" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2408.05683" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.14868" class="panel paper" keywords="underwater,color,uier,image,illuminance,illumination,dehazing,restoration,variational,proposed">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.14868" target="_blank" title="16/298"><span class="index notranslate">#16</span></a>
                <a id="title-2407.14868" class="title-link" href="/arxiv/2407.14868" target="_blank">Dual High-Order Total Variation Model for Underwater Image Restoration</a>
                <a id="pdf-2407.14868" class="title-pdf notranslate" onclick="togglePdf('2407.14868', 'https://arxiv.org/pdf/2407.14868', this)">[PDF<sup id="pdf-stars-2407.14868"></sup>]</a>
                <a id="copy-2407.14868" class="title-copy notranslate" onclick="copyToClipboard('2407.14868')">[Copy]</a>
                <a id="kimi-2407.14868" class="title-kimi notranslate" onclick="toggleKimi('2407.14868', this)">[Kimi<sup id="kimi-stars-2407.14868"></sup>]</a>
                <a id="rel-2407.14868" class="title-rel notranslate" onclick="openRelatedPapers('2407.14868')">[REL]</a>
            </h2>
            <p id="authors-2407.14868" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuemei Li" target="_blank"><span class="author notranslate">Yuemei Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guojia Hou" target="_blank"><span class="author notranslate">Guojia Hou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peixian Zhuang" target="_blank"><span class="author notranslate">Peixian Zhuang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhenkuan Pan" target="_blank"><span class="author notranslate">Zhenkuan Pan</span></a>
            </p>
            <p id="summary-2407.14868" class="summary">Underwater images are typically characterized by color cast, haze, blurring, and uneven illumination due to the selective absorption and scattering when light propagates through the water, which limits their practical applications. Underwater image enhancement and restoration (UIER) is one crucial mode to improve the visual quality of underwater images. However, most existing UIER methods concentrate on enhancing contrast and <mark data-markjs="true">dehazing</mark>, rarely pay attention to the local illumination differences within the image caused by illumination variations, thus introducing some undesirable artifacts and unnatural color. To address this issue, an effective variational framework is proposed based on an extended underwater image formation model (UIFM). Technically, dual high-order regularizations are successfully integrated into the variational model to acquire smoothed local ambient illuminance and structure-revealed reflectance in a unified manner. In our proposed framework, the weight factors-based color compensation is combined with the color balance to compensate for the attenuated color channels and remove the color cast. In particular, the local ambient illuminance with strong robustness is acquired by performing the local patch brightest pixel estimation and an improved gamma correction. Additionally, we design an iterative optimization algorithm relying on the alternating direction method of multipliers (ADMM) to accelerate the solution of the proposed variational model. Considerable experiments on three real-world underwater image datasets demonstrate that the proposed method outperforms several state-of-the-art methods with regard to visual quality and quantitative assessments. Moreover, the proposed method can also be extended to outdoor image <mark data-markjs="true">dehazing</mark>, low-light image enhancement, and some high-level vision tasks. The code is available at https://github.com/Hou-Guojia/UDHTV.</p>
            <p id="subjects-2407.14868" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.14868" class="metainfo date"><strong>Publish</strong>: 2024-07-20 13:06:37 UTC</p>
            <div id="pdf-container-2407.14868" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.14868" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.14868" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.14823" class="panel paper" keywords="dehazing,crossdehaze,image,augmentation,wengzp1,scaleupdehazing,rsid,vision,task,methods">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.14823" target="_blank" title="17/298"><span class="index notranslate">#17</span></a>
                <a id="title-2407.14823" class="title-link" href="/arxiv/2407.14823" target="_blank">CrossDehaze: Scaling Up Image <mark data-markjs="true">Dehazing</mark> with Cross-Data Vision Alignment and Augmentation</a>
                <a id="pdf-2407.14823" class="title-pdf notranslate" onclick="togglePdf('2407.14823', 'https://arxiv.org/pdf/2407.14823', this)">[PDF<sup id="pdf-stars-2407.14823"></sup>]</a>
                <a id="copy-2407.14823" class="title-copy notranslate" onclick="copyToClipboard('2407.14823')">[Copy]</a>
                <a id="kimi-2407.14823" class="title-kimi notranslate" onclick="toggleKimi('2407.14823', this)">[Kimi<sup id="kimi-stars-2407.14823"></sup>]</a>
                <a id="rel-2407.14823" class="title-rel notranslate" onclick="openRelatedPapers('2407.14823')">[REL]</a>
            </h2>
            <p id="authors-2407.14823" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yukai Shi" target="_blank"><span class="author notranslate">Yukai Shi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhipeng Weng" target="_blank"><span class="author notranslate">Zhipeng Weng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yupei Lin" target="_blank"><span class="author notranslate">Yupei Lin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cidan Shi" target="_blank"><span class="author notranslate">Cidan Shi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaojun Yang" target="_blank"><span class="author notranslate">Xiaojun Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liang Lin" target="_blank"><span class="author notranslate">Liang Lin</span></a>
            </p>
            <p id="summary-2407.14823" class="summary">In recent years, as computer vision tasks have increasingly relied on high-quality image inputs, the task of image <mark data-markjs="true">dehazing</mark> has received significant attention. Previously, many methods based on priors and deep learning have been proposed to address the task of image <mark data-markjs="true">dehazing</mark>. Ignoring the domain gap between different data, former de-hazing methods usually adopt multiple datasets for explicit training, which often makes the methods themselves be violated. To address this problem, we propose a novel method of internal and external data augmentation to improve the existing <mark data-markjs="true">dehazing</mark> methodology. By using cross-data external augmentor. The dataset inherits samples from different domains that are firmly aligned, making the model learn more robust and generalizable features. By using the internal data augmentation method, the model can fully exploit local information within the images, thereby obtaining more image details. To demonstrate the effectiveness of our proposed method, we conduct training on both the Natural Image Dataset (NID) and the Remote Sensing Image Dataset (RSID). Experimental results show that our method clearly resolves the domain gap in different <mark data-markjs="true">dehazing</mark> datasets and presents a new pipeline for joint training in the <mark data-markjs="true">dehazing</mark> task. Our approach significantly outperforms other advanced methods in <mark data-markjs="true">dehazing</mark> and produces dehazed images that are closest to real haze-free images. The code will be available at: https://github.com/wengzp1/ScaleUpDehazing</p>
            <p id="subjects-2407.14823" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
                ; <a href="/arxiv/cs.MM" target="_blank"><span class="subject">Multimedia</span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2407.14823" class="metainfo date"><strong>Publish</strong>: 2024-07-20 10:00:20 UTC</p>
            <div id="pdf-container-2407.14823" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.14823" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.14823" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.13719" class="panel paper" keywords="hazeclip,dehazing,hazy,troivyn,language,world,clip,real,image,guided">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.13719" target="_blank" title="18/298"><span class="index notranslate">#18</span></a>
                <a id="title-2407.13719" class="title-link" href="/arxiv/2407.13719" target="_blank">HazeCLIP: Towards Language Guided Real-World Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2407.13719" class="title-pdf notranslate" onclick="togglePdf('2407.13719', 'https://arxiv.org/pdf/2407.13719', this)">[PDF<sup id="pdf-stars-2407.13719">3</sup>]</a>
                <a id="copy-2407.13719" class="title-copy notranslate" onclick="copyToClipboard('2407.13719')">[Copy]</a>
                <a id="kimi-2407.13719" class="title-kimi notranslate" onclick="toggleKimi('2407.13719', this)">[Kimi<sup id="kimi-stars-2407.13719">5</sup>]</a>
                <a id="rel-2407.13719" class="title-rel notranslate" onclick="openRelatedPapers('2407.13719')">[REL]</a>
            </h2>
            <p id="authors-2407.13719" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruiyi Wang" target="_blank"><span class="author notranslate">Ruiyi Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenhao Li" target="_blank"><span class="author notranslate">Wenhao Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaohong Liu" target="_blank"><span class="author notranslate">Xiaohong Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chunyi Li" target="_blank"><span class="author notranslate">Chunyi Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zicheng Zhang" target="_blank"><span class="author notranslate">Zicheng Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiongkuo Min" target="_blank"><span class="author notranslate">Xiongkuo Min</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guangtao Zhai" target="_blank"><span class="author notranslate">Guangtao Zhai</span></a>
            </p>
            <p id="summary-2407.13719" class="summary">Existing methods have achieved remarkable performance in single image <mark data-markjs="true">dehazing</mark>, particularly on synthetic datasets. However, they often struggle with real-world hazy images due to domain shift, limiting their practical applicability. This paper introduces HazeCLIP, a language-guided adaptation framework designed to enhance the real-world performance of pre-trained <mark data-markjs="true">dehazing</mark> networks. Inspired by the Contrastive Language-Image Pre-training (CLIP) model's ability to distinguish between hazy and clean images, we utilize it to evaluate <mark data-markjs="true">dehazing</mark> results. Combined with a region-specific <mark data-markjs="true">dehazing</mark> technique and tailored prompt sets, CLIP model accurately identifies hazy areas, providing a high-quality, human-like prior that guides the fine-tuning process of pre-trained networks. Extensive experiments demonstrate that HazeCLIP achieves the state-of-the-art performance in real-word image <mark data-markjs="true">dehazing</mark>, evaluated through both visual quality and no-reference quality assessments. The code is available: https://github.com/Troivyn/HazeCLIP .</p>
            <p id="subjects-2407.13719" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.13719" class="metainfo date"><strong>Publish</strong>: 2024-07-18 17:18:25 UTC</p>
            <div id="pdf-container-2407.13719" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.13719" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.13719" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.11505" class="panel paper" keywords="dehazing,haze,haam,attention,mfem,haa,aware,image,net,multiscale">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.11505" target="_blank" title="19/298"><span class="index notranslate">#19</span></a>
                <a id="title-2407.11505" class="title-link" href="/arxiv/2407.11505" target="_blank">Haze-Aware Attention Network for Single-Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2407.11505" class="title-pdf notranslate" onclick="togglePdf('2407.11505', 'https://arxiv.org/pdf/2407.11505', this)">[PDF<sup id="pdf-stars-2407.11505"></sup>]</a>
                <a id="copy-2407.11505" class="title-copy notranslate" onclick="copyToClipboard('2407.11505')">[Copy]</a>
                <a id="kimi-2407.11505" class="title-kimi notranslate" onclick="toggleKimi('2407.11505', this)">[Kimi<sup id="kimi-stars-2407.11505"></sup>]</a>
                <a id="rel-2407.11505" class="title-rel notranslate" onclick="openRelatedPapers('2407.11505')">[REL]</a>
            </h2>
            <p id="authors-2407.11505" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lihan Tong" target="_blank"><span class="author notranslate">Lihan Tong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yun Liu" target="_blank"><span class="author notranslate">Yun Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weijia Li" target="_blank"><span class="author notranslate">Weijia Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liyuan Chen" target="_blank"><span class="author notranslate">Liyuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Erkang Chen" target="_blank"><span class="author notranslate">Erkang Chen</span></a>
            </p>
            <p id="summary-2407.11505" class="summary">Single-image <mark data-markjs="true">dehazing</mark> is a pivotal challenge in computer vision that seeks to remove haze from images and restore clean background details. Recognizing the limitations of traditional physical model-based methods and the inefficiencies of current attention-based solutions, we propose a new <mark data-markjs="true">dehazing</mark> network combining an innovative Haze-Aware Attention Module (HAAM) with a Multiscale Frequency Enhancement Module (MFEM). The HAAM is inspired by the atmospheric scattering model, thus skillfully integrating physical principles into high-dimensional features for targeted <mark data-markjs="true">dehazing</mark>. It picks up on latent features during the image restoration process, which gives a significant boost to the metrics, while the MFEM efficiently enhances high-frequency details, thus sidestepping wavelet or Fourier transform complexities. It employs multiscale fields to extract and emphasize key frequency components with minimal parameter overhead. Integrated into a simple U-Net framework, our Haze-Aware Attention Network (HAA-Net) for single-image <mark data-markjs="true">dehazing</mark> significantly outperforms existing attention-based and transformer models in efficiency and effectiveness. Tested across various public datasets, the HAA-Net sets new performance benchmarks. Our work not only advances the field of image <mark data-markjs="true">dehazing</mark> but also offers insights into the design of attention mechanisms for broader applications in computer vision.</p>
            <p id="subjects-2407.11505" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.11505" class="metainfo date"><strong>Publish</strong>: 2024-07-16 08:42:39 UTC</p>
            <div id="pdf-container-2407.11505" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.11505" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.11505" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.10226" class="panel paper" keywords="dehazing,dehaze,collaborative,dcm,branch,ddscm,domain,dual,contour,alleviate">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.10226" target="_blank" title="20/298"><span class="index notranslate">#20</span></a>
                <a id="title-2407.10226" class="title-link" href="/arxiv/2407.10226" target="_blank">Addressing Domain Discrepancy: A Dual-branch Collaborative Model to Unsupervised <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2407.10226" class="title-pdf notranslate" onclick="togglePdf('2407.10226', 'https://arxiv.org/pdf/2407.10226', this)">[PDF<sup id="pdf-stars-2407.10226">1</sup>]</a>
                <a id="copy-2407.10226" class="title-copy notranslate" onclick="copyToClipboard('2407.10226')">[Copy]</a>
                <a id="kimi-2407.10226" class="title-kimi notranslate" onclick="toggleKimi('2407.10226', this)">[Kimi<sup id="kimi-stars-2407.10226"></sup>]</a>
                <a id="rel-2407.10226" class="title-rel notranslate" onclick="openRelatedPapers('2407.10226')">[REL]</a>
            </h2>
            <p id="authors-2407.10226" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuaibin Fan" target="_blank"><span class="author notranslate">Shuaibin Fan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Minglong Xue" target="_blank"><span class="author notranslate">Minglong Xue</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aoxiang Ning" target="_blank"><span class="author notranslate">Aoxiang Ning</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Senming Zhong" target="_blank"><span class="author notranslate">Senming Zhong</span></a>
            </p>
            <p id="summary-2407.10226" class="summary">Although synthetic data can alleviate acquisition challenges in image <mark data-markjs="true">dehazing</mark> tasks, it also introduces the problem of domain bias when dealing with small-scale data. This paper proposes a novel dual-branch collaborative unpaired <mark data-markjs="true">dehazing</mark> model (DCM-dehaze) to address this issue. The proposed method consists of two collaborative branches: <mark data-markjs="true">dehazing</mark> and contour constraints. Specifically, we design a dual depthwise separable convolutional module (DDSCM) to enhance the information expressiveness of deeper features and the correlation to shallow features. In addition, we construct a bidirectional contour function to optimize the edge features of the image to enhance the clarity and fidelity of the image details. Furthermore, we present feature enhancers via a residual dense architecture to eliminate redundant features of the <mark data-markjs="true">dehazing</mark> process and further alleviate the domain deviation problem. Extensive experiments on benchmark datasets show that our method reaches the state-of-the-art. This project code will be available at \url{https://github.com/Fan-pixel/DCM-dehaze.</p>
            <p id="subjects-2407.10226" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.10226" class="metainfo date"><strong>Publish</strong>: 2024-07-14 14:47:32 UTC</p>
            <div id="pdf-container-2407.10226" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.10226" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.10226" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.09768" class="panel paper" keywords="inverse,deterioration,guidance,clustered,prototype,versatile,likelihood,restoration,licencing,problems">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.09768" target="_blank" title="21/298"><span class="index notranslate">#21</span></a>
                <a id="title-2407.09768" class="title-link" href="/arxiv/2407.09768" target="_blank">Prototype Clustered Diffusion Models for Versatile Inverse Problems</a>
                <a id="pdf-2407.09768" class="title-pdf notranslate" onclick="togglePdf('2407.09768', 'https://arxiv.org/pdf/2407.09768', this)">[PDF<sup id="pdf-stars-2407.09768">2</sup>]</a>
                <a id="copy-2407.09768" class="title-copy notranslate" onclick="copyToClipboard('2407.09768')">[Copy]</a>
                <a id="kimi-2407.09768" class="title-kimi notranslate" onclick="toggleKimi('2407.09768', this)">[Kimi<sup id="kimi-stars-2407.09768">1</sup>]</a>
                <a id="rel-2407.09768" class="title-rel notranslate" onclick="openRelatedPapers('2407.09768')">[REL]</a>
            </h2>
            <p id="authors-2407.09768" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinghao Zhang" target="_blank"><span class="author notranslate">Jinghao Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zizheng Yang" target="_blank"><span class="author notranslate">Zizheng Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qi Zhu" target="_blank"><span class="author notranslate">Qi Zhu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Feng Zhao" target="_blank"><span class="author notranslate">Feng Zhao</span></a>
            </p>
            <p id="summary-2407.09768" class="summary">Diffusion models have made remarkable progress in solving various inverse problems, attributing to the generative modeling capability of the data manifold. Posterior sampling from the conditional score function enable the precious data consistency certified by the measurement-based likelihood term. However, most prevailing approaches confined to the deterministic deterioration process of the measurement model, regardless of capricious unpredictable disturbance in real-world sceneries. To address this obstacle, we show that the measurement-based likelihood can be renovated with restoration-based likelihood via the opposite probabilistic graphic direction, licencing the patronage of various off-the-shelf restoration models and extending the strictly deterministic deterioration process to adaptable clustered processes with the supposed prototype, in what we call restorer guidance. Particularly, assembled with versatile prototypes optionally, we can resolve inverse problems with bunch of choices for assorted sample quality and realize the proficient deterioration control with assured realistic. We show that our work can be formally analogous to the transition from classifier guidance to classifier-free guidance in the field of inverse problem solver. Experiments on multifarious inverse problems demonstrate the effectiveness of our method, including image <mark data-markjs="true">dehazing</mark>, rain streak removal, and motion deblurring.</p>
            <p id="subjects-2407.09768" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.09768" class="metainfo date"><strong>Publish</strong>: 2024-07-13 04:24:53 UTC</p>
            <div id="pdf-container-2407.09768" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.09768" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.09768" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.08221" class="panel paper" keywords="gaura,degradations,rendering,degradation,generalizable,vinayak,scenes,desnowing,restoration,synthesis">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.08221" target="_blank" title="22/298"><span class="index notranslate">#22</span></a>
                <a id="title-2407.08221" class="title-link" href="/arxiv/2407.08221" target="_blank">GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views</a>
                <a id="pdf-2407.08221" class="title-pdf notranslate" onclick="togglePdf('2407.08221', 'https://arxiv.org/pdf/2407.08221', this)">[PDF<sup id="pdf-stars-2407.08221">2</sup>]</a>
                <a id="copy-2407.08221" class="title-copy notranslate" onclick="copyToClipboard('2407.08221')">[Copy]</a>
                <a id="kimi-2407.08221" class="title-kimi notranslate" onclick="toggleKimi('2407.08221', this)">[Kimi<sup id="kimi-stars-2407.08221">1</sup>]</a>
                <a id="rel-2407.08221" class="title-rel notranslate" onclick="openRelatedPapers('2407.08221')">[REL]</a>
            </h2>
            <p id="authors-2407.08221" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vinayak Gupta" target="_blank"><span class="author notranslate">Vinayak Gupta</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rongali Simhachala Venkata Girish" target="_blank"><span class="author notranslate">Rongali Simhachala Venkata Girish</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mukund Varma T" target="_blank"><span class="author notranslate">Mukund Varma T</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ayush Tewari" target="_blank"><span class="author notranslate">Ayush Tewari</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaushik Mitra" target="_blank"><span class="author notranslate">Kaushik Mitra</span></a>
            </p>
            <p id="summary-2407.08221" class="summary">Neural rendering methods can achieve near-photorealistic image synthesis of scenes from posed input images. However, when the images are imperfect, e.g., captured in very low-light conditions, state-of-the-art methods fail to reconstruct high-quality 3D scenes. Recent approaches have tried to address this limitation by modeling various degradation processes in the image formation model; however, this limits them to specific image degradations. In this paper, we propose a generalizable neural rendering method that can perform high-fidelity novel view synthesis under several degradations. Our method, GAURA, is learning-based and does not require any test-time scene-specific optimization. It is trained on a synthetic dataset that includes several degradation types. GAURA outperforms state-of-the-art methods on several benchmarks for low-light enhancement, <mark data-markjs="true">dehazing</mark>, deraining, and on-par for motion deblurring. Further, our model can be efficiently fine-tuned to any new incoming degradation using minimal data. We thus demonstrate adaptation results on two unseen degradations, desnowing and removing defocus blur. Code and video results are available at vinayak-vg.github.io/GAURA.</p>
            <p id="subjects-2407.08221" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.08221" class="metainfo date"><strong>Publish</strong>: 2024-07-11 06:44:37 UTC</p>
            <div id="pdf-container-2407.08221" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.08221" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.08221" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.00972" class="panel paper" keywords="dehazing,falcon,mask,adjoint,frequency,continuous,atmospheric,haze,link,quality">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.00972" target="_blank" title="23/298"><span class="index notranslate">#23</span></a>
                <a id="title-2407.00972" class="title-link" href="/arxiv/2407.00972" target="_blank">FALCON: Frequency Adjoint Link with CONtinuous Density Mask for Fast Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2407.00972" class="title-pdf notranslate" onclick="togglePdf('2407.00972', 'https://arxiv.org/pdf/2407.00972', this)">[PDF<sup id="pdf-stars-2407.00972"></sup>]</a>
                <a id="copy-2407.00972" class="title-copy notranslate" onclick="copyToClipboard('2407.00972')">[Copy]</a>
                <a id="kimi-2407.00972" class="title-kimi notranslate" onclick="toggleKimi('2407.00972', this)">[Kimi<sup id="kimi-stars-2407.00972"></sup>]</a>
                <a id="rel-2407.00972" class="title-rel notranslate" onclick="openRelatedPapers('2407.00972')">[REL]</a>
            </h2>
            <p id="authors-2407.00972" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Donghyun Kim" target="_blank"><span class="author notranslate">Donghyun Kim</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seil Kang" target="_blank"><span class="author notranslate">Seil Kang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seong Jae Hwang" target="_blank"><span class="author notranslate">Seong Jae Hwang</span></a>
            </p>
            <p id="summary-2407.00972" class="summary">Image <mark data-markjs="true">dehazing</mark>, addressing atmospheric interference like fog and haze, remains a pervasive challenge crucial for robust vision applications such as surveillance and remote sensing under adverse visibility. While various methodologies have evolved from early works predicting transmission matrix and atmospheric light features to deep learning and <mark data-markjs="true">dehazing</mark> networks, they innately prioritize <mark data-markjs="true">dehazing</mark> quality metrics, neglecting the need for real-time applicability in time-sensitive domains like autonomous driving. This work introduces FALCON (Frequency Adjoint Link with CONtinuous density mask), a single-image <mark data-markjs="true">dehazing</mark> system achieving state-of-the-art performance on both quality and speed. Particularly, we develop a novel bottleneck module, namely, Frequency Adjoint Link, operating in the frequency space to globally expand the receptive field with minimal growth in network size. Further, we leverage the underlying haze distribution based on the atmospheric scattering model via a Continuous Density Mask (CDM) which serves as a continuous-valued mask input prior and a differentiable auxiliary loss. Comprehensive experiments involving multiple state-of-the-art methods and ablation analysis demonstrate FALCON's exceptional performance in both <mark data-markjs="true">dehazing</mark> quality and speed (i.e., &gt;$180 frames-per-second), quantified by metrics such as FPS, PSNR, and SSIM.</p>
            <p id="subjects-2407.00972" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.00972" class="metainfo date"><strong>Publish</strong>: 2024-07-01 05:16:26 UTC</p>
            <div id="pdf-container-2407.00972" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.00972" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.00972" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.01636" class="panel paper" keywords="degradation,restoration,dformer,degradations,frequency,rformer,components,transformers,image,aware">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.01636" target="_blank" title="24/298"><span class="index notranslate">#24</span></a>
                <a id="title-2407.01636" class="title-link" href="/arxiv/2407.01636" target="_blank">Learning Frequency-Aware Dynamic Transformers for All-In-One Image Restoration</a>
                <a id="pdf-2407.01636" class="title-pdf notranslate" onclick="togglePdf('2407.01636', 'https://arxiv.org/pdf/2407.01636', this)">[PDF<sup id="pdf-stars-2407.01636">6</sup>]</a>
                <a id="copy-2407.01636" class="title-copy notranslate" onclick="copyToClipboard('2407.01636')">[Copy]</a>
                <a id="kimi-2407.01636" class="title-kimi notranslate" onclick="toggleKimi('2407.01636', this)">[Kimi<sup id="kimi-stars-2407.01636">2</sup>]</a>
                <a id="rel-2407.01636" class="title-rel notranslate" onclick="openRelatedPapers('2407.01636')">[REL]</a>
            </h2>
            <p id="authors-2407.01636" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zenglin Shi" target="_blank"><span class="author notranslate">Zenglin Shi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tong Su" target="_blank"><span class="author notranslate">Tong Su</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pei Liu" target="_blank"><span class="author notranslate">Pei Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yunpeng Wu" target="_blank"><span class="author notranslate">Yunpeng Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Le Zhang" target="_blank"><span class="author notranslate">Le Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Meng Wang" target="_blank"><span class="author notranslate">Meng Wang</span></a>
            </p>
            <p id="summary-2407.01636" class="summary">This work aims to tackle the all-in-one image restoration task, which seeks to handle multiple types of degradation with a single model. The primary challenge is to extract degradation representations from the input degraded images and use them to guide the model's adaptation to specific degradation types. Recognizing that various degradations affect image content differently across frequency bands, we propose a new all-in-one image restoration approach from a frequency perspective, leveraging advanced vision transformers. Our method consists of two main components: a frequency-aware Degradation prior learning transformer (Dformer) and a degradation-adaptive Restoration transformer (Rformer). The Dformer captures the essential characteristics of various degradations by decomposing inputs into different frequency components. By understanding how degradations affect these frequency components, the Dformer learns robust priors that effectively guide the restoration process. The Rformer then employs a degradation-adaptive self-attention module to selectively focus on the most affected frequency components, guided by the learned degradation representations. Extensive experimental results demonstrate that our approach outperforms the existing methods on four representative restoration tasks, including denoising, deraining, <mark data-markjs="true">dehazing</mark> and deblurring. Additionally, our method offers benefits for handling spatially variant degradations and unseen degradation levels.</p>
            <p id="subjects-2407.01636" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.01636" class="metainfo date"><strong>Publish</strong>: 2024-06-30 13:14:44 UTC</p>
            <div id="pdf-container-2407.01636" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.01636" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.01636" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.00676" class="panel paper" keywords="ipt,instruct,task,tasks,biases,specific,transformer,weight,modulation,propose">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.00676" target="_blank" title="25/298"><span class="index notranslate">#25</span></a>
                <a id="title-2407.00676" class="title-link" href="/arxiv/2407.00676" target="_blank">Instruct-IPT: All-in-One Image Processing Transformer via Weight Modulation</a>
                <a id="pdf-2407.00676" class="title-pdf notranslate" onclick="togglePdf('2407.00676', 'https://arxiv.org/pdf/2407.00676', this)" style="color: purple;">[PDF<sup id="pdf-stars-2407.00676">6</sup>]</a>
                <a id="copy-2407.00676" class="title-copy notranslate" onclick="copyToClipboard('2407.00676')">[Copy]</a>
                <a id="kimi-2407.00676" class="title-kimi notranslate" onclick="toggleKimi('2407.00676', this)">[Kimi<sup id="kimi-stars-2407.00676">7</sup>]</a>
                <a id="rel-2407.00676" class="title-rel notranslate" onclick="openRelatedPapers('2407.00676')">[REL]</a>
            </h2>
            <p id="authors-2407.00676" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuchuan Tian" target="_blank"><span class="author notranslate">Yuchuan Tian</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianhong Han" target="_blank"><span class="author notranslate">Jianhong Han</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hanting Chen" target="_blank"><span class="author notranslate">Hanting Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuanyuan Xi" target="_blank"><span class="author notranslate">Yuanyuan Xi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guoyang Zhang" target="_blank"><span class="author notranslate">Guoyang Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie Hu" target="_blank"><span class="author notranslate">Jie Hu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chao Xu" target="_blank"><span class="author notranslate">Chao Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yunhe Wang" target="_blank"><span class="author notranslate">Yunhe Wang</span></a>
            </p>
            <p id="summary-2407.00676" class="summary">Due to the unaffordable size and intensive computation costs of low-level vision models, All-in-One models that are designed to address a handful of low-level vision tasks simultaneously have been popular. However, existing All-in-One models are limited in terms of the range of tasks and performance. To overcome these limitations, we propose Instruct-IPT -- an All-in-One Image Processing Transformer that could effectively address manifold image restoration tasks with large inter-task gaps, such as denoising, deblurring, deraining, <mark data-markjs="true">dehazing</mark>, and desnowing. Rather than popular feature adaptation methods, we propose weight modulation that adapts weights to specific tasks. Firstly, we figure out task-sensitive weights via a toy experiment and introduce task-specific biases on top of them. Secondly, we conduct rank analysis for a good compression strategy and perform low-rank decomposition on the biases. Thirdly, we propose synchronous training that updates the task-general backbone model and the task-specific biases simultaneously. In this way, the model is instructed to learn general and task-specific knowledge. Via our simple yet effective method that instructs the IPT to be task experts, Instruct-IPT could better cooperate between tasks with distinct characteristics at humble costs. Further, we propose to maneuver Instruct-IPT with text instructions for better user interfaces. We have conducted experiments on Instruct-IPT to demonstrate the effectiveness of our method on manifold tasks, and we have effectively extended our method to diffusion denoisers as well. The code is available at https://github.com/huawei-noah/Pretrained-IPT.</p>
            <p id="subjects-2407.00676" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.00676" class="metainfo date"><strong>Publish</strong>: 2024-06-30 12:13:34 UTC</p>
            <div id="pdf-container-2407.00676" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.00676" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.00676" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2406.19703" class="panel paper" keywords="dehazing,routing,ksformer,mkra,lfpm,key,select,attention,multi,frequency">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2406.19703" target="_blank" title="26/298"><span class="index notranslate">#26</span></a>
                <a id="title-2406.19703" class="title-link" href="/arxiv/2406.19703" target="_blank">Vision Transformer with Key-select Routing Attention for Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2406.19703" class="title-pdf notranslate" onclick="togglePdf('2406.19703', 'https://arxiv.org/pdf/2406.19703', this)" style="color: purple;">[PDF<sup id="pdf-stars-2406.19703">5</sup>]</a>
                <a id="copy-2406.19703" class="title-copy notranslate" onclick="copyToClipboard('2406.19703')">[Copy]</a>
                <a id="kimi-2406.19703" class="title-kimi notranslate" onclick="toggleKimi('2406.19703', this)">[Kimi<sup id="kimi-stars-2406.19703"></sup>]</a>
                <a id="rel-2406.19703" class="title-rel notranslate" onclick="openRelatedPapers('2406.19703')">[REL]</a>
            </h2>
            <p id="authors-2406.19703" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lihan Tong" target="_blank"><span class="author notranslate">Lihan Tong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weijia Li" target="_blank"><span class="author notranslate">Weijia Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qingxia Yang" target="_blank"><span class="author notranslate">Qingxia Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liyuan Chen" target="_blank"><span class="author notranslate">Liyuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peng Chen" target="_blank"><span class="author notranslate">Peng Chen</span></a>
            </p>
            <p id="summary-2406.19703" class="summary">We present Ksformer, utilizing Multi-scale Key-select Routing Attention (MKRA) for intelligent selection of key areas through multi-channel, multi-scale windows with a top-k operator, and Lightweight Frequency Processing Module (LFPM) to enhance high-frequency features, outperforming other <mark data-markjs="true">dehazing</mark> methods in tests.</p>
            <p id="subjects-2406.19703" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2406.19703" class="metainfo date"><strong>Publish</strong>: 2024-06-28 07:28:50 UTC</p>
            <div id="pdf-container-2406.19703" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2406.19703" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2406.19703" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2406.09627" class="panel paper" keywords="robustsam,sam,anything,segment,degraded,segmentation,shot,688k,images,promptability">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2406.09627" target="_blank" title="27/298"><span class="index notranslate">#27</span></a>
                <a id="title-2406.09627" class="title-link" href="/arxiv/2406.09627" target="_blank">RobustSAM: Segment Anything Robustly on Degraded Images</a>
                <a id="pdf-2406.09627" class="title-pdf notranslate" onclick="togglePdf('2406.09627', 'https://arxiv.org/pdf/2406.09627', this)">[PDF<sup id="pdf-stars-2406.09627">8</sup>]</a>
                <a id="copy-2406.09627" class="title-copy notranslate" onclick="copyToClipboard('2406.09627')">[Copy]</a>
                <a id="kimi-2406.09627" class="title-kimi notranslate" onclick="toggleKimi('2406.09627', this)">[Kimi<sup id="kimi-stars-2406.09627">5</sup>]</a>
                <a id="rel-2406.09627" class="title-rel notranslate" onclick="openRelatedPapers('2406.09627')">[REL]</a>
            </h2>
            <p id="authors-2406.09627" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei-Ting Chen" target="_blank"><span class="author notranslate">Wei-Ting Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu-Jiet Vong" target="_blank"><span class="author notranslate">Yu-Jiet Vong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sy-Yen Kuo" target="_blank"><span class="author notranslate">Sy-Yen Kuo</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sizhuo Ma" target="_blank"><span class="author notranslate">Sizhuo Ma</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jian Wang" target="_blank"><span class="author notranslate">Jian Wang</span></a>
            </p>
            <p id="summary-2406.09627" class="summary">Segment Anything Model (SAM) has emerged as a transformative approach in image segmentation, acclaimed for its robust zero-shot segmentation capabilities and flexible prompting system. Nonetheless, its performance is challenged by images with degraded quality. Addressing this limitation, we propose the Robust Segment Anything Model (RobustSAM), which enhances SAM's performance on low-quality images while preserving its promptability and zero-shot generalization. Our method leverages the pre-trained SAM model with only marginal parameter increments and computational requirements. The additional parameters of RobustSAM can be optimized within 30 hours on eight GPUs, demonstrating its feasibility and practicality for typical research laboratories. We also introduce the Robust-Seg dataset, a collection of 688K image-mask pairs with different degradations designed to train and evaluate our model optimally. Extensive experiments across various segmentation tasks and datasets confirm RobustSAM's superior performance, especially under zero-shot conditions, underscoring its potential for extensive real-world application. Additionally, our method has been shown to effectively improve the performance of SAM-based downstream tasks such as single image <mark data-markjs="true">dehazing</mark> and deblurring.</p>
            <p id="subjects-2406.09627" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2406.09627" class="metainfo date"><strong>Publish</strong>: 2024-06-13 23:33:59 UTC</p>
            <div id="pdf-container-2406.09627" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2406.09627" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2406.09627" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2406.07966" class="panel paper" keywords="haze,rid,dehazing,world,coherence,label,unfolding,real,generator,cooperative">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2406.07966" target="_blank" title="28/298"><span class="index notranslate">#28</span></a>
                <a id="title-2406.07966" class="title-link" href="/arxiv/2406.07966" target="_blank">Real-world Image <mark data-markjs="true">Dehazing</mark> with Coherence-based Label Generator and Cooperative Unfolding Network</a>
                <a id="pdf-2406.07966" class="title-pdf notranslate" onclick="togglePdf('2406.07966', 'https://arxiv.org/pdf/2406.07966', this)" style="color: purple;">[PDF<sup id="pdf-stars-2406.07966">2</sup>]</a>
                <a id="copy-2406.07966" class="title-copy notranslate" onclick="copyToClipboard('2406.07966')">[Copy]</a>
                <a id="kimi-2406.07966" class="title-kimi notranslate" onclick="toggleKimi('2406.07966', this)">[Kimi<sup id="kimi-stars-2406.07966">1</sup>]</a>
                <a id="rel-2406.07966" class="title-rel notranslate" onclick="openRelatedPapers('2406.07966')">[REL]</a>
            </h2>
            <p id="authors-2406.07966" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chengyu Fang" target="_blank"><span class="author notranslate">Chengyu Fang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chunming He" target="_blank"><span class="author notranslate">Chunming He</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fengyang Xiao" target="_blank"><span class="author notranslate">Fengyang Xiao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yulun Zhang" target="_blank"><span class="author notranslate">Yulun Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Longxiang Tang" target="_blank"><span class="author notranslate">Longxiang Tang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuelin Zhang" target="_blank"><span class="author notranslate">Yuelin Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kai Li" target="_blank"><span class="author notranslate">Kai Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiu Li" target="_blank"><span class="author notranslate">Xiu Li</span></a>
            </p>
            <p id="summary-2406.07966" class="summary">Real-world Image <mark data-markjs="true">Dehazing</mark> (RID) aims to alleviate haze-induced degradation in real-world settings. This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data. To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details. Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training. Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions. We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks. Code will be available at \url{https://github.com/cnyvfang/CORUN-Colabator}.</p>
            <p id="subjects-2406.07966" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2406.07966" class="metainfo date"><strong>Publish</strong>: 2024-06-12 07:44:22 UTC</p>
            <div id="pdf-container-2406.07966" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2406.07966" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2406.07966" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2406.05700" class="panel paper" keywords="hdmba,dehazing,hsi,wssm,dehazemamba,haze,mamba,hyperspectral,remote,windows">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2406.05700" target="_blank" title="29/298"><span class="index notranslate">#29</span></a>
                <a id="title-2406.05700" class="title-link" href="/arxiv/2406.05700" target="_blank">HDMba: Hyperspectral Remote Sensing Imagery <mark data-markjs="true">Dehazing</mark> with State Space Model</a>
                <a id="pdf-2406.05700" class="title-pdf notranslate" onclick="togglePdf('2406.05700', 'https://arxiv.org/pdf/2406.05700', this)">[PDF<sup id="pdf-stars-2406.05700">2</sup>]</a>
                <a id="copy-2406.05700" class="title-copy notranslate" onclick="copyToClipboard('2406.05700')">[Copy]</a>
                <a id="kimi-2406.05700" class="title-kimi notranslate" onclick="toggleKimi('2406.05700', this)">[Kimi<sup id="kimi-stars-2406.05700"></sup>]</a>
                <a id="rel-2406.05700" class="title-rel notranslate" onclick="openRelatedPapers('2406.05700')">[REL]</a>
            </h2>
            <p id="authors-2406.05700" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hang Fu" target="_blank"><span class="author notranslate">Hang Fu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Genyun Sun" target="_blank"><span class="author notranslate">Genyun Sun</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yinhe Li" target="_blank"><span class="author notranslate">Yinhe Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinchang Ren" target="_blank"><span class="author notranslate">Jinchang Ren</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aizhu Zhang" target="_blank"><span class="author notranslate">Aizhu Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cheng Jing" target="_blank"><span class="author notranslate">Cheng Jing</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pedram Ghamisi" target="_blank"><span class="author notranslate">Pedram Ghamisi</span></a>
            </p>
            <p id="summary-2406.05700" class="summary">Haze contamination in hyperspectral remote sensing images (HSI) can lead to spatial visibility degradation and spectral distortion. Haze in HSI exhibits spatial irregularity and inhomogeneous spectral distribution, with few <mark data-markjs="true">dehazing</mark> networks available. Current CNN and Transformer-based <mark data-markjs="true">dehazing</mark> methods fail to balance global scene recovery, local detail retention, and computational efficiency. Inspired by the ability of Mamba to model long-range dependencies with linear complexity, we explore its potential for HSI <mark data-markjs="true">dehazing</mark> and propose the first HSI <mark data-markjs="true">Dehazing</mark> Mamba (HDMba) network. Specifically, we design a novel window selective scan module (WSSM) that captures local dependencies within windows and global correlations between windows by partitioning them. This approach improves the ability of conventional Mamba in local feature extraction. By modeling the local and global spectral-spatial information flow, we achieve a comprehensive analysis of hazy regions. The DehazeMamba layer (DML), constructed by WSSM, and residual DehazeMamba (RDM) blocks, composed of DMLs, are the core components of the HDMba framework. These components effectively characterize the complex distribution of haze in HSIs, aiding in scene reconstruction and <mark data-markjs="true">dehazing</mark>. Experimental results on the Gaofen-5 HSI dataset demonstrate that HDMba outperforms other state-of-the-art methods in <mark data-markjs="true">dehazing</mark> performance. The code will be available at https://github.com/RsAI-lab/HDMba.</p>
            <p id="subjects-2406.05700" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2406.05700" class="metainfo date"><strong>Publish</strong>: 2024-06-09 08:53:02 UTC</p>
            <div id="pdf-container-2406.05700" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2406.05700" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2406.05700" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2406.00629" class="panel paper" keywords="uhdformer,resolution,uhd,restoration,dualcmt,low,image,space,features,acm">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2406.00629" target="_blank" title="30/298"><span class="index notranslate">#30</span></a>
                <a id="title-2406.00629" class="title-link" href="/arxiv/2406.00629" target="_blank">Correlation Matching Transformation Transformers for UHD Image Restoration</a>
                <a id="pdf-2406.00629" class="title-pdf notranslate" onclick="togglePdf('2406.00629', 'https://arxiv.org/pdf/2406.00629', this)">[PDF<sup id="pdf-stars-2406.00629"></sup>]</a>
                <a id="copy-2406.00629" class="title-copy notranslate" onclick="copyToClipboard('2406.00629')">[Copy]</a>
                <a id="kimi-2406.00629" class="title-kimi notranslate" onclick="toggleKimi('2406.00629', this)">[Kimi<sup id="kimi-stars-2406.00629"></sup>]</a>
                <a id="rel-2406.00629" class="title-rel notranslate" onclick="openRelatedPapers('2406.00629')">[REL]</a>
            </h2>
            <p id="authors-2406.00629" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cong Wang" target="_blank"><span class="author notranslate">Cong Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinshan Pan" target="_blank"><span class="author notranslate">Jinshan Pan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Wang" target="_blank"><span class="author notranslate">Wei Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gang Fu" target="_blank"><span class="author notranslate">Gang Fu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Siyuan Liang" target="_blank"><span class="author notranslate">Siyuan Liang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mengzhu Wang" target="_blank"><span class="author notranslate">Mengzhu Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiao-Ming Wu" target="_blank"><span class="author notranslate">Xiao-Ming Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Liu" target="_blank"><span class="author notranslate">Jun Liu</span></a>
            </p>
            <p id="summary-2406.00629" class="summary">This paper proposes UHDformer, a general Transformer for Ultra-High-Definition (UHD) image restoration. UHDformer contains two learning spaces: (a) learning in high-resolution space and (b) learning in low-resolution space. The former learns multi-level high-resolution features and fuses low-high features and reconstructs the residual images, while the latter explores more representative features learning from the high-resolution ones to facilitate better restoration. To better improve feature representation in low-resolution space, we propose to build feature transformation from the high-resolution space to the low-resolution one. To that end, we propose two new modules: Dual-path Correlation Matching Transformation module (DualCMT) and Adaptive Channel Modulator (ACM). The DualCMT selects top C/r (r is greater or equal to 1 which controls the squeezing level) correlation channels from the max-pooling/mean-pooling high-resolution features to replace low-resolution ones in Transformers, which can effectively squeeze useless content to improve the feature representation in low-resolution space to facilitate better recovery. The ACM is exploited to adaptively modulate multi-level high-resolution features, enabling to provide more useful features to low-resolution space for better learning. Experimental results show that our UHDformer reduces about ninety-seven percent model sizes compared with most state-of-the-art methods while significantly improving performance under different training sets on 3 UHD image restoration tasks, including low-light image enhancement, image <mark data-markjs="true">dehazing</mark>, and image deblurring. The source codes will be made available at https://github.com/supersupercong/UHDformer.</p>
            <p id="subjects-2406.00629" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2406.00629" class="metainfo date"><strong>Publish</strong>: 2024-06-02 06:10:48 UTC</p>
            <div id="pdf-container-2406.00629" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2406.00629" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2406.00629" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2407.05169" class="panel paper" keywords="dehazing,dehazedct,deformable,homogeneous,transformer,movingforward100,convolutional,hazy,capability,retinex">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2407.05169" target="_blank" title="31/298"><span class="index notranslate">#31</span></a>
                <a id="title-2407.05169" class="title-link" href="/arxiv/2407.05169" target="_blank">DehazeDCT: Towards Effective Non-Homogeneous <mark data-markjs="true">Dehazing</mark> via Deformable Convolutional Transformer</a>
                <a id="pdf-2407.05169" class="title-pdf notranslate" onclick="togglePdf('2407.05169', 'https://arxiv.org/pdf/2407.05169', this)">[PDF<sup id="pdf-stars-2407.05169"></sup>]</a>
                <a id="copy-2407.05169" class="title-copy notranslate" onclick="copyToClipboard('2407.05169')">[Copy]</a>
                <a id="kimi-2407.05169" class="title-kimi notranslate" onclick="toggleKimi('2407.05169', this)">[Kimi<sup id="kimi-stars-2407.05169"></sup>]</a>
                <a id="rel-2407.05169" class="title-rel notranslate" onclick="openRelatedPapers('2407.05169')">[REL]</a>
            </h2>
            <p id="authors-2407.05169" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Dong" target="_blank"><span class="author notranslate">Wei Dong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Han Zhou" target="_blank"><span class="author notranslate">Han Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruiyi Wang" target="_blank"><span class="author notranslate">Ruiyi Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaohong Liu" target="_blank"><span class="author notranslate">Xiaohong Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guangtao Zhai" target="_blank"><span class="author notranslate">Guangtao Zhai</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Chen" target="_blank"><span class="author notranslate">Jun Chen</span></a>
            </p>
            <p id="summary-2407.05169" class="summary">Image <mark data-markjs="true">dehazing</mark>, a pivotal task in low-level vision, aims to restore the visibility and detail from hazy images. Many deep learning methods with powerful representation learning capability demonstrate advanced performance on non-homogeneous <mark data-markjs="true">dehazing</mark>, however, these methods usually struggle with processing high-resolution images (e.g., <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4000</mn><mo>&amp;#x00D7;</mo><mn>6000</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 6.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.208em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.354em, 1005.17em, 2.366em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3" style="font-family: MathJax_Main;">4000</span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main; padding-left: 0.222em;"></span><span class="mn" id="MathJax-Span-5" style="font-family: MathJax_Main; padding-left: 0.222em;">6000</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.089em; border-left: 0px solid; width: 0px; height: 0.964em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4000</mn><mo></mo><mn>6000</mn></math></span></span><script type="math/tex" id="MathJax-Element-1">4000 \times 6000</script>) due to their heavy computational demands. To address these challenges, we introduce an innovative non-homogeneous <mark data-markjs="true">Dehazing</mark> method via Deformable Convolutional Transformer-like architecture (DehazeDCT). Specifically, we first design a transformer-like network based on deformable convolution v4, which offers long-range dependency and adaptive spatial aggregation capabilities and demonstrates faster convergence and forward speed. Furthermore, we leverage a lightweight Retinex-inspired transformer to achieve color correction and structure refinement. Extensive experiment results and highly competitive performance of our method in NTIRE 2024 Dense and Non-Homogeneous <mark data-markjs="true">Dehazing</mark> Challenge, ranking second among all 16 submissions, demonstrate the superior capability of our proposed method. The code is available: https://github.com/movingforward100/<mark data-markjs="true">Dehazing</mark>_R.</p>
            <p id="subjects-2407.05169" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2407.05169" class="metainfo date"><strong>Publish</strong>: 2024-05-24 10:59:18 UTC</p>
            <div id="pdf-container-2407.05169" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2407.05169" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2407.05169" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.15817" class="panel paper" keywords="cl2s,dehazing,dm2f,elementary,yesianrohn,ablation,rethinking,image,model,haze">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.15817" target="_blank" title="32/298"><span class="index notranslate">#32</span></a>
                <a id="title-2405.15817" class="title-link" href="/arxiv/2405.15817" target="_blank">Rethinking the Elementary Function Fusion for Single-Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2405.15817" class="title-pdf notranslate" onclick="togglePdf('2405.15817', 'https://arxiv.org/pdf/2405.15817', this)">[PDF<sup id="pdf-stars-2405.15817"></sup>]</a>
                <a id="copy-2405.15817" class="title-copy notranslate" onclick="copyToClipboard('2405.15817')">[Copy]</a>
                <a id="kimi-2405.15817" class="title-kimi notranslate" onclick="toggleKimi('2405.15817', this)">[Kimi<sup id="kimi-stars-2405.15817">1</sup>]</a>
                <a id="rel-2405.15817" class="title-rel notranslate" onclick="openRelatedPapers('2405.15817')">[REL]</a>
            </h2>
            <p id="authors-2405.15817" class="metainfo authors notranslate"><strong>Author</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yesian Rohn" target="_blank"><span class="author notranslate">Yesian Rohn</span></a>
            </p>
            <p id="summary-2405.15817" class="summary">This paper addresses the limitations of physical models in the current field of image <mark data-markjs="true">dehazing</mark> by proposing an innovative <mark data-markjs="true">dehazing</mark> network (CL2S). Building on the DM2F model, it identifies issues in its ablation experiments and replaces the original logarithmic function model with a trigonometric (sine) model. This substitution aims to better fit the complex and variable distribution of haze. The approach also integrates the atmospheric scattering model and other elementary functions to enhance <mark data-markjs="true">dehazing</mark> performance. Experimental results demonstrate that CL2S achieves outstanding performance on multiple <mark data-markjs="true">dehazing</mark> datasets, particularly in maintaining image details and color authenticity. Additionally, systematic ablation experiments supplementing DM2F validate the concerns raised about DM2F and confirm the necessity and effectiveness of the functional components in the proposed CL2S model. Our code is available at \url{https://github.com/YesianRohn/CL2S}, where the corresponding pre-trained models can also be accessed.</p>
            <p id="subjects-2405.15817" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.15817" class="metainfo date"><strong>Publish</strong>: 2024-05-23 02:58:14 UTC</p>
            <div id="pdf-container-2405.15817" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.15817" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.15817" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.12265" class="panel paper" keywords="cie,xyz,srgb,paired,images,color,sel,linear,ssl,srgb2xyz">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.12265" target="_blank" title="33/298"><span class="index notranslate">#33</span></a>
                <a id="title-2405.12265" class="title-link" href="/arxiv/2405.12265" target="_blank">SEL-CIE: Knowledge-Guided Self-Supervised Learning Framework for CIE-XYZ Reconstruction from Non-Linear sRGB Images</a>
                <a id="pdf-2405.12265" class="title-pdf notranslate" onclick="togglePdf('2405.12265', 'https://arxiv.org/pdf/2405.12265', this)">[PDF<sup id="pdf-stars-2405.12265"></sup>]</a>
                <a id="copy-2405.12265" class="title-copy notranslate" onclick="copyToClipboard('2405.12265')">[Copy]</a>
                <a id="kimi-2405.12265" class="title-kimi notranslate" onclick="toggleKimi('2405.12265', this)">[Kimi<sup id="kimi-stars-2405.12265">2</sup>]</a>
                <a id="rel-2405.12265" class="title-rel notranslate" onclick="openRelatedPapers('2405.12265')">[REL]</a>
            </h2>
            <p id="authors-2405.12265" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shir Barzel" target="_blank"><span class="author notranslate">Shir Barzel</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Moshe Salhov" target="_blank"><span class="author notranslate">Moshe Salhov</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ofir Lindenbaum" target="_blank"><span class="author notranslate">Ofir Lindenbaum</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Amir Averbuch" target="_blank"><span class="author notranslate">Amir Averbuch</span></a>
            </p>
            <p id="summary-2405.12265" class="summary">Modern cameras typically offer two types of image states: a minimally processed linear raw RGB image representing the raw sensor data, and a highly-processed non-linear image state, such as the sRGB state. The CIE-XYZ color space is a device-independent linear space used as part of the camera pipeline and can be helpful for computer vision tasks, such as image deblurring, <mark data-markjs="true">dehazing</mark>, and color recognition tasks in medical applications, where color accuracy is important. However, images are usually saved in non-linear states, and achieving CIE-XYZ color images using conventional methods is not always possible. To tackle this issue, classical methodologies have been developed that focus on reversing the acquisition pipeline. More recently, supervised learning has been employed, using paired CIE-XYZ and sRGB representations of identical images. However, obtaining a large-scale dataset of CIE-XYZ and sRGB pairs can be challenging. To overcome this limitation and mitigate the reliance on large amounts of paired data, self-supervised learning (SSL) can be utilized as a substitute for relying solely on paired data. This paper proposes a framework for using SSL methods alongside paired data to reconstruct CIE-XYZ images and re-render sRGB images, outperforming existing approaches. The proposed framework is applied to the sRGB2XYZ dataset.</p>
            <p id="subjects-2405.12265" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/eess.IV" target="_blank"><span class="subject"><strong>Image and Video Processing</strong></span></a>
                ; <a href="/arxiv/cs.CV" target="_blank"><span class="subject">Computer Vision and Pattern Recognition</span></a>
            </p>
            <p id="date-2405.12265" class="metainfo date"><strong>Publish</strong>: 2024-05-20 17:20:41 UTC</p>
            <div id="pdf-container-2405.12265" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.12265" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.12265" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.10030" class="panel paper" keywords="rsdehamba,rsid,rsdhamba,haze,ssm,mamba,dehazing,remote,sensing,lightweight">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.10030" target="_blank" title="34/298"><span class="index notranslate">#34</span></a>
                <a id="title-2405.10030" class="title-link" href="/arxiv/2405.10030" target="_blank">RSDehamba: Lightweight Vision Mamba for Remote Sensing Satellite Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2405.10030" class="title-pdf notranslate" onclick="togglePdf('2405.10030', 'https://arxiv.org/pdf/2405.10030', this)">[PDF<sup id="pdf-stars-2405.10030">3</sup>]</a>
                <a id="copy-2405.10030" class="title-copy notranslate" onclick="copyToClipboard('2405.10030')">[Copy]</a>
                <a id="kimi-2405.10030" class="title-kimi notranslate" onclick="toggleKimi('2405.10030', this)">[Kimi<sup id="kimi-stars-2405.10030">5</sup>]</a>
                <a id="rel-2405.10030" class="title-rel notranslate" onclick="openRelatedPapers('2405.10030')">[REL]</a>
            </h2>
            <p id="authors-2405.10030" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huiling Zhou" target="_blank"><span class="author notranslate">Huiling Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xianhao Wu" target="_blank"><span class="author notranslate">Xianhao Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongming Chen" target="_blank"><span class="author notranslate">Hongming Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiang Chen" target="_blank"><span class="author notranslate">Xiang Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xin He" target="_blank"><span class="author notranslate">Xin He</span></a>
            </p>
            <p id="summary-2405.10030" class="summary">Remote sensing image <mark data-markjs="true">dehazing</mark> (RSID) aims to remove nonuniform and physically irregular haze factors for high-quality image restoration. The emergence of CNNs and Transformers has taken extraordinary strides in the RSID arena. However, these methods often struggle to demonstrate the balance of adequate long-range dependency modeling and maintaining computational efficiency. To this end, we propose the first lightweight network on the mamba-based model called RSDhamba in the field of RSID. Greatly inspired by the recent rise of Selective State Space Model (SSM) for its superior performance in modeling linear complexity and remote dependencies, our designed RSDehamba integrates the SSM framework into the U-Net architecture. Specifically, we propose the Vision Dehamba Block (VDB) as the core component of the overall network, which utilizes the linear complexity of SSM to achieve the capability of global context encoding. Simultaneously, the Direction-aware Scan Module (DSM) is designed to dynamically aggregate feature exchanges over different directional domains to effectively enhance the flexibility of sensing the spatially varying distribution of haze. In this way, our RSDhamba fully demonstrates the superiority of spatial distance capture dependencies and channel information exchange for better extraction of haze features. Extensive experimental results on widely used benchmarks validate the surpassing performance of our RSDehamba against existing state-of-the-art methods.</p>
            <p id="subjects-2405.10030" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.10030" class="metainfo date"><strong>Publish</strong>: 2024-05-16 12:12:07 UTC</p>
            <div id="pdf-container-2405.10030" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.10030" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.10030" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.09996" class="panel paper" keywords="dehazing,video,driving,aligned,hazy,frames,clear,reference,goprohazy,cosine">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.09996" target="_blank" title="35/298"><span class="index notranslate">#35</span></a>
                <a id="title-2405.09996" class="title-link" href="/arxiv/2405.09996" target="_blank">Driving-Video <mark data-markjs="true">Dehazing</mark> with Non-Aligned Regularization for Safety Assistance</a>
                <a id="pdf-2405.09996" class="title-pdf notranslate" onclick="togglePdf('2405.09996', 'https://arxiv.org/pdf/2405.09996', this)">[PDF<sup id="pdf-stars-2405.09996">1</sup>]</a>
                <a id="copy-2405.09996" class="title-copy notranslate" onclick="copyToClipboard('2405.09996')">[Copy]</a>
                <a id="kimi-2405.09996" class="title-kimi notranslate" onclick="toggleKimi('2405.09996', this)">[Kimi<sup id="kimi-stars-2405.09996">4</sup>]</a>
                <a id="rel-2405.09996" class="title-rel notranslate" onclick="openRelatedPapers('2405.09996')">[REL]</a>
            </h2>
            <p id="authors-2405.09996" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junkai Fan" target="_blank"><span class="author notranslate">Junkai Fan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiangwei Weng" target="_blank"><span class="author notranslate">Jiangwei Weng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kun Wang" target="_blank"><span class="author notranslate">Kun Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yijun Yang" target="_blank"><span class="author notranslate">Yijun Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianjun Qian" target="_blank"><span class="author notranslate">Jianjun Qian</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Li" target="_blank"><span class="author notranslate">Jun Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jian Yang" target="_blank"><span class="author notranslate">Jian Yang</span></a>
            </p>
            <p id="summary-2405.09996" class="summary">Real driving-video <mark data-markjs="true">dehazing</mark> poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions. In this paper, we propose a pioneering approach that addresses this challenge through a nonaligned regularization strategy. Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video <mark data-markjs="true">dehazing</mark> network. Our approach comprises two key components: reference matching and video <mark data-markjs="true">dehazing</mark>. Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos. Video <mark data-markjs="true">dehazing</mark> incorporates flow-guided cosine attention sampler and deformable cosine attention fusion modules to enhance spatial multiframe alignment and fuse their improved information. To validate our approach, we collect a GoProHazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments. Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video <mark data-markjs="true">dehazing</mark>. Project page.</p>
            <p id="subjects-2405.09996" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.09996" class="metainfo date"><strong>Publish</strong>: 2024-05-16 11:28:01 UTC</p>
            <div id="pdf-container-2405.09996" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.09996" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.09996" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.09083" class="panel paper" keywords="rshazediff,ddpm,remote,dehazing,sensing,images,hazy,diffusion,haze,fourier">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.09083" target="_blank" title="36/298"><span class="index notranslate">#36</span></a>
                <a id="title-2405.09083" class="title-link" href="/arxiv/2405.09083" target="_blank">RSHazeDiff: A Unified Fourier-aware Diffusion Model for Remote Sensing Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2405.09083" class="title-pdf notranslate" onclick="togglePdf('2405.09083', 'https://arxiv.org/pdf/2405.09083', this)">[PDF<sup id="pdf-stars-2405.09083">2</sup>]</a>
                <a id="copy-2405.09083" class="title-copy notranslate" onclick="copyToClipboard('2405.09083')">[Copy]</a>
                <a id="kimi-2405.09083" class="title-kimi notranslate" onclick="toggleKimi('2405.09083', this)">[Kimi<sup id="kimi-stars-2405.09083">4</sup>]</a>
                <a id="rel-2405.09083" class="title-rel notranslate" onclick="openRelatedPapers('2405.09083')">[REL]</a>
            </h2>
            <p id="authors-2405.09083" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiamei Xiong" target="_blank"><span class="author notranslate">Jiamei Xiong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xuefeng Yan" target="_blank"><span class="author notranslate">Xuefeng Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yongzhen Wang" target="_blank"><span class="author notranslate">Yongzhen Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Zhao" target="_blank"><span class="author notranslate">Wei Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiao-Ping Zhang" target="_blank"><span class="author notranslate">Xiao-Ping Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingqiang Wei" target="_blank"><span class="author notranslate">Mingqiang Wei</span></a>
            </p>
            <p id="summary-2405.09083" class="summary">Haze severely degrades the visual quality of remote sensing images and hampers the performance of automotive navigation, intelligent monitoring, and urban management. The emerging denoising diffusion probabilistic model (DDPM) exhibits the significant potential for dense haze removal with its strong generation ability. Since remote sensing images contain extensive small-scale texture structures, it is important to effectively restore image details from hazy images. However, current wisdom of DDPM fails to preserve image details and color fidelity well, limiting its <mark data-markjs="true">dehazing</mark> capacity for remote sensing images. In this paper, we propose a novel unified Fourier-aware diffusion model for remote sensing image <mark data-markjs="true">dehazing</mark>, termed RSHazeDiff. From a new perspective, RSHazeDiff explores the conditional DDPM to improve image quality in dense hazy scenarios, and it makes three key contributions. First, RSHazeDiff refines the training phase of diffusion process by performing noise estimation and reconstruction constraints in a coarse-to-fine fashion. Thus, it remedies the unpleasing results caused by the simple noise estimation constraint in DDPM. Second, by taking the frequency information as important prior knowledge during iterative sampling steps, RSHazeDiff can preserve more texture details and color fidelity in dehazed images. Third, we design a global compensated learning module to utilize the Fourier transform to capture the global dependency features of input images, which can effectively mitigate the effects of boundary artifacts when processing fixed-size patches. Experiments on both synthetic and real-world benchmarks validate the favorable performance of RSHazeDiff over multiple state-of-the-art methods. Source code will be released at https://github.com/jm-xiong/RSHazeDiff.</p>
            <p id="subjects-2405.09083" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.09083" class="metainfo date"><strong>Publish</strong>: 2024-05-15 04:22:27 UTC</p>
            <div id="pdf-container-2405.09083" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.09083" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.09083" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.07520" class="panel paper" keywords="dehazing,uav,remote,sensing,approaches,haze,imagery,review,benchmarked,datasets">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.07520" target="_blank" title="37/298"><span class="index notranslate">#37</span></a>
                <a id="title-2405.07520" class="title-link" href="/arxiv/2405.07520" target="_blank"><mark data-markjs="true">Dehazing</mark> Remote Sensing and UAV Imagery: A Review of Deep Learning, Prior-based, and Hybrid Approaches</a>
                <a id="pdf-2405.07520" class="title-pdf notranslate" onclick="togglePdf('2405.07520', 'https://arxiv.org/pdf/2405.07520', this)">[PDF<sup id="pdf-stars-2405.07520">2</sup>]</a>
                <a id="copy-2405.07520" class="title-copy notranslate" onclick="copyToClipboard('2405.07520')">[Copy]</a>
                <a id="kimi-2405.07520" class="title-kimi notranslate" onclick="toggleKimi('2405.07520', this)">[Kimi<sup id="kimi-stars-2405.07520">2</sup>]</a>
                <a id="rel-2405.07520" class="title-rel notranslate" onclick="openRelatedPapers('2405.07520')">[REL]</a>
            </h2>
            <p id="authors-2405.07520" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gao Yu Lee" target="_blank"><span class="author notranslate">Gao Yu Lee</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinkuan Chen" target="_blank"><span class="author notranslate">Jinkuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tanmoy Dam" target="_blank"><span class="author notranslate">Tanmoy Dam</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Md Meftahul Ferdaus" target="_blank"><span class="author notranslate">Md Meftahul Ferdaus</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel Puiu Poenar" target="_blank"><span class="author notranslate">Daniel Puiu Poenar</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vu N Duong" target="_blank"><span class="author notranslate">Vu N Duong</span></a>
            </p>
            <p id="summary-2405.07520" class="summary">High-quality images are crucial in remote sensing and UAV applications, but atmospheric haze can severely degrade image quality, making image <mark data-markjs="true">dehazing</mark> a critical research area. Since the introduction of deep convolutional neural networks, numerous approaches have been proposed, and even more have emerged with the development of vision transformers and contrastive/few-shot learning. Simultaneously, papers describing <mark data-markjs="true">dehazing</mark> architectures applicable to various Remote Sensing (RS) domains are also being published. This review goes beyond the traditional focus on benchmarked haze datasets, as we also explore the application of <mark data-markjs="true">dehazing</mark> techniques to remote sensing and UAV datasets, providing a comprehensive overview of both deep learning and prior-based approaches in these domains. We identify key challenges, including the lack of large-scale RS datasets and the need for more robust evaluation metrics, and outline potential solutions and future research directions to address them. This review is the first, to our knowledge, to provide comprehensive discussions on both existing and very recent <mark data-markjs="true">dehazing</mark> approaches (as of 2024) on benchmarked and RS datasets, including UAV-based imagery.</p>
            <p id="subjects-2405.07520" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.07520" class="metainfo date"><strong>Publish</strong>: 2024-05-13 07:35:24 UTC</p>
            <div id="pdf-container-2405.07520" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.07520" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.07520" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2405.05811" class="panel paper" keywords="pcsa,dehazing,dependencies,pixel,strip,sizes,parallel,capturing,cross,attention">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2405.05811" target="_blank" title="38/298"><span class="index notranslate">#38</span></a>
                <a id="title-2405.05811" class="title-link" href="/arxiv/2405.05811" target="_blank">Parallel Cross Strip Attention Network for Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2405.05811" class="title-pdf notranslate" onclick="togglePdf('2405.05811', 'https://arxiv.org/pdf/2405.05811', this)">[PDF<sup id="pdf-stars-2405.05811">5</sup>]</a>
                <a id="copy-2405.05811" class="title-copy notranslate" onclick="copyToClipboard('2405.05811')">[Copy]</a>
                <a id="kimi-2405.05811" class="title-kimi notranslate" onclick="toggleKimi('2405.05811', this)">[Kimi<sup id="kimi-stars-2405.05811">6</sup>]</a>
                <a id="rel-2405.05811" class="title-rel notranslate" onclick="openRelatedPapers('2405.05811')">[REL]</a>
            </h2>
            <p id="authors-2405.05811" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lihan Tong" target="_blank"><span class="author notranslate">Lihan Tong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yun Liu" target="_blank"><span class="author notranslate">Yun Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tian Ye" target="_blank"><span class="author notranslate">Tian Ye</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weijia Li" target="_blank"><span class="author notranslate">Weijia Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liyuan Chen" target="_blank"><span class="author notranslate">Liyuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Erkang Chen" target="_blank"><span class="author notranslate">Erkang Chen</span></a>
            </p>
            <p id="summary-2405.05811" class="summary">The objective of single image <mark data-markjs="true">dehazing</mark> is to restore hazy images and produce clear, high-quality visuals. Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size. While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal <mark data-markjs="true">dehazing</mark> performance. In this study, we introduce a novel <mark data-markjs="true">dehazing</mark> network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy. PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain. To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features.</p>
            <p id="subjects-2405.05811" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2405.05811" class="metainfo date"><strong>Publish</strong>: 2024-05-09 14:50:07 UTC</p>
            <div id="pdf-container-2405.05811" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2405.05811" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2405.05811" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.17825" class="panel paper" keywords="haze,odcr,uid,dehazing,hazy,orthogonal,features,image,unrelated,unpaired">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.17825" target="_blank" title="39/298"><span class="index notranslate">#39</span></a>
                <a id="title-2404.17825" class="title-link" href="/arxiv/2404.17825" target="_blank">ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2404.17825" class="title-pdf notranslate" onclick="togglePdf('2404.17825', 'https://arxiv.org/pdf/2404.17825', this)">[PDF<sup id="pdf-stars-2404.17825">4</sup>]</a>
                <a id="copy-2404.17825" class="title-copy notranslate" onclick="copyToClipboard('2404.17825')">[Copy]</a>
                <a id="kimi-2404.17825" class="title-kimi notranslate" onclick="toggleKimi('2404.17825', this)">[Kimi<sup id="kimi-stars-2404.17825">5</sup>]</a>
                <a id="rel-2404.17825" class="title-rel notranslate" onclick="openRelatedPapers('2404.17825')">[REL]</a>
            </h2>
            <p id="authors-2404.17825" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhongze Wang" target="_blank"><span class="author notranslate">Zhongze Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haitao Zhao" target="_blank"><span class="author notranslate">Haitao Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingchao Peng" target="_blank"><span class="author notranslate">Jingchao Peng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lujian Yao" target="_blank"><span class="author notranslate">Lujian Yao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaijie Zhao" target="_blank"><span class="author notranslate">Kaijie Zhao</span></a>
            </p>
            <p id="summary-2404.17825" class="summary">Unpaired image <mark data-markjs="true">dehazing</mark> (UID) holds significant research importance due to the challenges in acquiring haze/clear image pairs with identical backgrounds. This paper proposes a novel method for UID named Orthogonal Decoupling Contrastive Regularization (ODCR). Our method is grounded in the assumption that an image consists of both haze-related features, which influence the degree of haze, and haze-unrelated features, such as texture and semantic information. ODCR aims to ensure that the haze-related features of the <mark data-markjs="true">dehazing</mark> result closely resemble those of the clear image, while the haze-unrelated features align with the input hazy image. To accomplish the motivation, Orthogonal MLPs optimized geometrically on the Stiefel manifold are proposed, which can project image features into an orthogonal space, thereby reducing the relevance between different features. Furthermore, a task-driven Depth-wise Feature Classifier (DWFC) is proposed, which assigns weights to the orthogonal features based on the contribution of each channel's feature in predicting whether the feature source is hazy or clear in a self-supervised fashion. Finally, a Weighted PatchNCE (WPNCE) loss is introduced to achieve the pulling of haze-related features in the output image toward those of clear images, while bringing haze-unrelated features close to those of the hazy input. Extensive experiments demonstrate the superior performance of our ODCR method on UID.</p>
            <p id="subjects-2404.17825" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2404.17825" class="metainfo date"><strong>Publish</strong>: 2024-04-27 08:13:13 UTC</p>
            <div id="pdf-container-2404.17825" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.17825" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.17825" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.15638" class="panel paper" keywords="dehazing,priornet,hazy,clarity,lightweight,interactive,18kb,image,generalization,attention">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.15638" target="_blank" title="40/298"><span class="index notranslate">#40</span></a>
                <a id="title-2404.15638" class="title-link" href="/arxiv/2404.15638" target="_blank">PriorNet: A Novel Lightweight Network with Multidimensional Interactive Attention for Efficient Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2404.15638" class="title-pdf notranslate" onclick="togglePdf('2404.15638', 'https://arxiv.org/pdf/2404.15638', this)">[PDF<sup id="pdf-stars-2404.15638">4</sup>]</a>
                <a id="copy-2404.15638" class="title-copy notranslate" onclick="copyToClipboard('2404.15638')">[Copy]</a>
                <a id="kimi-2404.15638" class="title-kimi notranslate" onclick="toggleKimi('2404.15638', this)">[Kimi<sup id="kimi-stars-2404.15638">6</sup>]</a>
                <a id="rel-2404.15638" class="title-rel notranslate" onclick="openRelatedPapers('2404.15638')">[REL]</a>
            </h2>
            <p id="authors-2404.15638" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yutong Chen" target="_blank"><span class="author notranslate">Yutong Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhang Wen" target="_blank"><span class="author notranslate">Zhang Wen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chao Wang" target="_blank"><span class="author notranslate">Chao Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lei Gong" target="_blank"><span class="author notranslate">Lei Gong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhongchao Yi" target="_blank"><span class="author notranslate">Zhongchao Yi</span></a>
            </p>
            <p id="summary-2404.15638" class="summary">Hazy images degrade visual quality, and <mark data-markjs="true">dehazing</mark> is a crucial prerequisite for subsequent processing tasks. Most current <mark data-markjs="true">dehazing</mark> methods rely on neural networks and face challenges such as high computational parameter pressure and weak generalization capabilities. This paper introduces PriorNet--a novel, lightweight, and highly applicable <mark data-markjs="true">dehazing</mark> network designed to significantly improve the clarity and visual quality of hazy images while avoiding excessive detail extraction issues. The core of PriorNet is the original Multi-Dimensional Interactive Attention (MIA) mechanism, which effectively captures a wide range of haze characteristics, substantially reducing the computational load and generalization difficulties associated with complex systems. By utilizing a uniform convolutional kernel size and incorporating skip connections, we have streamlined the feature extraction process. Simplifying the number of layers and architecture not only enhances <mark data-markjs="true">dehazing</mark> efficiency but also facilitates easier deployment on edge devices. Extensive testing across multiple datasets has demonstrated PriorNet's exceptional performance in <mark data-markjs="true">dehazing</mark> and clarity restoration, maintaining image detail and color fidelity in single-image <mark data-markjs="true">dehazing</mark> tasks. Notably, with a model size of just 18Kb, PriorNet showcases superior <mark data-markjs="true">dehazing</mark> generalization capabilities compared to other methods. Our research makes a significant contribution to advancing image <mark data-markjs="true">dehazing</mark> technology, providing new perspectives and tools for the field and related domains, particularly emphasizing the importance of improving universality and deployability.</p>
            <p id="subjects-2404.15638" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
            </p>
            <p id="date-2404.15638" class="metainfo date"><strong>Publish</strong>: 2024-04-24 04:20:22 UTC</p>
            <div id="pdf-container-2404.15638" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.15638" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.15638" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.09269" class="panel paper" keywords="hazy,dehazing,panet,haze,images,hazing,mapper,image,world,augmentation">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.09269" target="_blank" title="41/298"><span class="index notranslate">#41</span></a>
                <a id="title-2404.09269" class="title-link" href="/arxiv/2404.09269" target="_blank">PANet: A Physics-guided Parametric Augmentation Net for Image <mark data-markjs="true">Dehazing</mark> by Hazing</a>
                <a id="pdf-2404.09269" class="title-pdf notranslate" onclick="togglePdf('2404.09269', 'https://arxiv.org/pdf/2404.09269', this)">[PDF<sup id="pdf-stars-2404.09269">2</sup>]</a>
                <a id="copy-2404.09269" class="title-copy notranslate" onclick="copyToClipboard('2404.09269')">[Copy]</a>
                <a id="kimi-2404.09269" class="title-kimi notranslate" onclick="toggleKimi('2404.09269', this)">[Kimi<sup id="kimi-stars-2404.09269">2</sup>]</a>
                <a id="rel-2404.09269" class="title-rel notranslate" onclick="openRelatedPapers('2404.09269')">[REL]</a>
            </h2>
            <p id="authors-2404.09269" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chih-Ling Chang" target="_blank"><span class="author notranslate">Chih-Ling Chang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fu-Jen Tsai" target="_blank"><span class="author notranslate">Fu-Jen Tsai</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zi-Ling Huang" target="_blank"><span class="author notranslate">Zi-Ling Huang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lin Gu" target="_blank"><span class="author notranslate">Lin Gu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chia-Wen Lin" target="_blank"><span class="author notranslate">Chia-Wen Lin</span></a>
            </p>
            <p id="summary-2404.09269" class="summary">Image <mark data-markjs="true">dehazing</mark> faces challenges when dealing with hazy images in real-world scenarios. A huge domain gap between synthetic and real-world haze images degrades <mark data-markjs="true">dehazing</mark> performance in practical settings. However, collecting real-world image datasets for training <mark data-markjs="true">dehazing</mark> models is challenging since both hazy and clean pairs must be captured under the same conditions. In this paper, we propose a Physics-guided Parametric Augmentation Network (PANet) that generates photo-realistic hazy and clean training pairs to effectively enhance real-world <mark data-markjs="true">dehazing</mark> performance. PANet comprises a Haze-to-Parameter Mapper (HPM) to project hazy images into a parameter space and a Parameter-to-Haze Mapper (PHM) to map the resampled haze parameters back to hazy images. In the parameter space, we can pixel-wisely resample individual haze parameter maps to generate diverse hazy images with physically-explainable haze conditions unseen in the training set. Our experimental results demonstrate that PANet can augment diverse realistic hazy images to enrich existing hazy image benchmarks so as to effectively boost the performances of state-of-the-art image <mark data-markjs="true">dehazing</mark> models.</p>
            <p id="subjects-2404.09269" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2404.09269" class="metainfo date"><strong>Publish</strong>: 2024-04-14 14:24:13 UTC</p>
            <div id="pdf-container-2404.09269" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.09269" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.09269" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.07790" class="panel paper" keywords="vifnet,dehazing,infrared,airsim,visible,haze,fusion,image,information,dsfe">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.07790" target="_blank" title="42/298"><span class="index notranslate">#42</span></a>
                <a id="title-2404.07790" class="title-link" href="/arxiv/2404.07790" target="_blank">VIFNet: An End-to-end Visible-Infrared Fusion Network for Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2404.07790" class="title-pdf notranslate" onclick="togglePdf('2404.07790', 'https://arxiv.org/pdf/2404.07790', this)">[PDF<sup id="pdf-stars-2404.07790">3</sup>]</a>
                <a id="copy-2404.07790" class="title-copy notranslate" onclick="copyToClipboard('2404.07790')">[Copy]</a>
                <a id="kimi-2404.07790" class="title-kimi notranslate" onclick="toggleKimi('2404.07790', this)">[Kimi<sup id="kimi-stars-2404.07790">6</sup>]</a>
                <a id="rel-2404.07790" class="title-rel notranslate" onclick="openRelatedPapers('2404.07790')">[REL]</a>
            </h2>
            <p id="authors-2404.07790" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Meng Yu" target="_blank"><span class="author notranslate">Meng Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Te Cui" target="_blank"><span class="author notranslate">Te Cui</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoyang Lu" target="_blank"><span class="author notranslate">Haoyang Lu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yufeng Yue" target="_blank"><span class="author notranslate">Yufeng Yue</span></a>
            </p>
            <p id="summary-2404.07790" class="summary">Image <mark data-markjs="true">dehazing</mark> poses significant challenges in environmental perception. Recent research mainly focus on deep learning-based methods with single modality, while they may result in severe information loss especially in dense-haze scenarios. The infrared image exhibits robustness to the haze, however, existing methods have primarily treated the infrared modality as auxiliary information, failing to fully explore its rich information in <mark data-markjs="true">dehazing</mark>. To address this challenge, the key insight of this study is to design a visible-infrared fusion network for image <mark data-markjs="true">dehazing</mark>. In particular, we propose a multi-scale Deep Structure Feature Extraction (DSFE) module, which incorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial and marginal information within the deep structural features. Additionally, we introduce an inconsistency weighted fusion strategy to merge the two modalities by leveraging the more reliable information. To validate this, we construct a visible-infrared multimodal dataset called AirSim-VID based on the AirSim simulation platform. Extensive experiments performed on challenging real and simulated image datasets demonstrate that VIFNet can outperform many state-of-the-art competing methods. The code and dataset are available at https://github.com/mengyu212/VIFNet_<mark data-markjs="true">dehazing</mark>.</p>
            <p id="subjects-2404.07790" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2404.07790" class="metainfo date"><strong>Publish</strong>: 2024-04-11 14:31:11 UTC</p>
            <div id="pdf-container-2404.07790" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.07790" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.07790" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.02460" class="panel paper" keywords="tsnet,dehazing,msfm,stage,alm,network,image,learning,fusion,datasets">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.02460" target="_blank" title="43/298"><span class="index notranslate">#43</span></a>
                <a id="title-2404.02460" class="title-link" href="/arxiv/2404.02460" target="_blank">TSNet:A Two-stage Network for Image <mark data-markjs="true">Dehazing</mark> with Multi-scale Fusion and Adaptive Learning</a>
                <a id="pdf-2404.02460" class="title-pdf notranslate" onclick="togglePdf('2404.02460', 'https://arxiv.org/pdf/2404.02460', this)">[PDF<sup id="pdf-stars-2404.02460">2</sup>]</a>
                <a id="copy-2404.02460" class="title-copy notranslate" onclick="copyToClipboard('2404.02460')">[Copy]</a>
                <a id="kimi-2404.02460" class="title-kimi notranslate" onclick="toggleKimi('2404.02460', this)">[Kimi<sup id="kimi-stars-2404.02460">2</sup>]</a>
                <a id="rel-2404.02460" class="title-rel notranslate" onclick="openRelatedPapers('2404.02460')">[REL]</a>
            </h2>
            <p id="authors-2404.02460" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaolin Gong" target="_blank"><span class="author notranslate">Xiaolin Gong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zehan Zheng" target="_blank"><span class="author notranslate">Zehan Zheng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Heyuan Du" target="_blank"><span class="author notranslate">Heyuan Du</span></a>
            </p>
            <p id="summary-2404.02460" class="summary">Image <mark data-markjs="true">dehazing</mark> has been a popular topic of research for a long time. Previous deep learning-based image <mark data-markjs="true">dehazing</mark> methods have failed to achieve satisfactory <mark data-markjs="true">dehazing</mark> effects on both synthetic datasets and real-world datasets, exhibiting poor generalization. Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images. To address these issues, this paper proposes a two-stage image <mark data-markjs="true">dehazing</mark> network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM). Specifically, MSFM and ALM enhance the generalization of TSNet. The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives. The ALM can actively learn of regions of interest in images and restore texture details more effectively. Additionally, TSNet is designed as a two-stage network, where the first-stage network performs image <mark data-markjs="true">dehazing</mark>, and the second-stage network is employed to improve issues such as artifacts and color distortion present in the results of the first-stage network. We also change the learning objective from ground truth images to opposite fog maps, which improves the learning efficiency of TSNet. Extensive experiments demonstrate that TSNet exhibits superior <mark data-markjs="true">dehazing</mark> performance on both synthetic and real-world datasets compared to previous state-of-the-art methods.</p>
            <p id="subjects-2404.02460" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
            </p>
            <p id="date-2404.02460" class="metainfo date"><strong>Publish</strong>: 2024-04-03 05:02:46 UTC</p>
            <div id="pdf-container-2404.02460" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.02460" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.02460" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.02154" class="panel paper" keywords="dynet,restoration,image,degradations,weights,dynamic,pre,akshaydudhane16,training,bulkier">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.02154" target="_blank" title="44/298"><span class="index notranslate">#44</span></a>
                <a id="title-2404.02154" class="title-link" href="/arxiv/2404.02154" target="_blank">Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration</a>
                <a id="pdf-2404.02154" class="title-pdf notranslate" onclick="togglePdf('2404.02154', 'https://arxiv.org/pdf/2404.02154', this)">[PDF<sup id="pdf-stars-2404.02154">11</sup>]</a>
                <a id="copy-2404.02154" class="title-copy notranslate" onclick="copyToClipboard('2404.02154')">[Copy]</a>
                <a id="kimi-2404.02154" class="title-kimi notranslate" onclick="toggleKimi('2404.02154', this)">[Kimi<sup id="kimi-stars-2404.02154">10</sup>]</a>
                <a id="rel-2404.02154" class="title-rel notranslate" onclick="openRelatedPapers('2404.02154')">[REL]</a>
            </h2>
            <p id="authors-2404.02154" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Akshay Dudhane" target="_blank"><span class="author notranslate">Akshay Dudhane</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Omkar Thawakar" target="_blank"><span class="author notranslate">Omkar Thawakar</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Syed Waqas Zamir" target="_blank"><span class="author notranslate">Syed Waqas Zamir</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Salman Khan" target="_blank"><span class="author notranslate">Salman Khan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fahad Shahbaz Khan" target="_blank"><span class="author notranslate">Fahad Shahbaz Khan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ming-Hsuan Yang" target="_blank"><span class="author notranslate">Ming-Hsuan Yang</span></a>
            </p>
            <p id="summary-2404.02154" class="summary">All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples. We validate our DyNet for image denoising, deraining, and <mark data-markjs="true">dehazing</mark> in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet.</p>
            <p id="subjects-2404.02154" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2404.02154" class="metainfo date"><strong>Publish</strong>: 2024-04-02 17:58:49 UTC</p>
            <div id="pdf-container-2404.02154" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.02154" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.02154" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.01998" class="panel paper" keywords="rsfnet,enhancement,specularity,factorization,factors,light,lle,homepage,unrolling,deraining">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.01998" target="_blank" title="45/298"><span class="index notranslate">#45</span></a>
                <a id="title-2404.01998" class="title-link" href="/arxiv/2404.01998" target="_blank">Specularity Factorization for Low-Light Enhancement</a>
                <a id="pdf-2404.01998" class="title-pdf notranslate" onclick="togglePdf('2404.01998', 'https://arxiv.org/pdf/2404.01998', this)">[PDF<sup id="pdf-stars-2404.01998">3</sup>]</a>
                <a id="copy-2404.01998" class="title-copy notranslate" onclick="copyToClipboard('2404.01998')">[Copy]</a>
                <a id="kimi-2404.01998" class="title-kimi notranslate" onclick="toggleKimi('2404.01998', this)">[Kimi<sup id="kimi-stars-2404.01998">5</sup>]</a>
                <a id="rel-2404.01998" class="title-rel notranslate" onclick="openRelatedPapers('2404.01998')">[REL]</a>
            </h2>
            <p id="authors-2404.01998" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Saurabh Saini" target="_blank"><span class="author notranslate">Saurabh Saini</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=P J Narayanan" target="_blank"><span class="author notranslate">P J Narayanan</span></a>
            </p>
            <p id="summary-2404.01998" class="summary">We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and <mark data-markjs="true">dehazing</mark> with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet. The code and data is released for reproducibility on the project homepage.</p>
            <p id="subjects-2404.01998" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
            </p>
            <p id="date-2404.01998" class="metainfo date"><strong>Publish</strong>: 2024-04-02 14:41:42 UTC</p>
            <div id="pdf-container-2404.01998" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.01998" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.01998" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.01604" class="panel paper" keywords="wavedh,dehazing,wavelet,frequency,downsampling,convnet,feature,refinement,image,block">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.01604" target="_blank" title="46/298"><span class="index notranslate">#46</span></a>
                <a id="title-2404.01604" class="title-link" href="/arxiv/2404.01604" target="_blank">WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2404.01604" class="title-pdf notranslate" onclick="togglePdf('2404.01604', 'https://arxiv.org/pdf/2404.01604', this)">[PDF<sup id="pdf-stars-2404.01604"></sup>]</a>
                <a id="copy-2404.01604" class="title-copy notranslate" onclick="copyToClipboard('2404.01604')">[Copy]</a>
                <a id="kimi-2404.01604" class="title-kimi notranslate" onclick="toggleKimi('2404.01604', this)">[Kimi<sup id="kimi-stars-2404.01604">2</sup>]</a>
                <a id="rel-2404.01604" class="title-rel notranslate" onclick="openRelatedPapers('2404.01604')">[REL]</a>
            </h2>
            <p id="authors-2404.01604" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seongmin Hwang" target="_blank"><span class="author notranslate">Seongmin Hwang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daeyoung Han" target="_blank"><span class="author notranslate">Daeyoung Han</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cheolkon Jung" target="_blank"><span class="author notranslate">Cheolkon Jung</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Moongu Jeon" target="_blank"><span class="author notranslate">Moongu Jeon</span></a>
            </p>
            <p id="summary-2404.01604" class="summary">The surge in interest regarding image <mark data-markjs="true">dehazing</mark> has led to notable advancements in deep learning-based single image <mark data-markjs="true">dehazing</mark> approaches, exhibiting impressive performance in recent studies. Despite these strides, many existing methods fall short in meeting the efficiency demands of practical applications. In this paper, we introduce WaveDH, a novel and compact ConvNet designed to address this efficiency gap in image <mark data-markjs="true">dehazing</mark>. Our WaveDH leverages wavelet sub-bands for guided up-and-downsampling and frequency-aware feature refinement. The key idea lies in utilizing wavelet decomposition to extract low-and-high frequency components from feature levels, allowing for faster processing while upholding high-quality reconstruction. The downsampling block employs a novel squeeze-and-attention scheme to optimize the feature downsampling process in a structurally compact manner through wavelet domain learning, preserving discriminative features while discarding noise components. In our upsampling block, we introduce a dual-upsample and fusion mechanism to enhance high-frequency component awareness, aiding in the reconstruction of high-frequency details. Departing from conventional <mark data-markjs="true">dehazing</mark> methods that treat low-and-high frequency components equally, our feature refinement block strategically processes features with a frequency-aware approach. By employing a coarse-to-fine methodology, it not only refines the details at frequency levels but also significantly optimizes computational costs. The refinement is performed in a maximum 8x downsampled feature space, striking a favorable efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our method, WaveDH, outperforms many state-of-the-art methods on several image <mark data-markjs="true">dehazing</mark> benchmarks with significantly reduced computational costs. Our code is available at https://github.com/AwesomeHwang/WaveDH.</p>
            <p id="subjects-2404.01604" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2404.01604" class="metainfo date"><strong>Publish</strong>: 2024-04-02 02:52:05 UTC</p>
            <div id="pdf-container-2404.01604" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.01604" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.01604" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2404.00288" class="panel paper" keywords="prompt,restoration,frequency,fpro,modulator,image,joshyzhou,prompting,deraindrop,seeing">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.00288" target="_blank" title="47/298"><span class="index notranslate">#47</span></a>
                <a id="title-2404.00288" class="title-link" href="/arxiv/2404.00288" target="_blank">Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration</a>
                <a id="pdf-2404.00288" class="title-pdf notranslate" onclick="togglePdf('2404.00288', 'https://arxiv.org/pdf/2404.00288', this)">[PDF<sup id="pdf-stars-2404.00288">3</sup>]</a>
                <a id="copy-2404.00288" class="title-copy notranslate" onclick="copyToClipboard('2404.00288')">[Copy]</a>
                <a id="kimi-2404.00288" class="title-kimi notranslate" onclick="toggleKimi('2404.00288', this)">[Kimi<sup id="kimi-stars-2404.00288">3</sup>]</a>
                <a id="rel-2404.00288" class="title-rel notranslate" onclick="openRelatedPapers('2404.00288')">[REL]</a>
            </h2>
            <p id="authors-2404.00288" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shihao Zhou" target="_blank"><span class="author notranslate">Shihao Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinshan Pan" target="_blank"><span class="author notranslate">Jinshan Pan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinglei Shi" target="_blank"><span class="author notranslate">Jinglei Shi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Duosheng Chen" target="_blank"><span class="author notranslate">Duosheng Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lishen Qu" target="_blank"><span class="author notranslate">Lishen Qu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jufeng Yang" target="_blank"><span class="author notranslate">Jufeng Yang</span></a>
            </p>
            <p id="summary-2404.00288" class="summary">How to explore useful features from images as prompts to guide the deep image restoration models is an effective way to solve image restoration. In contrast to mining spatial relations within images as prompt, which leads to characteristics of different frequencies being neglected and further remaining subtle or undetectable artifacts in the restored image, we develop a Frequency Prompting image restoration method, dubbed FPro, which can effectively provide prompt components from a frequency perspective to guild the restoration model address these differences. Specifically, we first decompose input features into separate frequency parts via dynamically learned filters, where we introduce a gating mechanism for suppressing the less informative elements within the kernels. To propagate useful frequency information as prompt, we then propose a dual prompt block, consisting of a low-frequency prompt modulator (LPM) and a high-frequency prompt modulator (HPM), to handle signals from different bands respectively. Each modulator contains a generation process to incorporate prompting components into the extracted frequency maps, and a modulation part that modifies the prompt feature with the guidance of the decoder features. Experimental results on commonly used benchmarks have demonstrated the favorable performance of our pipeline against SOTA methods on 5 image restoration tasks, including deraining, deraindrop, demoiring, deblurring, and <mark data-markjs="true">dehazing</mark>. The source code and pre-trained models will be available at https://github.com/joshyZhou/FPro.</p>
            <p id="subjects-2404.00288" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2404.00288" class="metainfo date"><strong>Publish</strong>: 2024-03-30 08:42:34 UTC</p>
            <div id="pdf-container-2404.00288" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.00288" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2404.00288" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.18548" class="panel paper" keywords="nighttime,dehazing,brightness,haze,frequency,daytime,glow,sfsnid,semi,supervised">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.18548" target="_blank" title="48/298"><span class="index notranslate">#48</span></a>
                <a id="title-2403.18548" class="title-link" href="/arxiv/2403.18548" target="_blank">A Semi-supervised Nighttime <mark data-markjs="true">Dehazing</mark> Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint</a>
                <a id="pdf-2403.18548" class="title-pdf notranslate" onclick="togglePdf('2403.18548', 'https://arxiv.org/pdf/2403.18548', this)">[PDF<sup id="pdf-stars-2403.18548"></sup>]</a>
                <a id="copy-2403.18548" class="title-copy notranslate" onclick="copyToClipboard('2403.18548')">[Copy]</a>
                <a id="kimi-2403.18548" class="title-kimi notranslate" onclick="toggleKimi('2403.18548', this)">[Kimi<sup id="kimi-stars-2403.18548"></sup>]</a>
                <a id="rel-2403.18548" class="title-rel notranslate" onclick="openRelatedPapers('2403.18548')">[REL]</a>
            </h2>
            <p id="authors-2403.18548" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaofeng Cong" target="_blank"><span class="author notranslate">Xiaofeng Cong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie Gui" target="_blank"><span class="author notranslate">Jie Gui</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jing Zhang" target="_blank"><span class="author notranslate">Jing Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junming Hou" target="_blank"><span class="author notranslate">Junming Hou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao Shen" target="_blank"><span class="author notranslate">Hao Shen</span></a>
            </p>
            <p id="summary-2403.18548" class="summary">Existing research based on deep learning has extensively explored the problem of daytime image <mark data-markjs="true">dehazing</mark>. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a <mark data-markjs="true">dehazing</mark> model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime <mark data-markjs="true">dehazing</mark>. First, the spatial attention and frequency spectrum filtering are implemented as a spatial-frequency domain information interaction module to handle the first issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source code and Supplementary Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.</p>
            <p id="subjects-2403.18548" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.18548" class="metainfo date"><strong>Publish</strong>: 2024-03-27 13:27:02 UTC</p>
            <div id="pdf-container-2403.18548" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.18548" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.18548" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.14614" class="panel paper" keywords="restoration,adair,frequency,degradation,degradations,different,image,subbands,input,mining">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.14614" target="_blank" title="49/298"><span class="index notranslate">#49</span></a>
                <a id="title-2403.14614" class="title-link" href="/arxiv/2403.14614" target="_blank">AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation</a>
                <a id="pdf-2403.14614" class="title-pdf notranslate" onclick="togglePdf('2403.14614', 'https://arxiv.org/pdf/2403.14614', this)">[PDF<sup id="pdf-stars-2403.14614">5</sup>]</a>
                <a id="copy-2403.14614" class="title-copy notranslate" onclick="copyToClipboard('2403.14614')">[Copy]</a>
                <a id="kimi-2403.14614" class="title-kimi notranslate" onclick="toggleKimi('2403.14614', this)">[Kimi<sup id="kimi-stars-2403.14614">10</sup>]</a>
                <a id="rel-2403.14614" class="title-rel notranslate" onclick="openRelatedPapers('2403.14614')">[REL]</a>
            </h2>
            <p id="authors-2403.14614" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuning Cui" target="_blank"><span class="author notranslate">Yuning Cui</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Syed Waqas Zamir" target="_blank"><span class="author notranslate">Syed Waqas Zamir</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Salman Khan" target="_blank"><span class="author notranslate">Salman Khan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alois Knoll" target="_blank"><span class="author notranslate">Alois Knoll</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mubarak Shah" target="_blank"><span class="author notranslate">Mubarak Shah</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fahad Shahbaz Khan" target="_blank"><span class="author notranslate">Fahad Shahbaz Khan</span></a>
            </p>
            <p id="summary-2403.14614" class="summary">In the image acquisition process, various forms of degradation, including noise, haze, and rain, are frequently introduced. These degradations typically arise from the inherent limitations of cameras or unfavorable ambient conditions. To recover clean images from degraded versions, numerous specialized restoration methods have been developed, each targeting a specific type of degradation. Recently, all-in-one algorithms have garnered significant attention by addressing different types of degradations within a single model without requiring prior information of the input degradation type. However, these methods purely operate in the spatial domain and do not delve into the distinct frequency variations inherent to different degradation types. To address this gap, we propose an adaptive all-in-one image restoration network based on frequency mining and modulation. Our approach is motivated by the observation that different degradation types impact the image content on different frequency subbands, thereby requiring different treatments for each restoration task. Specifically, we first mine low- and high-frequency information from the input features, guided by the adaptively decoupled spectra of the degraded image. The extracted features are then modulated by a bidirectional operator to facilitate interactions between different frequency components. Finally, the modulated features are merged into the original input for a progressively guided restoration. With this approach, the model achieves adaptive reconstruction by accentuating the informative frequency subbands according to different input degradations. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on different image restoration tasks, including denoising, <mark data-markjs="true">dehazing</mark>, deraining, motion deblurring, and low-light image enhancement. Our code is available at https://github.com/c-yn/AdaIR.</p>
            <p id="subjects-2403.14614" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.14614" class="metainfo date"><strong>Publish</strong>: 2024-03-21 17:58:14 UTC</p>
            <div id="pdf-container-2403.14614" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.14614" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.14614" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.09233" class="panel paper" keywords="yolo,detection,hazy,haze,subnetwork,adverse,weather,network,object,decline">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.09233" target="_blank" title="50/298"><span class="index notranslate">#50</span></a>
                <a id="title-2403.09233" class="title-link" href="/arxiv/2403.09233" target="_blank">D-YOLO a robust framework for object detection in adverse weather conditions</a>
                <a id="pdf-2403.09233" class="title-pdf notranslate" onclick="togglePdf('2403.09233', 'https://arxiv.org/pdf/2403.09233', this)">[PDF<sup id="pdf-stars-2403.09233">1</sup>]</a>
                <a id="copy-2403.09233" class="title-copy notranslate" onclick="copyToClipboard('2403.09233')">[Copy]</a>
                <a id="kimi-2403.09233" class="title-kimi notranslate" onclick="toggleKimi('2403.09233', this)">[Kimi<sup id="kimi-stars-2403.09233">4</sup>]</a>
                <a id="rel-2403.09233" class="title-rel notranslate" onclick="openRelatedPapers('2403.09233')">[REL]</a>
            </h2>
            <p id="authors-2403.09233" class="metainfo authors notranslate"><strong>Author</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zihan Chu" target="_blank"><span class="author notranslate">Zihan Chu</span></a>
            </p>
            <p id="summary-2403.09233" class="summary">Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks. Most existing approaches attempts to rectify hazy images before performing object detection, which increases the complexity of the network and may result in the loss in latent information. To better integrate image restoration and object detection tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration. We also proposed a subnetwork to provide haze-free features to the detection network. Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network. Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level <mark data-markjs="true">dehazing</mark> and high-level detection.</p>
            <p id="subjects-2403.09233" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2403.09233" class="metainfo date"><strong>Publish</strong>: 2024-03-14 09:57:15 UTC</p>
            <div id="pdf-container-2403.09233" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.09233" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.09233" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.07408" class="panel paper" keywords="nighttime,nighthaze,augmentation,mae,dehazing,severe,priors,images,self,clear">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.07408" target="_blank" title="51/298"><span class="index notranslate">#51</span></a>
                <a id="title-2403.07408" class="title-link" href="/arxiv/2403.07408" target="_blank">NightHaze: Nighttime Image <mark data-markjs="true">Dehazing</mark> via Self-Prior Learning</a>
                <a id="pdf-2403.07408" class="title-pdf notranslate" onclick="togglePdf('2403.07408', 'https://arxiv.org/pdf/2403.07408', this)">[PDF<sup id="pdf-stars-2403.07408">2</sup>]</a>
                <a id="copy-2403.07408" class="title-copy notranslate" onclick="copyToClipboard('2403.07408')">[Copy]</a>
                <a id="kimi-2403.07408" class="title-kimi notranslate" onclick="toggleKimi('2403.07408', this)">[Kimi<sup id="kimi-stars-2403.07408">3</sup>]</a>
                <a id="rel-2403.07408" class="title-rel notranslate" onclick="openRelatedPapers('2403.07408')">[REL]</a>
            </h2>
            <p id="authors-2403.07408" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Beibei Lin" target="_blank"><span class="author notranslate">Beibei Lin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yeying Jin" target="_blank"><span class="author notranslate">Yeying Jin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wending Yan" target="_blank"><span class="author notranslate">Wending Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Ye" target="_blank"><span class="author notranslate">Wei Ye</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuan Yuan" target="_blank"><span class="author notranslate">Yuan Yuan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Robby T. Tan" target="_blank"><span class="author notranslate">Robby T. Tan</span></a>
            </p>
            <p id="summary-2403.07408" class="summary">Masked autoencoder (MAE) shows that severe augmentation during training produces robust representations for high-level tasks. This paper brings the MAE-like framework to nighttime image enhancement, demonstrating that severe augmentation during training produces strong network priors that are resilient to real-world night haze degradations. We propose a novel nighttime image <mark data-markjs="true">dehazing</mark> method with self-prior learning. Our main novelty lies in the design of severe augmentation, which allows our model to learn robust priors. Unlike MAE that uses masking, we leverage two key challenging factors of nighttime images as augmentation: light effects and noise. During training, we intentionally degrade clear images by blending them with light effects as well as by adding noise, and subsequently restore the clear images. This enables our model to learn clear background priors. By increasing the noise values to approach as high as the pixel intensity values of the glow and light effect blended images, our augmentation becomes severe, resulting in stronger priors. While our self-prior learning is considerably effective in suppressing glow and revealing details of background scenes, in some cases, there are still some undesired artifacts that remain, particularly in the forms of over-suppression. To address these artifacts, we propose a self-refinement module based on the semi-supervised teacher-student framework. Our NightHaze, especially our MAE-like self-prior learning, shows that models trained with severe augmentation effectively improve the visibility of input haze images, approaching the clarity of clear nighttime images. Extensive experiments demonstrate that our NightHaze achieves state-of-the-art performance, outperforming existing nighttime image <mark data-markjs="true">dehazing</mark> methods by a substantial margin of 15.5% for MUSIQ and 23.5% for ClipIQA.</p>
            <p id="subjects-2403.07408" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.07408" class="metainfo date"><strong>Publish</strong>: 2024-03-12 08:35:42 UTC</p>
            <div id="pdf-container-2403.07408" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.07408" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.07408" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.04443" class="panel paper" keywords="friendnet,dehazing,detection,object,guidance,friendly,network,image,block,fanyihua0309">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.04443" target="_blank" title="52/298"><span class="index notranslate">#52</span></a>
                <a id="title-2403.04443" class="title-link" href="/arxiv/2403.04443" target="_blank">FriendNet: Detection-Friendly <mark data-markjs="true">Dehazing</mark> Network</a>
                <a id="pdf-2403.04443" class="title-pdf notranslate" onclick="togglePdf('2403.04443', 'https://arxiv.org/pdf/2403.04443', this)">[PDF<sup id="pdf-stars-2403.04443">3</sup>]</a>
                <a id="copy-2403.04443" class="title-copy notranslate" onclick="copyToClipboard('2403.04443')">[Copy]</a>
                <a id="kimi-2403.04443" class="title-kimi notranslate" onclick="toggleKimi('2403.04443', this)">[Kimi<sup id="kimi-stars-2403.04443">8</sup>]</a>
                <a id="rel-2403.04443" class="title-rel notranslate" onclick="openRelatedPapers('2403.04443')">[REL]</a>
            </h2>
            <p id="authors-2403.04443" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yihua Fan" target="_blank"><span class="author notranslate">Yihua Fan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yongzhen Wang" target="_blank"><span class="author notranslate">Yongzhen Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingqiang Wei" target="_blank"><span class="author notranslate">Mingqiang Wei</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fu Lee Wang" target="_blank"><span class="author notranslate">Fu Lee Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoran Xie" target="_blank"><span class="author notranslate">Haoran Xie</span></a>
            </p>
            <p id="summary-2403.04443" class="summary">Adverse weather conditions often impair the quality of captured images, inevitably inducing cutting-edge object detection models for advanced driver assistance systems (ADAS) and autonomous driving. In this paper, we raise an intriguing question: can the combination of image restoration and object detection enhance detection performance in adverse weather conditions? To answer it, we propose an effective architecture that bridges image <mark data-markjs="true">dehazing</mark> and object detection together via guidance information and task-driven learning to achieve detection-friendly <mark data-markjs="true">dehazing</mark>, termed FriendNet. FriendNet aims to deliver both high-quality perception and high detection capacity. Different from existing efforts that intuitively treat image <mark data-markjs="true">dehazing</mark> as pre-processing, FriendNet establishes a positive correlation between these two tasks. Clean features generated by the <mark data-markjs="true">dehazing</mark> network potentially contribute to improvements in object detection performance. Conversely, object detection crucially guides the learning process of the image <mark data-markjs="true">dehazing</mark> network under the task-driven learning scheme. We shed light on how downstream tasks can guide upstream <mark data-markjs="true">dehazing</mark> processes, considering both network architecture and learning objectives. We design Guidance Fusion Block (GFB) and Guidance Attention Block (GAB) to facilitate the integration of detection information into the network. Furthermore, the incorporation of the detection task loss aids in refining the optimization process. Additionally, we introduce a new Physics-aware Feature Enhancement Block (PFEB), which integrates physics-based priors to enhance the feature extraction and representation capabilities. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art methods on both image quality and detection precision. Our source code is available at https://github.com/fanyihua0309/FriendNet.</p>
            <p id="subjects-2403.04443" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.04443" class="metainfo date"><strong>Publish</strong>: 2024-03-07 12:19:04 UTC</p>
            <div id="pdf-container-2403.04443" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.04443" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.04443" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.01105" class="panel paper" keywords="dehazing,depth,hazy,image,dehazed,mutual,promotion,estimation,collaborative,areas">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.01105" target="_blank" title="53/298"><span class="index notranslate">#53</span></a>
                <a id="title-2403.01105" class="title-link" href="/arxiv/2403.01105" target="_blank">Depth Information Assisted Collaborative Mutual Promotion Network for Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2403.01105" class="title-pdf notranslate" onclick="togglePdf('2403.01105', 'https://arxiv.org/pdf/2403.01105', this)">[PDF<sup id="pdf-stars-2403.01105"></sup>]</a>
                <a id="copy-2403.01105" class="title-copy notranslate" onclick="copyToClipboard('2403.01105')">[Copy]</a>
                <a id="kimi-2403.01105" class="title-kimi notranslate" onclick="toggleKimi('2403.01105', this)">[Kimi<sup id="kimi-stars-2403.01105">3</sup>]</a>
                <a id="rel-2403.01105" class="title-rel notranslate" onclick="openRelatedPapers('2403.01105')">[REL]</a>
            </h2>
            <p id="authors-2403.01105" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yafei Zhang" target="_blank"><span class="author notranslate">Yafei Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shen Zhou" target="_blank"><span class="author notranslate">Shen Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huafeng Li" target="_blank"><span class="author notranslate">Huafeng Li</span></a>
            </p>
            <p id="summary-2403.01105" class="summary">Recovering a clear image from a single hazy image is an open inverse problem. Although significant research progress has been made, most existing methods ignore the effect that downstream tasks play in promoting upstream <mark data-markjs="true">dehazing</mark>. From the perspective of the haze generation mechanism, there is a potential relationship between the depth information of the scene and the hazy image. Based on this, we propose a dual-task collaborative mutual promotion framework to achieve the <mark data-markjs="true">dehazing</mark> of a single image. This framework integrates depth estimation and <mark data-markjs="true">dehazing</mark> by a dual-task interaction mechanism and achieves mutual enhancement of their performance. To realize the joint optimization of the two tasks, an alternative implementation mechanism with the difference perception is developed. On the one hand, the difference perception between the depth maps of the <mark data-markjs="true">dehazing</mark> result and the ideal image is proposed to promote the <mark data-markjs="true">dehazing</mark> network to pay attention to the non-ideal areas of the <mark data-markjs="true">dehazing</mark>. On the other hand, by improving the depth estimation performance in the difficult-to-recover areas of the hazy image, the <mark data-markjs="true">dehazing</mark> network can explicitly use the depth information of the hazy image to assist the clear image recovery. To promote the depth estimation, we propose to use the difference between the dehazed image and the ground truth to guide the depth estimation network to focus on the dehazed unideal areas. It allows <mark data-markjs="true">dehazing</mark> and depth estimation to leverage their strengths in a mutually reinforcing manner. Experimental results show that the proposed method can achieve better performance than that of the state-of-the-art approaches.</p>
            <p id="subjects-2403.01105" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.01105" class="metainfo date"><strong>Publish</strong>: 2024-03-02 06:29:44 UTC</p>
            <div id="pdf-container-2403.01105" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.01105" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.01105" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.18181" class="panel paper" keywords="foggy,fog,matching,stereo,cfdnet,contrastive,distillation,clean,depth,feature">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.18181" target="_blank" title="54/298"><span class="index notranslate">#54</span></a>
                <a id="title-2402.18181" class="title-link" href="/arxiv/2402.18181" target="_blank">CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation</a>
                <a id="pdf-2402.18181" class="title-pdf notranslate" onclick="togglePdf('2402.18181', 'https://arxiv.org/pdf/2402.18181', this)">[PDF<sup id="pdf-stars-2402.18181">2</sup>]</a>
                <a id="copy-2402.18181" class="title-copy notranslate" onclick="copyToClipboard('2402.18181')">[Copy]</a>
                <a id="kimi-2402.18181" class="title-kimi notranslate" onclick="toggleKimi('2402.18181', this)">[Kimi<sup id="kimi-stars-2402.18181">1</sup>]</a>
                <a id="rel-2402.18181" class="title-rel notranslate" onclick="openRelatedPapers('2402.18181')">[REL]</a>
            </h2>
            <p id="authors-2402.18181" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zihua Liu" target="_blank"><span class="author notranslate">Zihua Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yizhou Li" target="_blank"><span class="author notranslate">Yizhou Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Masatoshi Okutomi" target="_blank"><span class="author notranslate">Masatoshi Okutomi</span></a>
            </p>
            <p id="summary-2402.18181" class="summary">Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and <mark data-markjs="true">dehazing</mark>, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.</p>
            <p id="subjects-2402.18181" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2402.18181" class="metainfo date"><strong>Publish</strong>: 2024-02-28 09:12:01 UTC</p>
            <div id="pdf-container-2402.18181" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.18181" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.18181" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.18134" class="panel paper" keywords="polarization,polarized,images,deblurring,dop,aop,deblur,since,captured,shakes">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.18134" target="_blank" title="55/298"><span class="index notranslate">#55</span></a>
                <a id="title-2402.18134" class="title-link" href="/arxiv/2402.18134" target="_blank">Learning to Deblur Polarized Images</a>
                <a id="pdf-2402.18134" class="title-pdf notranslate" onclick="togglePdf('2402.18134', 'https://arxiv.org/pdf/2402.18134', this)">[PDF<sup id="pdf-stars-2402.18134"></sup>]</a>
                <a id="copy-2402.18134" class="title-copy notranslate" onclick="copyToClipboard('2402.18134')">[Copy]</a>
                <a id="kimi-2402.18134" class="title-kimi notranslate" onclick="toggleKimi('2402.18134', this)">[Kimi<sup id="kimi-stars-2402.18134">1</sup>]</a>
                <a id="rel-2402.18134" class="title-rel notranslate" onclick="openRelatedPapers('2402.18134')">[REL]</a>
            </h2>
            <p id="authors-2402.18134" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chu Zhou" target="_blank"><span class="author notranslate">Chu Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Minggui Teng" target="_blank"><span class="author notranslate">Minggui Teng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinyu Zhou" target="_blank"><span class="author notranslate">Xinyu Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chao Xu" target="_blank"><span class="author notranslate">Chao Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Boxin Sh" target="_blank"><span class="author notranslate">Boxin Sh</span></a>
            </p>
            <p id="summary-2402.18134" class="summary">A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image <mark data-markjs="true">dehazing</mark> and reflection removal.</p>
            <p id="subjects-2402.18134" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2402.18134" class="metainfo date"><strong>Publish</strong>: 2024-02-28 07:56:28 UTC</p>
            <div id="pdf-container-2402.18134" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.18134" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.18134" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.15784" class="panel paper" keywords="constyle,restoration,irconstyle,contrastive,nafnet,image,textbf,deraining,dehazing,style">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.15784" target="_blank" title="56/298"><span class="index notranslate">#56</span></a>
                <a id="title-2402.15784" class="title-link" href="/arxiv/2402.15784" target="_blank">IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer</a>
                <a id="pdf-2402.15784" class="title-pdf notranslate" onclick="togglePdf('2402.15784', 'https://arxiv.org/pdf/2402.15784', this)">[PDF<sup id="pdf-stars-2402.15784">1</sup>]</a>
                <a id="copy-2402.15784" class="title-copy notranslate" onclick="copyToClipboard('2402.15784')">[Copy]</a>
                <a id="kimi-2402.15784" class="title-kimi notranslate" onclick="toggleKimi('2402.15784', this)">[Kimi<sup id="kimi-stars-2402.15784">3</sup>]</a>
                <a id="rel-2402.15784" class="title-rel notranslate" onclick="openRelatedPapers('2402.15784')">[REL]</a>
            </h2>
            <p id="authors-2402.15784" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongqi Fan" target="_blank"><span class="author notranslate">Dongqi Fan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xin Zhao" target="_blank"><span class="author notranslate">Xin Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liang Chang" target="_blank"><span class="author notranslate">Liang Chang</span></a>
            </p>
            <p id="summary-2402.15784" class="summary">Recently, the contrastive learning paradigm has achieved remarkable success in high-level tasks such as classification, detection, and segmentation. However, contrastive learning applied in low-level tasks, like image restoration, is limited, and its effectiveness is uncertain. This raises a question: Why does the contrastive learning paradigm not yield satisfactory results in image restoration? In this paper, we conduct in-depth analyses and propose three guidelines to address the above question. In addition, inspired by style transfer and based on contrastive learning, we propose a novel module for image restoration called \textbf{ConStyle}, which can be efficiently integrated into any U-Net structure network. By leveraging the flexibility of ConStyle, we develop a \textbf{general restoration network} for image restoration. ConStyle and the general restoration network together form an image restoration framework, namely \textbf{IRConStyle}. To demonstrate the capability and compatibility of ConStyle, we replace the general restoration network with transformer-based, CNN-based, and MLP-based networks, respectively. We perform extensive experiments on various image restoration tasks, including denoising, deblurring, deraining, and <mark data-markjs="true">dehazing</mark>. The results on 19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based network and significantly enhance performance. For instance, ConStyle NAFNet significantly outperforms the original NAFNet on SOTS outdoor (<mark data-markjs="true">dehazing</mark>) and Rain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB with 85% fewer parameters.</p>
            <p id="subjects-2402.15784" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2402.15784" class="metainfo date"><strong>Publish</strong>: 2024-02-24 10:52:50 UTC</p>
            <div id="pdf-container-2402.15784" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.15784" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.15784" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.05281" class="panel paper" keywords="underwater,image,images,degradation,dehazing,formation,informed,water,driven,hazing">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.05281" target="_blank" title="57/298"><span class="index notranslate">#57</span></a>
                <a id="title-2402.05281" class="title-link" href="/arxiv/2402.05281" target="_blank">Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning</a>
                <a id="pdf-2402.05281" class="title-pdf notranslate" onclick="togglePdf('2402.05281', 'https://arxiv.org/pdf/2402.05281', this)">[PDF<sup id="pdf-stars-2402.05281">1</sup>]</a>
                <a id="copy-2402.05281" class="title-copy notranslate" onclick="copyToClipboard('2402.05281')">[Copy]</a>
                <a id="kimi-2402.05281" class="title-kimi notranslate" onclick="toggleKimi('2402.05281', this)">[Kimi<sup id="kimi-stars-2402.05281">1</sup>]</a>
                <a id="rel-2402.05281" class="title-rel notranslate" onclick="openRelatedPapers('2402.05281')">[REL]</a>
            </h2>
            <p id="authors-2402.05281" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tanmoy Mondal" target="_blank"><span class="author notranslate">Tanmoy Mondal</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ricardo Mendoza" target="_blank"><span class="author notranslate">Ricardo Mendoza</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lucas Drumetz" target="_blank"><span class="author notranslate">Lucas Drumetz</span></a>
            </p>
            <p id="summary-2402.05281" class="summary">In general, underwater images suffer from color distortion and low contrast, because light is attenuated and backscattered as it propagates through water (differently depending on wavelength and on the properties of the water body). An existing simple degradation model (similar to atmospheric image "hazing" effects), though helpful, is not sufficient to properly represent the underwater image degradation because there are unaccounted for and non-measurable factors e.g. scattering of light due to turbidity of water, reflective characteristics of turbid medium etc. We propose a deep learning-based architecture to automatically simulate the underwater effects where only a <mark data-markjs="true">dehazing</mark>-like image formation equation is known to the network, and the additional degradation due to the other unknown factors if inferred in a data-driven way. We only use RGB images (because in real-time scenario depth image is not available) to estimate the depth image. For testing, we have proposed (due to the lack of real underwater image datasets) a complex image formation model/equation to manually generate images that resemble real underwater images (used as ground truth). However, only the classical image formation equation (the one used for image <mark data-markjs="true">dehazing</mark>) is informed to the network. This mimics the fact that in a real scenario, the physics are never completely known and only simplified models are known. Thanks to the ground truth, generated by a complex image formation equation, we could successfully perform a qualitative and quantitative evaluation of proposed technique, compared to other purely data driven approaches</p>
            <p id="subjects-2402.05281" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2402.05281" class="metainfo date"><strong>Publish</strong>: 2024-02-07 21:53:28 UTC</p>
            <div id="pdf-container-2402.05281" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.05281" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.05281" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.04139" class="panel paper" keywords="dehazing,uvm,mamba,image,ssm,325,shaped,url,zzr,100fps">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.04139" target="_blank" title="58/298"><span class="index notranslate">#58</span></a>
                <a id="title-2402.04139" class="title-link" href="/arxiv/2402.04139" target="_blank">U-shaped Vision Mamba for Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2402.04139" class="title-pdf notranslate" onclick="togglePdf('2402.04139', 'https://arxiv.org/pdf/2402.04139', this)" style="color: purple;">[PDF<sup id="pdf-stars-2402.04139">8</sup>]</a>
                <a id="copy-2402.04139" class="title-copy notranslate" onclick="copyToClipboard('2402.04139')">[Copy]</a>
                <a id="kimi-2402.04139" class="title-kimi notranslate" onclick="toggleKimi('2402.04139', this)">[Kimi<sup id="kimi-stars-2402.04139">10</sup>]</a>
                <a id="rel-2402.04139" class="title-rel notranslate" onclick="openRelatedPapers('2402.04139')">[REL]</a>
            </h2>
            <p id="authors-2402.04139" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhuoran Zheng" target="_blank"><span class="author notranslate">Zhuoran Zheng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen Wu" target="_blank"><span class="author notranslate">Chen Wu</span></a>
            </p>
            <p id="summary-2402.04139" class="summary">Currently, Transformer is the most popular architecture for image <mark data-markjs="true">dehazing</mark>, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image <mark data-markjs="true">dehazing</mark> network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image <mark data-markjs="true">dehazing</mark> as well as other image restoration tasks. The URL of the code is \url{https://github.com/zzr-idam/UVM-Net}. Our method takes only \textbf{0.009} seconds to infer a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>325</mn><mo>&amp;#x00D7;</mo><mn>325</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.219em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.365em, 1004.17em, 2.366em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-7"><span class="mn" id="MathJax-Span-8" style="font-family: MathJax_Main;">325</span><span class="mo" id="MathJax-Span-9" style="font-family: MathJax_Main; padding-left: 0.222em;"></span><span class="mn" id="MathJax-Span-10" style="font-family: MathJax_Main; padding-left: 0.222em;">325</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.089em; border-left: 0px solid; width: 0px; height: 0.951em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>325</mn><mo></mo><mn>325</mn></math></span></span><script type="math/tex" id="MathJax-Element-2">325 \times 325</script> resolution image (100FPS) without I/O handling time.</p>
            <p id="subjects-2402.04139" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2402.04139" class="metainfo date"><strong>Publish</strong>: 2024-02-06 16:46:28 UTC</p>
            <div id="pdf-container-2402.04139" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.04139" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.04139" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2402.01368" class="panel paper" keywords="lir,restoration,lightweight,attention,image,degradations,block,baseline,blocks,adaptive">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2402.01368" target="_blank" title="59/298"><span class="index notranslate">#59</span></a>
                <a id="title-2402.01368" class="title-link" href="/arxiv/2402.01368" target="_blank">LIR: A Lightweight Baseline for Image Restoration</a>
                <a id="pdf-2402.01368" class="title-pdf notranslate" onclick="togglePdf('2402.01368', 'https://arxiv.org/pdf/2402.01368', this)" style="color: purple;">[PDF<sup id="pdf-stars-2402.01368">6</sup>]</a>
                <a id="copy-2402.01368" class="title-copy notranslate" onclick="copyToClipboard('2402.01368')">[Copy]</a>
                <a id="kimi-2402.01368" class="title-kimi notranslate" onclick="toggleKimi('2402.01368', this)">[Kimi<sup id="kimi-stars-2402.01368">3</sup>]</a>
                <a id="rel-2402.01368" class="title-rel notranslate" onclick="openRelatedPapers('2402.01368')">[REL]</a>
            </h2>
            <p id="authors-2402.01368" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongqi Fan" target="_blank"><span class="author notranslate">Dongqi Fan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ting Yue" target="_blank"><span class="author notranslate">Ting Yue</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xin Zhao" target="_blank"><span class="author notranslate">Xin Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Renjing Xu" target="_blank"><span class="author notranslate">Renjing Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liang Chang" target="_blank"><span class="author notranslate">Liang Chang</span></a>
            </p>
            <p id="summary-2402.01368" class="summary">Recently, there have been significant advancements in Image Restoration based on CNN and transformer. However, the inherent characteristics of the Image Restoration task are often overlooked in many works. They, instead, tend to focus on the basic block design and stack numerous such blocks to the model, leading to parameters redundant and computations unnecessary. Thus, the efficiency of the image restoration is hindered. In this paper, we propose a Lightweight Baseline network for Image Restoration called LIR to efficiently restore the image and remove degradations. First of all, through an ingenious structural design, LIR removes the degradations existing in the local and global residual connections that are ignored by modern networks. Then, a Lightweight Adaptive Attention (LAA) Block is introduced which is mainly composed of proposed Adaptive Filters and Attention Blocks. The proposed Adaptive Filter is used to adaptively extract high-frequency information and enhance object contours in various IR tasks, and Attention Block involves a novel Patch Attention module to approximate the self-attention part of the transformer. On the deraining task, our LIR achieves the state-of-the-art Structure Similarity Index Measure (SSIM) and comparable performance to state-of-the-art models on Peak Signal-to-Noise Ratio (PSNR). For denoising, <mark data-markjs="true">dehazing</mark>, and deblurring tasks, LIR also achieves a comparable performance to state-of-the-art models with a parameter size of about 30\%. In addition, it is worth noting that our LIR produces better visual results that are more in line with the human aesthetic.</p>
            <p id="subjects-2402.01368" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2402.01368" class="metainfo date"><strong>Publish</strong>: 2024-02-02 12:39:47 UTC</p>
            <div id="pdf-container-2402.01368" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2402.01368" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2402.01368" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2401.16468" class="panel paper" keywords="restoration,instructir,image,degradation,instructions,quality,degraded,prompts,guide,1db">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2401.16468" target="_blank" title="60/298"><span class="index notranslate">#60</span></a>
                <a id="title-2401.16468" class="title-link" href="/arxiv/2401.16468" target="_blank">InstructIR: High-Quality Image Restoration Following Human Instructions</a>
                <a id="pdf-2401.16468" class="title-pdf notranslate" onclick="togglePdf('2401.16468', 'https://arxiv.org/pdf/2401.16468', this)">[PDF<sup id="pdf-stars-2401.16468">6</sup>]</a>
                <a id="copy-2401.16468" class="title-copy notranslate" onclick="copyToClipboard('2401.16468')">[Copy]</a>
                <a id="kimi-2401.16468" class="title-kimi notranslate" onclick="toggleKimi('2401.16468', this)">[Kimi<sup id="kimi-stars-2401.16468">5</sup>]</a>
                <a id="rel-2401.16468" class="title-rel notranslate" onclick="openRelatedPapers('2401.16468')">[REL]</a>
            </h2>
            <p id="authors-2401.16468" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Marcos V. Conde" target="_blank"><span class="author notranslate">Marcos V. Conde</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gregor Geigle" target="_blank"><span class="author notranslate">Gregor Geigle</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Radu Timofte" target="_blank"><span class="author notranslate">Radu Timofte</span></a>
            </p>
            <p id="summary-2401.16468" class="summary">Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, <mark data-markjs="true">dehazing</mark>, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models are available at: https://github.com/mv-lab/InstructIR</p>
            <p id="subjects-2401.16468" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2401.16468" class="metainfo date"><strong>Publish</strong>: 2024-01-29 18:53:33 UTC</p>
            <div id="pdf-container-2401.16468" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2401.16468" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2401.16468" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2401.07213" class="panel paper" keywords="haze,dehazing,depth,agnostic,sots,csc,scene,methods,ability,achieve">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2401.07213" target="_blank" title="61/298"><span class="index notranslate">#61</span></a>
                <a id="title-2401.07213" class="title-link" href="/arxiv/2401.07213" target="_blank">Depth-agnostic Single Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2401.07213" class="title-pdf notranslate" onclick="togglePdf('2401.07213', 'https://arxiv.org/pdf/2401.07213', this)" style="color: purple;">[PDF<sup id="pdf-stars-2401.07213">4</sup>]</a>
                <a id="copy-2401.07213" class="title-copy notranslate" onclick="copyToClipboard('2401.07213')">[Copy]</a>
                <a id="kimi-2401.07213" class="title-kimi notranslate" onclick="toggleKimi('2401.07213', this)">[Kimi<sup id="kimi-stars-2401.07213">5</sup>]</a>
                <a id="rel-2401.07213" class="title-rel notranslate" onclick="openRelatedPapers('2401.07213')">[REL]</a>
            </h2>
            <p id="authors-2401.07213" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Honglei Xu" target="_blank"><span class="author notranslate">Honglei Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yan Shu" target="_blank"><span class="author notranslate">Yan Shu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shaohui Liu" target="_blank"><span class="author notranslate">Shaohui Liu</span></a>
            </p>
            <p id="summary-2401.07213" class="summary">Single image <mark data-markjs="true">dehazing</mark> is a challenging ill-posed problem. Existing datasets for training deep learning-based methods can be generated by hand-crafted or synthetic schemes. However, the former often suffers from small scales, while the latter forces models to learn scene depth instead of haze distribution, decreasing their <mark data-markjs="true">dehazing</mark> ability. To overcome the problem, we propose a simple yet novel synthetic method to decouple the relationship between haze density and scene depth, by which a depth-agnostic dataset (DA-HAZE) is generated. Meanwhile, a Global Shuffle Strategy (GSS) is proposed for generating differently scaled datasets, thereby enhancing the generalization ability of the model. Extensive experiments indicate that models trained on DA-HAZE achieve significant improvements on real-world benchmarks, with less discrepancy between SOTS and DA-SOTS (the test set of DA-HAZE). Additionally, Depth-agnostic <mark data-markjs="true">dehazing</mark> is a more complicated task because of the lack of depth prior. Therefore, an efficient architecture with stronger feature modeling ability and fewer computational costs is necessary. We revisit the U-Net-based architectures for <mark data-markjs="true">dehazing</mark>, in which dedicatedly designed blocks are incorporated. However, the performances of blocks are constrained by limited feature fusion methods. To this end, we propose a Convolutional Skip Connection (CSC) module, allowing vanilla feature fusion methods to achieve promising results with minimal costs. Extensive experimental results demonstrate that current state-of-the-art methods. equipped with CSC can achieve better performance and reasonable computational expense, whether the haze distribution is relevant to the scene depth.</p>
            <p id="subjects-2401.07213" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2401.07213" class="metainfo date"><strong>Publish</strong>: 2024-01-14 06:33:11 UTC</p>
            <div id="pdf-container-2401.07213" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2401.07213" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2401.07213" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2403.12054" class="panel paper" keywords="rsvt,haze,hazy,saturation,dehazing,value,regional,soft,segmentation,removal">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2403.12054" target="_blank" title="62/298"><span class="index notranslate">#62</span></a>
                <a id="title-2403.12054" class="title-link" href="/arxiv/2403.12054" target="_blank">Haze Removal via Regional Saturation-Value Translation and Soft Segmentation</a>
                <a id="pdf-2403.12054" class="title-pdf notranslate" onclick="togglePdf('2403.12054', 'https://arxiv.org/pdf/2403.12054', this)" style="color: purple;">[PDF<sup id="pdf-stars-2403.12054"></sup>]</a>
                <a id="copy-2403.12054" class="title-copy notranslate" onclick="copyToClipboard('2403.12054')">[Copy]</a>
                <a id="kimi-2403.12054" class="title-kimi notranslate" onclick="toggleKimi('2403.12054', this)">[Kimi<sup id="kimi-stars-2403.12054">2</sup>]</a>
                <a id="rel-2403.12054" class="title-rel notranslate" onclick="openRelatedPapers('2403.12054')">[REL]</a>
            </h2>
            <p id="authors-2403.12054" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Le-Anh Tran" target="_blank"><span class="author notranslate">Le-Anh Tran</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dong-Chul Park" target="_blank"><span class="author notranslate">Dong-Chul Park</span></a>
            </p>
            <p id="summary-2403.12054" class="summary">This paper proposes a single image <mark data-markjs="true">dehazing</mark> prior, called Regional Saturation-Value Translation (RSVT), to tackle the color distortion problems caused by conventional <mark data-markjs="true">dehazing</mark> approaches in bright regions. The RSVT prior is developed based on two key observations regarding the relationship between hazy and haze-free points in the HSV color space. First, the hue component shows marginal variation between corresponding hazy and haze-free points, consolidating a hypothesis that the pixel value variability induced by haze primarily occurs in the saturation and value spaces. Second, in the 2D saturation-value coordinate system, most lines passing through hazy-clean point pairs are likely to intersect near the atmospheric light coordinates. Accordingly, haze removal for the bright regions can be performed by properly translating saturation-value coordinates. In addition, an effective soft segmentation method based on a morphological min-max channel is introduced. By combining the soft segmentation mask with the RSVT prior, a comprehensive single image <mark data-markjs="true">dehazing</mark> framework is devised. Experimental results on various synthetic and realistic hazy image datasets demonstrate that the proposed scheme successfully addresses color distortion issues and restores visually appealing images. The code of this work is available at https://github.com/tranleanh/rsvt.</p>
            <p id="subjects-2403.12054" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2403.12054" class="metainfo date"><strong>Publish</strong>: 2024-01-07 07:52:50 UTC</p>
            <div id="pdf-container-2403.12054" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2403.12054" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2403.12054" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.17334" class="panel paper" keywords="restoration,image,removing,textual,degradations,text,representations,degradation,guidance,mrluin">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.17334" target="_blank" title="63/298"><span class="index notranslate">#63</span></a>
                <a id="title-2312.17334" class="title-link" href="/arxiv/2312.17334" target="_blank">Improving Image Restoration through Removing Degradations in Textual Representations</a>
                <a id="pdf-2312.17334" class="title-pdf notranslate" onclick="togglePdf('2312.17334', 'https://arxiv.org/pdf/2312.17334', this)" style="color: purple;">[PDF<sup id="pdf-stars-2312.17334">5</sup>]</a>
                <a id="copy-2312.17334" class="title-copy notranslate" onclick="copyToClipboard('2312.17334')">[Copy]</a>
                <a id="kimi-2312.17334" class="title-kimi notranslate" onclick="toggleKimi('2312.17334', this)">[Kimi<sup id="kimi-stars-2312.17334">6</sup>]</a>
                <a id="rel-2312.17334" class="title-rel notranslate" onclick="openRelatedPapers('2312.17334')">[REL]</a>
            </h2>
            <p id="authors-2312.17334" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingbo Lin" target="_blank"><span class="author notranslate">Jingbo Lin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhilu Zhang" target="_blank"><span class="author notranslate">Zhilu Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuxiang Wei" target="_blank"><span class="author notranslate">Yuxiang Wei</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongwei Ren" target="_blank"><span class="author notranslate">Dongwei Ren</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongsheng Jiang" target="_blank"><span class="author notranslate">Dongsheng Jiang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wangmeng Zuo" target="_blank"><span class="author notranslate">Wangmeng Zuo</span></a>
            </p>
            <p id="summary-2312.17334" class="summary">In this paper, we introduce a new perspective for improving image restoration by removing degradation in the textual representations of a given degraded image. Intuitively, restoration is much easier on text modality than image one. For example, it can be easily conducted by removing degradation-related words while keeping the content-aware words. Hence, we combine the advantages of images in detail description and ones of text in degradation removal to perform restoration. To address the cross-modal assistance, we propose to map the degraded images into textual representations for removing the degradations, and then convert the restored textual representations into a guidance image for assisting image restoration. In particular, We ingeniously embed an image-to-text mapper and text restoration module into CLIP-equipped text-to-image models to generate the guidance. Then, we adopt a simple coarse-to-fine approach to dynamically inject multi-scale information from guidance to image restoration networks. Extensive experiments are conducted on various image restoration tasks, including deblurring, <mark data-markjs="true">dehazing</mark>, deraining, and denoising, and all-in-one image restoration. The results showcase that our method outperforms state-of-the-art ones across all these tasks. The codes and models are available at \url{https://github.com/mrluin/TextualDegRemoval}.</p>
            <p id="subjects-2312.17334" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2312.17334" class="metainfo date"><strong>Publish</strong>: 2023-12-28 19:18:17 UTC</p>
            <div id="pdf-container-2312.17334" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.17334" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.17334" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.09955" class="panel paper" keywords="haze,dhformer,dehazing,attention,hazy,module,residual,image,vision,transformer">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.09955" target="_blank" title="64/298"><span class="index notranslate">#64</span></a>
                <a id="title-2312.09955" class="title-link" href="/arxiv/2312.09955" target="_blank">DHFormer: A Vision Transformer-Based Attention Module for Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2312.09955" class="title-pdf notranslate" onclick="togglePdf('2312.09955', 'https://arxiv.org/pdf/2312.09955', this)" style="color: purple;">[PDF<sup id="pdf-stars-2312.09955"></sup>]</a>
                <a id="copy-2312.09955" class="title-copy notranslate" onclick="copyToClipboard('2312.09955')">[Copy]</a>
                <a id="kimi-2312.09955" class="title-kimi notranslate" onclick="toggleKimi('2312.09955', this)">[Kimi<sup id="kimi-stars-2312.09955"></sup>]</a>
                <a id="rel-2312.09955" class="title-rel notranslate" onclick="openRelatedPapers('2312.09955')">[REL]</a>
            </h2>
            <p id="authors-2312.09955" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Abdul Wasi" target="_blank"><span class="author notranslate">Abdul Wasi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=O. Jeba Shiney" target="_blank"><span class="author notranslate">O. Jeba Shiney</span></a>
            </p>
            <p id="summary-2312.09955" class="summary">Images acquired in hazy conditions have degradations induced in them. <mark data-markjs="true">Dehazing</mark> such images is a vexed and ill-posed problem. Scores of prior-based and learning-based approaches have been proposed to mitigate the effect of haze and generate haze-free images. Many conventional methods are constrained by their lack of awareness regarding scene depth and their incapacity to capture long-range dependencies. In this paper, a method that uses residual learning and vision transformers in an attention module is proposed. It essentially comprises two networks: In the first one, the network takes the ratio of a hazy image and the approximated transmission matrix to estimate a residual map. The second network takes this residual image as input and passes it through convolution layers before superposing it on the generated feature maps. It is then passed through global context and depth-aware transformer encoders to obtain channel attention. The attention module then infers the spatial attention map before generating the final haze-free image. Experimental results, including several quantitative metrics, demonstrate the efficiency and scalability of the suggested methodology.</p>
            <p id="subjects-2312.09955" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2312.09955" class="metainfo date"><strong>Publish</strong>: 2023-12-15 17:05:32 UTC</p>
            <div id="pdf-container-2312.09955" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.09955" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.09955" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.07849" class="panel paper" keywords="rshazenet,dehazing,remote,minimal,module,transposed,sensing,decoder,itfm,mpeb">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.07849" target="_blank" title="65/298"><span class="index notranslate">#65</span></a>
                <a id="title-2312.07849" class="title-link" href="/arxiv/2312.07849" target="_blank">Encoder-minimal and Decoder-minimal Framework for Remote Sensing Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2312.07849" class="title-pdf notranslate" onclick="togglePdf('2312.07849', 'https://arxiv.org/pdf/2312.07849', this)" style="color: purple;">[PDF<sup id="pdf-stars-2312.07849"></sup>]</a>
                <a id="copy-2312.07849" class="title-copy notranslate" onclick="copyToClipboard('2312.07849')">[Copy]</a>
                <a id="kimi-2312.07849" class="title-kimi notranslate" onclick="toggleKimi('2312.07849', this)">[Kimi<sup id="kimi-stars-2312.07849"></sup>]</a>
                <a id="rel-2312.07849" class="title-rel notranslate" onclick="openRelatedPapers('2312.07849')">[REL]</a>
            </h2>
            <p id="authors-2312.07849" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuanbo Wen" target="_blank"><span class="author notranslate">Yuanbo Wen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tao Gao" target="_blank"><span class="author notranslate">Tao Gao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziqi Li" target="_blank"><span class="author notranslate">Ziqi Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jing Zhang" target="_blank"><span class="author notranslate">Jing Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ting Chen" target="_blank"><span class="author notranslate">Ting Chen</span></a>
            </p>
            <p id="summary-2312.07849" class="summary">Haze obscures remote sensing images, hindering valuable information extraction. To this end, we propose RSHazeNet, an encoder-minimal and decoder-minimal framework for efficient remote sensing image <mark data-markjs="true">dehazing</mark>. Specifically, regarding the process of merging features within the same level, we develop an innovative module called intra-level transposed fusion module (ITFM). This module employs adaptive transposed self-attention to capture comprehensive context-aware information, facilitating the robust context-aware feature fusion. Meanwhile, we present a cross-level multi-view interaction module (CMIM) to enable effective interactions between features from various levels, mitigating the loss of information due to the repeated sampling operations. In addition, we propose a multi-view progressive extraction block (MPEB) that partitions the features into four distinct components and employs convolution with varying kernel sizes, groups, and dilation factors to facilitate view-progressive feature learning. Extensive experiments demonstrate the superiority of our proposed RSHazeNet. We release the source code and all pre-trained models at \url{https://github.com/chdwyb/RSHazeNet}.</p>
            <p id="subjects-2312.07849" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2312.07849" class="metainfo date"><strong>Publish</strong>: 2023-12-13 02:35:02 UTC</p>
            <div id="pdf-container-2312.07849" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.07849" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.07849" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.06850" class="panel paper" keywords="ndels,nighttime,light,dehazing,enhancement,low,suppression,clipiqa,maniqa,glows">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.06850" target="_blank" title="66/298"><span class="index notranslate">#66</span></a>
                <a id="title-2312.06850" class="title-link" href="/arxiv/2312.06850" target="_blank">NDELS: A Novel Approach for Nighttime <mark data-markjs="true">Dehazing</mark>, Low-Light Enhancement, and Light Suppression</a>
                <a id="pdf-2312.06850" class="title-pdf notranslate" onclick="togglePdf('2312.06850', 'https://arxiv.org/pdf/2312.06850', this)">[PDF<sup id="pdf-stars-2312.06850"></sup>]</a>
                <a id="copy-2312.06850" class="title-copy notranslate" onclick="copyToClipboard('2312.06850')">[Copy]</a>
                <a id="kimi-2312.06850" class="title-kimi notranslate" onclick="toggleKimi('2312.06850', this)">[Kimi<sup id="kimi-stars-2312.06850"></sup>]</a>
                <a id="rel-2312.06850" class="title-rel notranslate" onclick="openRelatedPapers('2312.06850')">[REL]</a>
            </h2>
            <p id="authors-2312.06850" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Silvano A. Bernabel" target="_blank"><span class="author notranslate">Silvano A. Bernabel</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sos S. Agaian" target="_blank"><span class="author notranslate">Sos S. Agaian</span></a>
            </p>
            <p id="summary-2312.06850" class="summary">This paper tackles the intricate challenge of improving the quality of nighttime images under hazy and low-light conditions. Overcoming issues including nonuniform illumination glows, texture blurring, glow effects, color distortion, noise disturbance, and overall, low light have proven daunting. Despite the inherent difficulties, this paper introduces a pioneering solution named Nighttime <mark data-markjs="true">Dehazing</mark>, Low-Light Enhancement, and Light Suppression (NDELS). NDELS utilizes a unique network that combines three essential processes to enhance visibility, brighten low-light regions, and effectively suppress glare from bright light sources. In contrast to limited progress in nighttime <mark data-markjs="true">dehazing</mark>, unlike its daytime counterpart, NDELS presents a comprehensive and innovative approach. The efficacy of NDELS is rigorously validated through extensive comparisons with eight state-of-the-art algorithms across four diverse datasets. Experimental results showcase the superior performance of our method, demonstrating its outperformance in terms of overall image quality, including color and edge enhancement. Quantitative (PSNR, SSIM) and qualitative metrics (CLIPIQA, MANIQA, TRES), measure these results.</p>
            <p id="subjects-2312.06850" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
            </p>
            <p id="date-2312.06850" class="metainfo date"><strong>Publish</strong>: 2023-12-11 21:38:32 UTC</p>
            <div id="pdf-container-2312.06850" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.06850" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.06850" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.06162" class="panel paper" keywords="restoration,textual,degradation,prompts,prompt,image,motong,textpromptir,guided,visual">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.06162" target="_blank" title="67/298"><span class="index notranslate">#67</span></a>
                <a id="title-2312.06162" class="title-link" href="/arxiv/2312.06162" target="_blank">Textual Prompt Guided Image Restoration</a>
                <a id="pdf-2312.06162" class="title-pdf notranslate" onclick="togglePdf('2312.06162', 'https://arxiv.org/pdf/2312.06162', this)" style="color: purple;">[PDF<sup id="pdf-stars-2312.06162"></sup>]</a>
                <a id="copy-2312.06162" class="title-copy notranslate" onclick="copyToClipboard('2312.06162')">[Copy]</a>
                <a id="kimi-2312.06162" class="title-kimi notranslate" onclick="toggleKimi('2312.06162', this)">[Kimi<sup id="kimi-stars-2312.06162"></sup>]</a>
                <a id="rel-2312.06162" class="title-rel notranslate" onclick="openRelatedPapers('2312.06162')">[REL]</a>
            </h2>
            <p id="authors-2312.06162" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qiuhai Yan" target="_blank"><span class="author notranslate">Qiuhai Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aiwen Jiang" target="_blank"><span class="author notranslate">Aiwen Jiang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kang Chen" target="_blank"><span class="author notranslate">Kang Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Long Peng" target="_blank"><span class="author notranslate">Long Peng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qiaosi Yi" target="_blank"><span class="author notranslate">Qiaosi Yi</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chunjie Zhang" target="_blank"><span class="author notranslate">Chunjie Zhang</span></a>
            </p>
            <p id="summary-2312.06162" class="summary">Image restoration has always been a cutting-edge topic in the academic and industrial fields of computer vision. Since degradation signals are often random and diverse, "all-in-one" models that can do blind image restoration have been concerned in recent years. Early works require training specialized headers and tails to handle each degradation of concern, which are manually cumbersome. Recent works focus on learning visual prompts from data distribution to identify degradation type. However, the prompts employed in most of models are non-text, lacking sufficient emphasis on the importance of human-in-the-loop. In this paper, an effective textual prompt guided image restoration model has been proposed. In this model, task-specific BERT is fine-tuned to accurately understand user's instructions and generating textual prompt guidance. Depth-wise multi-head transposed attentions and gated convolution modules are designed to bridge the gap between textual prompts and visual features. The proposed model has innovatively introduced semantic prompts into low-level visual domain. It highlights the potential to provide a natural, precise, and controllable way to perform image restoration tasks. Extensive experiments have been done on public denoising, <mark data-markjs="true">dehazing</mark> and deraining datasets. The experiment results demonstrate that, compared with popular state-of-the-art methods, the proposed model can obtain much more superior performance, achieving accurate recognition and removal of degradation without increasing model's complexity. Related source codes and data will be publicly available on github site https://github.com/MoTong-AI-studio/TextPromptIR.</p>
            <p id="subjects-2312.06162" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2312.06162" class="metainfo date"><strong>Publish</strong>: 2023-12-11 06:56:41 UTC</p>
            <div id="pdf-container-2312.06162" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.06162" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.06162" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2312.05038" class="panel paper" keywords="prompt,restoration,pip,universal,image,prompts,degradation,module,longzilicart,enhance">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2312.05038" target="_blank" title="68/298"><span class="index notranslate">#68</span></a>
                <a id="title-2312.05038" class="title-link" href="/arxiv/2312.05038" target="_blank">Prompt-In-Prompt Learning for Universal Image Restoration</a>
                <a id="pdf-2312.05038" class="title-pdf notranslate" onclick="togglePdf('2312.05038', 'https://arxiv.org/pdf/2312.05038', this)">[PDF<sup id="pdf-stars-2312.05038">8</sup>]</a>
                <a id="copy-2312.05038" class="title-copy notranslate" onclick="copyToClipboard('2312.05038')">[Copy]</a>
                <a id="kimi-2312.05038" class="title-kimi notranslate" onclick="toggleKimi('2312.05038', this)">[Kimi<sup id="kimi-stars-2312.05038">11</sup>]</a>
                <a id="rel-2312.05038" class="title-rel notranslate" onclick="openRelatedPapers('2312.05038')">[REL]</a>
            </h2>
            <p id="authors-2312.05038" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zilong Li" target="_blank"><span class="author notranslate">Zilong Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yiming Lei" target="_blank"><span class="author notranslate">Yiming Lei</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenglong Ma" target="_blank"><span class="author notranslate">Chenglong Ma</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junping Zhang" target="_blank"><span class="author notranslate">Junping Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongming Shan" target="_blank"><span class="author notranslate">Hongming Shan</span></a>
            </p>
            <p id="summary-2312.05038" class="summary">Image restoration, which aims to retrieve and enhance degraded images, is fundamental across a wide range of applications. While conventional deep learning approaches have notably improved the image quality across various tasks, they still suffer from (i) the high storage cost needed for various task-specific models and (ii) the lack of interactivity and flexibility, hindering their wider application. Drawing inspiration from the pronounced success of prompts in both linguistic and visual domains, we propose novel Prompt-In-Prompt learning for universal image restoration, named PIP. First, we present two novel prompts, a degradation-aware prompt to encode high-level degradation knowledge and a basic restoration prompt to provide essential low-level information. Second, we devise a novel prompt-to-prompt interaction module to fuse these two prompts into a universal restoration prompt. Third, we introduce a selective prompt-to-feature interaction module to modulate the degradation-related feature. By doing so, the resultant PIP works as a plug-and-play module to enhance existing restoration models for universal image restoration. Extensive experimental results demonstrate the superior performance of PIP on multiple restoration tasks, including image denoising, deraining, <mark data-markjs="true">dehazing</mark>, deblurring, and low-light enhancement. Remarkably, PIP is interpretable, flexible, efficient, and easy-to-use, showing promising potential for real-world applications. The code is available at https://github.com/longzilicart/pip_universal.</p>
            <p id="subjects-2312.05038" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2312.05038" class="metainfo date"><strong>Publish</strong>: 2023-12-08 13:36:01 UTC</p>
            <div id="pdf-container-2312.05038" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2312.05038" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2312.05038" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2309.17389" class="panel paper" keywords="pttd,dehazing,prompt,hazy,real,cecret3350,pipeline,deviation,fam,test">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2309.17389" target="_blank" title="69/298"><span class="index notranslate">#69</span></a>
                <a id="title-2309.17389" class="title-link" href="/arxiv/2309.17389" target="_blank">Prompt-based test-time real image <mark data-markjs="true">dehazing</mark>: a novel pipeline</a>
                <a id="pdf-2309.17389" class="title-pdf notranslate" onclick="togglePdf('2309.17389', 'https://arxiv.org/pdf/2309.17389', this)">[PDF<sup id="pdf-stars-2309.17389"></sup>]</a>
                <a id="copy-2309.17389" class="title-copy notranslate" onclick="copyToClipboard('2309.17389')">[Copy]</a>
                <a id="kimi-2309.17389" class="title-kimi notranslate" onclick="toggleKimi('2309.17389', this)">[Kimi<sup id="kimi-stars-2309.17389"></sup>]</a>
                <a id="rel-2309.17389" class="title-rel notranslate" onclick="openRelatedPapers('2309.17389')">[REL]</a>
            </h2>
            <p id="authors-2309.17389" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zixuan Chen" target="_blank"><span class="author notranslate">Zixuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zewei He" target="_blank"><span class="author notranslate">Zewei He</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziqian Lu" target="_blank"><span class="author notranslate">Ziqian Lu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xuecheng Sun" target="_blank"><span class="author notranslate">Xuecheng Sun</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhe-Ming Lu" target="_blank"><span class="author notranslate">Zhe-Ming Lu</span></a>
            </p>
            <p id="summary-2309.17389" class="summary">Existing methods attempt to improve models' generalization ability on real-world hazy images by exploring well-designed training schemes (e.g., CycleGAN, prior loss). However, most of them need very complicated training procedures to achieve satisfactory results. In this work, we present a totally novel testing pipeline called Prompt-based Test-Time <mark data-markjs="true">Dehazing</mark> (PTTD) to help generate visually pleasing results of real-captured hazy images during the inference phase. We experimentally find that given a <mark data-markjs="true">dehazing</mark> model trained on synthetic data, by fine-tuning the statistics (i.e., mean and standard deviation) of encoding features, PTTD is able to narrow the domain gap, boosting the performance of real image <mark data-markjs="true">dehazing</mark>. Accordingly, we first apply a prompt generation module (PGM) to generate a visual prompt, which is the source of appropriate statistical perturbations for mean and standard deviation. And then, we employ the feature adaptation module (FAM) into the existing <mark data-markjs="true">dehazing</mark> models for adjusting the original statistics with the guidance of the generated prompt. Note that, PTTD is model-agnostic and can be equipped with various state-of-the-art <mark data-markjs="true">dehazing</mark> models trained on synthetic hazy-clean pairs. Extensive experimental results demonstrate that our PTTD is flexible meanwhile achieves superior performance against state-of-the-art <mark data-markjs="true">dehazing</mark> methods in real-world scenarios. The source code of our PTTD will be made available at https://github.com/cecret3350/PTTD-<mark data-markjs="true">Dehazing</mark>.</p>
            <p id="subjects-2309.17389" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2309.17389" class="metainfo date"><strong>Publish</strong>: 2023-09-29 16:50:38 UTC</p>
            <div id="pdf-container-2309.17389" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2309.17389" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2309.17389" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2309.16494" class="panel paper" keywords="dehazing,cnlb,msfe,mrfnln,msfab,block,receptive,extracting,multi,local">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2309.16494" target="_blank" title="70/298"><span class="index notranslate">#70</span></a>
                <a id="title-2309.16494" class="title-link" href="/arxiv/2309.16494" target="_blank">Accurate and lightweight <mark data-markjs="true">dehazing</mark> via multi-receptive-field non-local network and novel contrastive regularization</a>
                <a id="pdf-2309.16494" class="title-pdf notranslate" onclick="togglePdf('2309.16494', 'https://arxiv.org/pdf/2309.16494', this)" style="color: purple;">[PDF<sup id="pdf-stars-2309.16494"></sup>]</a>
                <a id="copy-2309.16494" class="title-copy notranslate" onclick="copyToClipboard('2309.16494')">[Copy]</a>
                <a id="kimi-2309.16494" class="title-kimi notranslate" onclick="toggleKimi('2309.16494', this)">[Kimi<sup id="kimi-stars-2309.16494"></sup>]</a>
                <a id="rel-2309.16494" class="title-rel notranslate" onclick="openRelatedPapers('2309.16494')">[REL]</a>
            </h2>
            <p id="authors-2309.16494" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zewei He" target="_blank"><span class="author notranslate">Zewei He</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zixuan Chen" target="_blank"><span class="author notranslate">Zixuan Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziqian Lu" target="_blank"><span class="author notranslate">Ziqian Lu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xuecheng Sun" target="_blank"><span class="author notranslate">Xuecheng Sun</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhe-Ming Lu" target="_blank"><span class="author notranslate">Zhe-Ming Lu</span></a>
            </p>
            <p id="summary-2309.16494" class="summary">Recently, deep learning-based methods have dominated image <mark data-markjs="true">dehazing</mark> domain. Although very competitive <mark data-markjs="true">dehazing</mark> performance has been achieved with sophisticated models, effective solutions for extracting useful features are still under-explored. In addition, non-local network, which has made a breakthrough in many vision tasks, has not been appropriately applied to image <mark data-markjs="true">dehazing</mark>. Thus, a multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and cross non-local block (CNLB) is presented in this paper. We start with extracting richer features for <mark data-markjs="true">dehazing</mark>. Specifically, we design a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x00D7;</mo><mn>1</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-11" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.24em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.365em, 1002.17em, 2.344em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-12"><span class="mn" id="MathJax-Span-13" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-14" style="font-family: MathJax_Main; padding-left: 0.222em;"></span><span class="mn" id="MathJax-Span-15" style="font-family: MathJax_Main; padding-left: 0.222em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.063em; border-left: 0px solid; width: 0px; height: 0.924em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo></mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-3">1\times 1</script>, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn><mo>&amp;#x00D7;</mo><mn>3</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.24em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.366em, 1002.2em, 2.366em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mn" id="MathJax-Span-18" style="font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-19" style="font-family: MathJax_Main; padding-left: 0.222em;"></span><span class="mn" id="MathJax-Span-20" style="font-family: MathJax_Main; padding-left: 0.222em;">3</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.089em; border-left: 0px solid; width: 0px; height: 0.949em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mo></mo><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-4">3\times 3</script>, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mo>&amp;#x00D7;</mo><mn>5</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-21" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.24em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.365em, 1002.19em, 2.366em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-22"><span class="mn" id="MathJax-Span-23" style="font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main; padding-left: 0.222em;"></span><span class="mn" id="MathJax-Span-25" style="font-family: MathJax_Main; padding-left: 0.222em;">5</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.089em; border-left: 0px solid; width: 0px; height: 0.951em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo></mo><mn>5</mn></math></span></span><script type="math/tex" id="MathJax-Element-5">5\times 5</script>) for extracting multi-scale features. Following MSFE, we employ an attention sub-block to make the model adaptively focus on important channels/regions. The MSFE and attention sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in the representation space. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art <mark data-markjs="true">dehazing</mark> methods with less than 1.5 Million parameters.</p>
            <p id="subjects-2309.16494" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2309.16494" class="metainfo date"><strong>Publish</strong>: 2023-09-28 14:59:16 UTC</p>
            <div id="pdf-container-2309.16494" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2309.16494" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2309.16494" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2309.06023" class="panel paper" keywords="restoration,mclir,contrastive,retrained,spn,image,tasks,negative,ffanet,task">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2309.06023" target="_blank" title="71/298"><span class="index notranslate">#71</span></a>
                <a id="title-2309.06023" class="title-link" href="/arxiv/2309.06023" target="_blank">Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration</a>
                <a id="pdf-2309.06023" class="title-pdf notranslate" onclick="togglePdf('2309.06023', 'https://arxiv.org/pdf/2309.06023', this)" style="color: purple;">[PDF<sup id="pdf-stars-2309.06023"></sup>]</a>
                <a id="copy-2309.06023" class="title-copy notranslate" onclick="copyToClipboard('2309.06023')">[Copy]</a>
                <a id="kimi-2309.06023" class="title-kimi notranslate" onclick="toggleKimi('2309.06023', this)">[Kimi<sup id="kimi-stars-2309.06023"></sup>]</a>
                <a id="rel-2309.06023" class="title-rel notranslate" onclick="openRelatedPapers('2309.06023')">[REL]</a>
            </h2>
            <p id="authors-2309.06023" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gang Wu" target="_blank"><span class="author notranslate">Gang Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junjun Jiang" target="_blank"><span class="author notranslate">Junjun Jiang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kui Jiang" target="_blank"><span class="author notranslate">Kui Jiang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xianming Liu" target="_blank"><span class="author notranslate">Xianming Liu</span></a>
            </p>
            <p id="summary-2309.06023" class="summary">Contrastive learning has emerged as a prevailing paradigm for high-level vision tasks, which, by introducing properly negative samples, has also been exploited for low-level vision tasks to achieve a compact optimization space to account for their ill-posed nature. However, existing methods rely on manually predefined and task-oriented negatives, which often exhibit pronounced task-specific biases. To address this challenge, our paper introduces an innovative method termed 'learning from history', which dynamically generates negative samples from the target model itself. Our approach, named Model Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models as negative models, making it compatible with diverse image restoration tasks. We propose the Self-Prior guided Negative loss (SPN) to enable it. This approach significantly enhances existing models when retrained with the proposed model contrastive paradigm. The results show significant improvements in image restoration across various tasks and architectures. For example, models retrained with SPN outperform the original FFANet and DehazeFormer by 3.41 dB and 0.57 dB on the RESIDE indoor dataset for image <mark data-markjs="true">dehazing</mark>. Similarly, they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over lightweight SwinIR, respectively. Code and retrained models are available at https://github.com/Aitical/MCLIR.</p>
            <p id="subjects-2309.06023" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2309.06023" class="metainfo date"><strong>Publish</strong>: 2023-09-12 07:50:54 UTC</p>
            <div id="pdf-container-2309.06023" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2309.06023" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2309.06023" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2309.00514" class="panel paper" keywords="aea,gfa,crosshair,mdc,200pixels,enhancement,correction,net,aspherical,eccentric">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2309.00514" target="_blank" title="72/298"><span class="index notranslate">#72</span></a>
                <a id="title-2309.00514" class="title-link" href="/arxiv/2309.00514" target="_blank">A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</a>
                <a id="pdf-2309.00514" class="title-pdf notranslate" onclick="togglePdf('2309.00514', 'https://arxiv.org/pdf/2309.00514', this)">[PDF<sup id="pdf-stars-2309.00514"></sup>]</a>
                <a id="copy-2309.00514" class="title-copy notranslate" onclick="copyToClipboard('2309.00514')">[Copy]</a>
                <a id="kimi-2309.00514" class="title-kimi notranslate" onclick="toggleKimi('2309.00514', this)">[Kimi<sup id="kimi-stars-2309.00514"></sup>]</a>
                <a id="rel-2309.00514" class="title-rel notranslate" onclick="openRelatedPapers('2309.00514')">[REL]</a>
            </h2>
            <p id="authors-2309.00514" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fanyi Wang" target="_blank"><span class="author notranslate">Fanyi Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pin Cao" target="_blank"><span class="author notranslate">Pin Cao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yihui Zhang" target="_blank"><span class="author notranslate">Yihui Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haotian Hu" target="_blank"><span class="author notranslate">Haotian Hu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yongying Yang" target="_blank"><span class="author notranslate">Yongying Yang</span></a>
            </p>
            <p id="summary-2309.00514" class="summary">In the procedure of surface defects detection for large-aperture aspherical optical elements, it is of vital significance to adjust the optical axis of the element to be coaxial with the mechanical spin axis accurately. Therefore, a machine vision method for eccentric error correction is proposed in this paper. Focusing on the severe defocus blur of reference crosshair image caused by the imaging characteristic of the aspherical optical element, which may lead to the failure of correction, an Adaptive Enhancement Algorithm (AEA) is proposed to strengthen the crosshair image. AEA is consisted of existed Guided Filter Dark Channel <mark data-markjs="true">Dehazing</mark> Algorithm (GFA) and proposed lightweight Multi-scale Densely Connected Network (MDC-Net). The enhancement effect of GFA is excellent but time-consuming, and the enhancement effect of MDC-Net is slightly inferior but strongly real-time. As AEA will be executed dozens of times during each correction procedure, its real-time performance is very important. Therefore, by setting the empirical threshold of definition evaluation function SMD2, GFA and MDC-Net are respectively applied to highly and slightly blurred crosshair images so as to ensure the enhancement effect while saving as much time as possible. AEA has certain robustness in time-consuming performance, which takes an average time of 0.2721s and 0.0963s to execute GFA and MDC-Net separately on ten 200pixels 200pixels Region of Interest (ROI) images with different degrees of blur. And the eccentricity error can be reduced to within 10um by our method.</p>
            <p id="subjects-2309.00514" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2309.00514" class="metainfo date"><strong>Publish</strong>: 2023-09-01 15:06:39 UTC</p>
            <div id="pdf-container-2309.00514" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2309.00514" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2309.00514" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.14036" class="panel paper" keywords="taylorformer,dehazing,transformer,taylor,multi,branch,receptive,patch,expanded,computational">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.14036" target="_blank" title="73/298"><span class="index notranslate">#73</span></a>
                <a id="title-2308.14036" class="title-link" href="/arxiv/2308.14036" target="_blank">MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2308.14036" class="title-pdf notranslate" onclick="togglePdf('2308.14036', 'https://arxiv.org/pdf/2308.14036', this)" style="color: purple;">[PDF<sup id="pdf-stars-2308.14036"></sup>]</a>
                <a id="copy-2308.14036" class="title-copy notranslate" onclick="copyToClipboard('2308.14036')">[Copy]</a>
                <a id="kimi-2308.14036" class="title-kimi notranslate" onclick="toggleKimi('2308.14036', this)">[Kimi<sup id="kimi-stars-2308.14036"></sup>]</a>
                <a id="rel-2308.14036" class="title-rel notranslate" onclick="openRelatedPapers('2308.14036')">[REL]</a>
            </h2>
            <p id="authors-2308.14036" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuwei Qiu" target="_blank"><span class="author notranslate">Yuwei Qiu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaihao Zhang" target="_blank"><span class="author notranslate">Kaihao Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenxi Wang" target="_blank"><span class="author notranslate">Chenxi Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenhan Luo" target="_blank"><span class="author notranslate">Wenhan Luo</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongdong Li" target="_blank"><span class="author notranslate">Hongdong Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhi Jin" target="_blank"><span class="author notranslate">Zhi Jin</span></a>
            </p>
            <p id="summary-2308.14036" class="summary">In recent years, Transformer networks are beginning to replace pure convolutional neural networks (CNNs) in the field of computer vision due to their global receptive field and adaptability to input. However, the quadratic computational complexity of softmax-attention limits the wide application in image <mark data-markjs="true">dehazing</mark> task, especially for high-resolution images. To address this issue, we propose a new Transformer variant, which applies the Taylor expansion to approximate the softmax-attention and achieves linear computational complexity. A multi-scale attention refinement module is proposed as a complement to correct the error of the Taylor expansion. Furthermore, we introduce a multi-branch architecture with multi-scale patch embedding to the proposed Transformer, which embeds features by overlapping deformable convolution of different scales. The design of multi-scale patch embedding is based on three key ideas: 1) various sizes of the receptive field; 2) multi-level semantic information; 3) flexible shapes of the receptive field. Our model, named Multi-branch Transformer expanded by Taylor formula (MB-TaylorFormer), can embed coarse to fine features more flexibly at the patch embedding stage and capture long-distance pixel interactions with limited computational cost. Experimental results on several <mark data-markjs="true">dehazing</mark> benchmarks show that MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a light computational burden. The source code and pre-trained models are available at https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.</p>
            <p id="subjects-2308.14036" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2308.14036" class="metainfo date"><strong>Publish</strong>: 2023-08-27 08:10:23 UTC</p>
            <div id="pdf-container-2308.14036" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.14036" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.14036" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.11949" class="panel paper" keywords="dehazing,dehazeddpm,ddpm,hazy,haze,image,scenarios,information,marvelous,ability">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.11949" target="_blank" title="74/298"><span class="index notranslate">#74</span></a>
                <a id="title-2308.11949" class="title-link" href="/arxiv/2308.11949" target="_blank">High-quality Image <mark data-markjs="true">Dehazing</mark> with Diffusion Model</a>
                <a id="pdf-2308.11949" class="title-pdf notranslate" onclick="togglePdf('2308.11949', 'https://arxiv.org/pdf/2308.11949', this)" style="color: purple;">[PDF<sup id="pdf-stars-2308.11949"></sup>]</a>
                <a id="copy-2308.11949" class="title-copy notranslate" onclick="copyToClipboard('2308.11949')">[Copy]</a>
                <a id="kimi-2308.11949" class="title-kimi notranslate" onclick="toggleKimi('2308.11949', this)">[Kimi<sup id="kimi-stars-2308.11949"></sup>]</a>
                <a id="rel-2308.11949" class="title-rel notranslate" onclick="openRelatedPapers('2308.11949')">[REL]</a>
            </h2>
            <p id="authors-2308.11949" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hu Yu" target="_blank"><span class="author notranslate">Hu Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie Huang" target="_blank"><span class="author notranslate">Jie Huang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaiwen Zheng" target="_blank"><span class="author notranslate">Kaiwen Zheng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Feng Zhao" target="_blank"><span class="author notranslate">Feng Zhao</span></a>
            </p>
            <p id="summary-2308.11949" class="summary">Image <mark data-markjs="true">dehazing</mark> is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of <mark data-markjs="true">dehazing</mark> task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image <mark data-markjs="true">dehazing</mark> framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the <mark data-markjs="true">dehazing</mark> task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.</p>
            <p id="subjects-2308.11949" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
            </p>
            <p id="date-2308.11949" class="metainfo date"><strong>Publish</strong>: 2023-08-23 06:45:11 UTC</p>
            <div id="pdf-container-2308.11949" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.11949" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.11949" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.10510" class="panel paper" keywords="dehazing,diffusion,fcb,hazy,frequency,haze,compensated,real,hazeaug,jilly">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.10510" target="_blank" title="75/298"><span class="index notranslate">#75</span></a>
                <a id="title-2308.10510" class="title-link" href="/arxiv/2308.10510" target="_blank">Frequency Compensated Diffusion Model for Real-scene <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2308.10510" class="title-pdf notranslate" onclick="togglePdf('2308.10510', 'https://arxiv.org/pdf/2308.10510', this)">[PDF<sup id="pdf-stars-2308.10510"></sup>]</a>
                <a id="copy-2308.10510" class="title-copy notranslate" onclick="copyToClipboard('2308.10510')">[Copy]</a>
                <a id="kimi-2308.10510" class="title-kimi notranslate" onclick="toggleKimi('2308.10510', this)">[Kimi<sup id="kimi-stars-2308.10510"></sup>]</a>
                <a id="rel-2308.10510" class="title-rel notranslate" onclick="openRelatedPapers('2308.10510')">[REL]</a>
            </h2>
            <p id="authors-2308.10510" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jing Wang" target="_blank"><span class="author notranslate">Jing Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Songtao Wu" target="_blank"><span class="author notranslate">Songtao Wu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kuanhong Xu" target="_blank"><span class="author notranslate">Kuanhong Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhiqiang Yuan" target="_blank"><span class="author notranslate">Zhiqiang Yuan</span></a>
            </p>
            <p id="summary-2308.10510" class="summary">Due to distribution shift, deep learning based methods for image <mark data-markjs="true">dehazing</mark> suffer from performance degradation when applied to real-world hazy images. In this paper, we consider a <mark data-markjs="true">dehazing</mark> framework based on conditional diffusion models for improved generalization to real haze. First, we find that optimizing the training objective of diffusion models, i.e., Gaussian noise vectors, is non-trivial. The spectral bias of deep networks hinders the higher frequency modes in Gaussian vectors from being learned and hence impairs the reconstruction of image details. To tackle this issue, we design a network unit, named Frequency Compensation block (FCB), with a bank of filters that jointly emphasize the mid-to-high frequencies of an input signal. We demonstrate that diffusion models with FCB achieve significant gains in both perceptual and distortion metrics. Second, to further boost the generalization performance, we propose a novel data synthesis pipeline, HazeAug, to augment haze in terms of degree and diversity. Within the framework, a solid baseline for blind <mark data-markjs="true">dehazing</mark> is set up where models are trained on synthetic hazy-clean pairs, and directly generalize to real data. Extensive evaluations show that the proposed <mark data-markjs="true">dehazing</mark> diffusion model significantly outperforms state-of-the-art methods on real-world images. Our code is at https://github.com/W-Jilly/frequency-compensated-diffusion-model-pytorch.</p>
            <p id="subjects-2308.10510" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2308.10510" class="metainfo date"><strong>Publish</strong>: 2023-08-21 06:50:44 UTC</p>
            <div id="pdf-container-2308.10510" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.10510" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.10510" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.06998" class="panel paper" keywords="mitnet,stage,information,dehazing,mutual,triple,hazy,interaction,haze,cross">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.06998" target="_blank" title="76/298"><span class="index notranslate">#76</span></a>
                <a id="title-2308.06998" class="title-link" href="/arxiv/2308.06998" target="_blank">Mutual Information-driven Triple Interaction Network for Efficient Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2308.06998" class="title-pdf notranslate" onclick="togglePdf('2308.06998', 'https://arxiv.org/pdf/2308.06998', this)">[PDF<sup id="pdf-stars-2308.06998"></sup>]</a>
                <a id="copy-2308.06998" class="title-copy notranslate" onclick="copyToClipboard('2308.06998')">[Copy]</a>
                <a id="kimi-2308.06998" class="title-kimi notranslate" onclick="toggleKimi('2308.06998', this)">[Kimi<sup id="kimi-stars-2308.06998"></sup>]</a>
                <a id="rel-2308.06998" class="title-rel notranslate" onclick="openRelatedPapers('2308.06998')">[REL]</a>
            </h2>
            <p id="authors-2308.06998" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao Shen" target="_blank"><span class="author notranslate">Hao Shen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhong-Qiu Zhao" target="_blank"><span class="author notranslate">Zhong-Qiu Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yulun Zhang" target="_blank"><span class="author notranslate">Yulun Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhao Zhang" target="_blank"><span class="author notranslate">Zhao Zhang</span></a>
            </p>
            <p id="summary-2308.06998" class="summary">Multi-stage architectures have exhibited efficacy in image <mark data-markjs="true">dehazing</mark>, which usually decomposes a challenging task into multiple more tractable sub-tasks and progressively estimates latent hazy-free images. Despite the remarkable progress, existing methods still suffer from the following shortcomings: (1) limited exploration of frequency domain information; (2) insufficient information interaction; (3) severe feature redundancy. To remedy these issues, we propose a novel Mutual Information-driven Triple interaction Network (MITNet) based on spatial-frequency dual domain information and two-stage architecture. To be specific, the first stage, named amplitude-guided haze removal, aims to recover the amplitude spectrum of the hazy images for haze removal. And the second stage, named phase-guided structure refined, devotes to learning the transformation and refinement of the phase spectrum. To facilitate the information exchange between two stages, an Adaptive Triple Interaction Module (ATIM) is developed to simultaneously aggregate cross-domain, cross-scale, and cross-stage features, where the fused features are further used to generate content-adaptive dynamic filters so that applying them to enhance global context representation. In addition, we impose the mutual information minimization constraint on paired scale encoder and decoder features from both stages. Such an operation can effectively reduce information redundancy and enhance cross-stage feature complementarity. Extensive experiments on multiple public datasets exhibit that our MITNet performs superior performance with lower model complexity.The code and models are available at https://github.com/it-hao/MITNet.</p>
            <p id="subjects-2308.06998" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2308.06998" class="metainfo date"><strong>Publish</strong>: 2023-08-14 08:23:58 UTC</p>
            <div id="pdf-container-2308.06998" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.06998" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.06998" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.01738" class="panel paper" keywords="glow,nighttime,haze,light,apsf,visibility,hazy,images,low,rendered">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.01738" target="_blank" title="77/298"><span class="index notranslate">#77</span></a>
                <a id="title-2308.01738" class="title-link" href="/arxiv/2308.01738" target="_blank">Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution</a>
                <a id="pdf-2308.01738" class="title-pdf notranslate" onclick="togglePdf('2308.01738', 'https://arxiv.org/pdf/2308.01738', this)">[PDF<sup id="pdf-stars-2308.01738"></sup>]</a>
                <a id="copy-2308.01738" class="title-copy notranslate" onclick="copyToClipboard('2308.01738')">[Copy]</a>
                <a id="kimi-2308.01738" class="title-kimi notranslate" onclick="toggleKimi('2308.01738', this)">[Kimi<sup id="kimi-stars-2308.01738"></sup>]</a>
                <a id="rel-2308.01738" class="title-rel notranslate" onclick="openRelatedPapers('2308.01738')">[REL]</a>
            </h2>
            <p id="authors-2308.01738" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yeying Jin" target="_blank"><span class="author notranslate">Yeying Jin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Beibei Lin" target="_blank"><span class="author notranslate">Beibei Lin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wending Yan" target="_blank"><span class="author notranslate">Wending Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuan Yuan" target="_blank"><span class="author notranslate">Yuan Yuan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Ye" target="_blank"><span class="author notranslate">Wei Ye</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Robby T. Tan" target="_blank"><span class="author notranslate">Robby T. Tan</span></a>
            </p>
            <p id="summary-2308.01738" class="summary">Visibility in hazy nighttime scenes is frequently reduced by multiple factors, including low light, intense glow, light scattering, and the presence of multicolored light sources. Existing nighttime <mark data-markjs="true">dehazing</mark> methods often struggle with handling glow or low-light conditions, resulting in either excessively dark visuals or unsuppressed glow outputs. In this paper, we enhance the visibility from a single nighttime haze image by suppressing glow and enhancing low-light regions. To handle glow effects, our framework learns from the rendered glow pairs. Specifically, a light source aware network is proposed to detect light sources of night images, followed by the APSF (Atmospheric Point Spread Function)-guided glow rendering. Our framework is then trained on the rendered images, resulting in glow suppression. Moreover, we utilize gradient-adaptive convolution, to capture edges and textures in hazy scenes. By leveraging extracted edges and textures, we enhance the contrast of the scene without losing important structural details. To boost low-light intensity, our network learns an attention map, then adjusted by gamma correction. This attention has high values on low-light regions and low values on haze and glow regions. Extensive evaluation on real nighttime haze images, demonstrates the effectiveness of our method. Our experiments demonstrate that our method achieves a PSNR of 30.38dB, outperforming state-of-the-art methods by 13% on GTA5 nighttime haze dataset. Our data and code is available at https://github.com/jinyeying/nighttime_dehaze.</p>
            <p id="subjects-2308.01738" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2308.01738" class="metainfo date"><strong>Publish</strong>: 2023-08-03 12:58:23 UTC</p>
            <div id="pdf-container-2308.01738" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.01738" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.01738" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2308.00591" class="panel paper" keywords="hazy,light,visibility,enhancement,low,dehazing,proposed,dusk,scenarios,morning">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2308.00591" target="_blank" title="78/298"><span class="index notranslate">#78</span></a>
                <a id="title-2308.00591" class="title-link" href="/arxiv/2308.00591" target="_blank">Visibility Enhancement for Low-light Hazy Scenarios</a>
                <a id="pdf-2308.00591" class="title-pdf notranslate" onclick="togglePdf('2308.00591', 'https://arxiv.org/pdf/2308.00591', this)">[PDF<sup id="pdf-stars-2308.00591"></sup>]</a>
                <a id="copy-2308.00591" class="title-copy notranslate" onclick="copyToClipboard('2308.00591')">[Copy]</a>
                <a id="kimi-2308.00591" class="title-kimi notranslate" onclick="toggleKimi('2308.00591', this)">[Kimi<sup id="kimi-stars-2308.00591"></sup>]</a>
                <a id="rel-2308.00591" class="title-rel notranslate" onclick="openRelatedPapers('2308.00591')">[REL]</a>
            </h2>
            <p id="authors-2308.00591" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chaoqun Zhuang" target="_blank"><span class="author notranslate">Chaoqun Zhuang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yunfei Liu" target="_blank"><span class="author notranslate">Yunfei Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sijia Wen" target="_blank"><span class="author notranslate">Sijia Wen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Feng Lu" target="_blank"><span class="author notranslate">Feng Lu</span></a>
            </p>
            <p id="summary-2308.00591" class="summary">Low-light hazy scenes commonly appear at dusk and early morning. The visual enhancement for low-light hazy images is an ill-posed problem. Even though numerous methods have been proposed for image <mark data-markjs="true">dehazing</mark> and low-light enhancement respectively, simply integrating them cannot deliver pleasing results for this particular task. In this paper, we present a novel method to enhance visibility for low-light hazy scenarios. To handle this challenging task, we propose two key techniques, namely cross-consistency <mark data-markjs="true">dehazing</mark>-enhancement framework and physically based simulation for low-light hazy dataset. Specifically, the framework is designed for enhancing visibility of the input image via fully utilizing the clues from different sub-tasks. The simulation is designed for generating the dataset with ground-truths by the proposed low-light hazy imaging model. The extensive experimental results show that the proposed method outperforms the SOTA solutions on different metrics including SSIM (9.19%) and PSNR(5.03%). In addition, we conduct a user study on real images to demonstrate the effectiveness and necessity of the proposed method by human visual perception.</p>
            <p id="subjects-2308.00591" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2308.00591" class="metainfo date"><strong>Publish</strong>: 2023-08-01 15:07:38 UTC</p>
            <div id="pdf-container-2308.00591" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2308.00591" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2308.00591" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.16050" class="panel paper" keywords="dehazing,hazy,haze,dataset,level,methods,image,video,dists,multi">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.16050" target="_blank" title="79/298"><span class="index notranslate">#79</span></a>
                <a id="title-2307.16050" class="title-link" href="/arxiv/2307.16050" target="_blank">A New Multi-Level Hazy Image and Video Dataset for Benchmark of <mark data-markjs="true">Dehazing</mark> Methods</a>
                <a id="pdf-2307.16050" class="title-pdf notranslate" onclick="togglePdf('2307.16050', 'https://arxiv.org/pdf/2307.16050', this)">[PDF<sup id="pdf-stars-2307.16050"></sup>]</a>
                <a id="copy-2307.16050" class="title-copy notranslate" onclick="copyToClipboard('2307.16050')">[Copy]</a>
                <a id="kimi-2307.16050" class="title-kimi notranslate" onclick="toggleKimi('2307.16050', this)">[Kimi<sup id="kimi-stars-2307.16050"></sup>]</a>
                <a id="rel-2307.16050" class="title-rel notranslate" onclick="openRelatedPapers('2307.16050')">[REL]</a>
            </h2>
            <p id="authors-2307.16050" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bedrettin Cetinkaya" target="_blank"><span class="author notranslate">Bedrettin Cetinkaya</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yucel Cimtay" target="_blank"><span class="author notranslate">Yucel Cimtay</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fatma Nazli Gunay" target="_blank"><span class="author notranslate">Fatma Nazli Gunay</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gokce Nur Yilmaz" target="_blank"><span class="author notranslate">Gokce Nur Yilmaz</span></a>
            </p>
            <p id="summary-2307.16050" class="summary">The changing level of haze is one of the main factors which affects the success of the proposed <mark data-markjs="true">dehazing</mark> methods. However, there is a lack of controlled multi-level hazy dataset in the literature. Therefore, in this study, a new multi-level hazy color image dataset is presented. Color video data is captured for two real scenes with a controlled level of haze. The distance of the scene objects from the camera, haze level, and ground truth (clear image) are available so that different <mark data-markjs="true">dehazing</mark> methods and models can be benchmarked. In this study, the <mark data-markjs="true">dehazing</mark> performance of five different <mark data-markjs="true">dehazing</mark> methods/models is compared on the dataset based on SSIM, PSNR, VSI and DISTS image quality metrics. Results show that traditional methods can generalize the <mark data-markjs="true">dehazing</mark> problem better than many deep learning based methods. The performance of deep models depends mostly on the scene and is generally poor on cross-dataset <mark data-markjs="true">dehazing</mark>.</p>
            <p id="subjects-2307.16050" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/eess.IV" target="_blank"><span class="subject"><strong>Image and Video Processing</strong></span></a>
            </p>
            <p id="date-2307.16050" class="metainfo date"><strong>Publish</strong>: 2023-07-29 18:48:28 UTC</p>
            <div id="pdf-container-2307.16050" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.16050" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.16050" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.13927" class="panel paper" keywords="dehazing,density,dfr,haze,hazy,differences,proposal,feature,refinement,features">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.13927" target="_blank" title="80/298"><span class="index notranslate">#80</span></a>
                <a id="title-2307.13927" class="title-link" href="/arxiv/2307.13927" target="_blank">DFR-Net: Density Feature Refinement Network for Image <mark data-markjs="true">Dehazing</mark> Utilizing Haze Density Difference</a>
                <a id="pdf-2307.13927" class="title-pdf notranslate" onclick="togglePdf('2307.13927', 'https://arxiv.org/pdf/2307.13927', this)">[PDF<sup id="pdf-stars-2307.13927"></sup>]</a>
                <a id="copy-2307.13927" class="title-copy notranslate" onclick="copyToClipboard('2307.13927')">[Copy]</a>
                <a id="kimi-2307.13927" class="title-kimi notranslate" onclick="toggleKimi('2307.13927', this)">[Kimi<sup id="kimi-stars-2307.13927"></sup>]</a>
                <a id="rel-2307.13927" class="title-rel notranslate" onclick="openRelatedPapers('2307.13927')">[REL]</a>
            </h2>
            <p id="authors-2307.13927" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhongze Wang" target="_blank"><span class="author notranslate">Zhongze Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haitao Zhao" target="_blank"><span class="author notranslate">Haitao Zhao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lujian Yao" target="_blank"><span class="author notranslate">Lujian Yao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingchao Peng" target="_blank"><span class="author notranslate">Jingchao Peng</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaijie Zhao" target="_blank"><span class="author notranslate">Kaijie Zhao</span></a>
            </p>
            <p id="summary-2307.13927" class="summary">In image <mark data-markjs="true">dehazing</mark> task, haze density is a key feature and affects the performance of <mark data-markjs="true">dehazing</mark> methods. However, some of the existing methods lack a comparative image to measure densities, and others create intermediate results but lack the exploitation of their density differences, which can facilitate perception of density. To address these deficiencies, we propose a density-aware <mark data-markjs="true">dehazing</mark> method named Density Feature Refinement Network (DFR-Net) that extracts haze density features from density differences and leverages density differences to refine density features. In DFR-Net, we first generate a proposal image that has lower overall density than the hazy input, bringing in global density differences. Additionally, the <mark data-markjs="true">dehazing</mark> residual of the proposal image reflects the level of <mark data-markjs="true">dehazing</mark> performance and provides local density differences that indicate localized hard <mark data-markjs="true">dehazing</mark> or high density areas. Subsequently, we introduce a Global Branch (GB) and a Local Branch (LB) to achieve density-awareness. In GB, we use Siamese networks for feature extraction of hazy inputs and proposal images, and we propose a Global Density Feature Refinement (GDFR) module that can refine features by pushing features with different global densities further away. In LB, we explore local density features from the <mark data-markjs="true">dehazing</mark> residuals between hazy inputs and proposal images and introduce an Intermediate <mark data-markjs="true">Dehazing</mark> Residual Feedforward (IDRF) module to update local features and pull them closer to clear image features. Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on various datasets.</p>
            <p id="subjects-2307.13927" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2307.13927" class="metainfo date"><strong>Publish</strong>: 2023-07-26 02:53:29 UTC</p>
            <div id="pdf-container-2307.13927" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.13927" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.13927" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.11204" class="panel paper" keywords="haze,ultrasound,dehazing,cardiac,diffusion,patients,imaging,tissue,intercostal,diagnosis">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.11204" target="_blank" title="81/298"><span class="index notranslate">#81</span></a>
                <a id="title-2307.11204" class="title-link" href="/arxiv/2307.11204" target="_blank"><mark data-markjs="true">Dehazing</mark> Ultrasound using Diffusion Models</a>
                <a id="pdf-2307.11204" class="title-pdf notranslate" onclick="togglePdf('2307.11204', 'https://arxiv.org/pdf/2307.11204', this)">[PDF<sup id="pdf-stars-2307.11204"></sup>]</a>
                <a id="copy-2307.11204" class="title-copy notranslate" onclick="copyToClipboard('2307.11204')">[Copy]</a>
                <a id="kimi-2307.11204" class="title-kimi notranslate" onclick="toggleKimi('2307.11204', this)">[Kimi<sup id="kimi-stars-2307.11204"></sup>]</a>
                <a id="rel-2307.11204" class="title-rel notranslate" onclick="openRelatedPapers('2307.11204')">[REL]</a>
            </h2>
            <p id="authors-2307.11204" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tristan S. W. Stevens" target="_blank"><span class="author notranslate">Tristan S. W. Stevens</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Faik C. Meral" target="_blank"><span class="author notranslate">Faik C. Meral</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jason Yu" target="_blank"><span class="author notranslate">Jason Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Iason Z. Apostolakis" target="_blank"><span class="author notranslate">Iason Z. Apostolakis</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jean-Luc Robert" target="_blank"><span class="author notranslate">Jean-Luc Robert</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruud J. G. van Sloun" target="_blank"><span class="author notranslate">Ruud J. G. van Sloun</span></a>
            </p>
            <p id="summary-2307.11204" class="summary">Echocardiography has been a prominent tool for the diagnosis of cardiac disease. However, these diagnoses can be heavily impeded by poor image quality. Acoustic clutter emerges due to multipath reflections imposed by layers of skin, subcutaneous fat, and intercostal muscle between the transducer and heart. As a result, haze and other noise artifacts pose a real challenge to cardiac ultrasound imaging. In many cases, especially with difficult-to-image patients such as patients with obesity, a diagnosis from B-Mode ultrasound imaging is effectively rendered unusable, forcing sonographers to resort to contrast-enhanced ultrasound examinations or refer patients to other imaging modalities. Tissue harmonic imaging has been a popular approach to combat haze, but in severe cases is still heavily impacted by haze. Alternatively, denoising algorithms are typically unable to remove highly structured and correlated noise, such as haze. It remains a challenge to accurately describe the statistical properties of structured haze, and develop an inference method to subsequently remove it. Diffusion models have emerged as powerful generative models and have shown their effectiveness in a variety of inverse problems. In this work, we present a joint posterior sampling framework that combines two separate diffusion models to model the distribution of both clean ultrasound and haze in an unsupervised manner. Furthermore, we demonstrate techniques for effectively training diffusion models on radio-frequency ultrasound data and highlight the advantages over image data. Experiments on both \emph{in-vitro} and \emph{in-vivo} cardiac datasets show that the proposed <mark data-markjs="true">dehazing</mark> method effectively removes haze while preserving signals from weakly reflected tissue.</p>
            <p id="subjects-2307.11204" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/eess.SP" target="_blank"><span class="subject"><strong>Signal Processing</strong></span></a>
            </p>
            <p id="date-2307.11204" class="metainfo date"><strong>Publish</strong>: 2023-07-20 19:46:57 UTC</p>
            <div id="pdf-container-2307.11204" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.11204" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.11204" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.05075" class="panel paper" keywords="degradations,uni,removal,world,images,real,supervised,addressing,stage,teacher">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.05075" target="_blank" title="82/298"><span class="index notranslate">#82</span></a>
                <a id="title-2307.05075" class="title-link" href="/arxiv/2307.05075" target="_blank">Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images</a>
                <a id="pdf-2307.05075" class="title-pdf notranslate" onclick="togglePdf('2307.05075', 'https://arxiv.org/pdf/2307.05075', this)">[PDF<sup id="pdf-stars-2307.05075"></sup>]</a>
                <a id="copy-2307.05075" class="title-copy notranslate" onclick="copyToClipboard('2307.05075')">[Copy]</a>
                <a id="kimi-2307.05075" class="title-kimi notranslate" onclick="toggleKimi('2307.05075', this)">[Kimi<sup id="kimi-stars-2307.05075"></sup>]</a>
                <a id="rel-2307.05075" class="title-rel notranslate" onclick="openRelatedPapers('2307.05075')">[REL]</a>
            </h2>
            <p id="authors-2307.05075" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yongheng Zhang" target="_blank"><span class="author notranslate">Yongheng Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Danfeng Yan" target="_blank"><span class="author notranslate">Danfeng Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuanqiang Cai" target="_blank"><span class="author notranslate">Yuanqiang Cai</span></a>
            </p>
            <p id="summary-2307.05075" class="summary">Removing multiple degradations, such as haze, rain, and blur, from real-world images poses a challenging and illposed problem. Recently, unified models that can handle different degradations have been proposed and yield promising results. However, these approaches focus on synthetic images and experience a significant performance drop when applied to realworld images. In this paper, we introduce Uni-Removal, a twostage semi-supervised framework for addressing the removal of multiple degradations in real-world images using a unified model and parameters. In the knowledge transfer stage, Uni-Removal leverages a supervised multi-teacher and student architecture in the knowledge transfer stage to facilitate learning from pretrained teacher networks specialized in different degradation types. A multi-grained contrastive loss is introduced to enhance learning from feature and image spaces. In the domain adaptation stage, unsupervised fine-tuning is performed by incorporating an adversarial discriminator on real-world images. The integration of an extended multi-grained contrastive loss and generative adversarial loss enables the adaptation of the student network from synthetic to real-world domains. Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method. We compare our Uni-Removal framework with state-of-the-art supervised and unsupervised methods, showcasing its promising results in real-world image <mark data-markjs="true">dehazing</mark>, deraining, and deblurring simultaneously.</p>
            <p id="subjects-2307.05075" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
            </p>
            <p id="date-2307.05075" class="metainfo date"><strong>Publish</strong>: 2023-07-11 07:18:15 UTC</p>
            <div id="pdf-container-2307.05075" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.05075" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.05075" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.04149" class="panel paper" keywords="lga,architectures,graph,attention,context,latent,global,image,extent,computationally">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.04149" target="_blank" title="83/298"><span class="index notranslate">#83</span></a>
                <a id="title-2307.04149" class="title-link" href="/arxiv/2307.04149" target="_blank">Latent Graph Attention for Enhanced Spatial Context</a>
                <a id="pdf-2307.04149" class="title-pdf notranslate" onclick="togglePdf('2307.04149', 'https://arxiv.org/pdf/2307.04149', this)">[PDF<sup id="pdf-stars-2307.04149"></sup>]</a>
                <a id="copy-2307.04149" class="title-copy notranslate" onclick="copyToClipboard('2307.04149')">[Copy]</a>
                <a id="kimi-2307.04149" class="title-kimi notranslate" onclick="toggleKimi('2307.04149', this)">[Kimi<sup id="kimi-stars-2307.04149"></sup>]</a>
                <a id="rel-2307.04149" class="title-rel notranslate" onclick="openRelatedPapers('2307.04149')">[REL]</a>
            </h2>
            <p id="authors-2307.04149" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ayush Singh" target="_blank"><span class="author notranslate">Ayush Singh</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yash Bhambhu" target="_blank"><span class="author notranslate">Yash Bhambhu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Himanshu Buckchash" target="_blank"><span class="author notranslate">Himanshu Buckchash</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Deepak K. Gupta" target="_blank"><span class="author notranslate">Deepak K. Gupta</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dilip K. Prasad" target="_blank"><span class="author notranslate">Dilip K. Prasad</span></a>
            </p>
            <p id="summary-2307.04149" class="summary">Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also takes into account the influence of the intermediate pixels. Moreover, the depth of the graph network can be used to adapt the extent of contextual spread to the target dataset, thereby being able to explicitly control the added computational cost. To enhance the learning mechanism of LGA, we also introduce a novel contrastive loss term that helps our LGA module to couple well with the original architecture at the expense of minimal additional computational load. We show that incorporating LGA improves the performance on three challenging applications, namely transparent object segmentation, image restoration for <mark data-markjs="true">dehazing</mark> and optical flow estimation.</p>
            <p id="subjects-2307.04149" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
            </p>
            <p id="date-2307.04149" class="metainfo date"><strong>Publish</strong>: 2023-07-09 10:56:44 UTC</p>
            <div id="pdf-container-2307.04149" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.04149" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.04149" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2307.04100" class="panel paper" keywords="fusion,ssl,visible,nir,rgb,multispectral,infrared,rgbt,self,supervised">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2307.04100" target="_blank" title="84/298"><span class="index notranslate">#84</span></a>
                <a id="title-2307.04100" class="title-link" href="/arxiv/2307.04100" target="_blank">Visible and infrared self-supervised fusion trained on a single example</a>
                <a id="pdf-2307.04100" class="title-pdf notranslate" onclick="togglePdf('2307.04100', 'https://arxiv.org/pdf/2307.04100', this)" style="color: purple;">[PDF<sup id="pdf-stars-2307.04100"></sup>]</a>
                <a id="copy-2307.04100" class="title-copy notranslate" onclick="copyToClipboard('2307.04100')">[Copy]</a>
                <a id="kimi-2307.04100" class="title-kimi notranslate" onclick="toggleKimi('2307.04100', this)">[Kimi<sup id="kimi-stars-2307.04100"></sup>]</a>
                <a id="rel-2307.04100" class="title-rel notranslate" onclick="openRelatedPapers('2307.04100')">[REL]</a>
            </h2>
            <p id="authors-2307.04100" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nati Ofir" target="_blank"><span class="author notranslate">Nati Ofir</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jean-Christophe Nebel" target="_blank"><span class="author notranslate">Jean-Christophe Nebel</span></a>
            </p>
            <p id="summary-2307.04100" class="summary">Multispectral imaging is an important task of image processing and computer vision, which is especially relevant to applications such as <mark data-markjs="true">dehazing</mark> or object detection. With the development of the RGBT (RGB &amp; Thermal) sensor, the problem of visible (RGB) to Near Infrared (NIR) image fusion has become particularly timely. Indeed, while visible images see color, but suffer from noise, haze, and clouds, the NIR channel captures a clearer picture. The proposed approach fuses these two channels by training a Convolutional Neural Network by Self Supervised Learning (SSL) on a single example. For each such pair, RGB and NIR, the network is trained for seconds to deduce the final fusion. The SSL is based on the comparison of the Structure of Similarity and Edge-Preservation losses, where the labels for the SSL are the input channels themselves. This fusion preserves the relevant detail of each spectral channel without relying on a heavy training process. Experiments demonstrate that the proposed approach achieves similar or better qualitative and quantitative multispectral fusion results than other state-of-the-art methods that do not rely on heavy training and/or large datasets.</p>
            <p id="subjects-2307.04100" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2307.04100" class="metainfo date"><strong>Publish</strong>: 2023-07-09 05:25:46 UTC</p>
            <div id="pdf-container-2307.04100" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2307.04100" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2307.04100" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2306.15870" class="panel paper" keywords="dehaze,level,tasks,vision,low,anything,large,model,datasets,integrate">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2306.15870" target="_blank" title="85/298"><span class="index notranslate">#85</span></a>
                <a id="title-2306.15870" class="title-link" href="/arxiv/2306.15870" target="_blank">Let Segment Anything Help Image Dehaze</a>
                <a id="pdf-2306.15870" class="title-pdf notranslate" onclick="togglePdf('2306.15870', 'https://arxiv.org/pdf/2306.15870', this)" style="color: purple;">[PDF<sup id="pdf-stars-2306.15870"></sup>]</a>
                <a id="copy-2306.15870" class="title-copy notranslate" onclick="copyToClipboard('2306.15870')">[Copy]</a>
                <a id="kimi-2306.15870" class="title-kimi notranslate" onclick="toggleKimi('2306.15870', this)">[Kimi<sup id="kimi-stars-2306.15870"></sup>]</a>
                <a id="rel-2306.15870" class="title-rel notranslate" onclick="openRelatedPapers('2306.15870')">[REL]</a>
            </h2>
            <p id="authors-2306.15870" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zheyan Jin" target="_blank"><span class="author notranslate">Zheyan Jin</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shiqi Chen" target="_blank"><span class="author notranslate">Shiqi Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yueting Chen" target="_blank"><span class="author notranslate">Yueting Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhihai Xu" target="_blank"><span class="author notranslate">Zhihai Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huajun Feng" target="_blank"><span class="author notranslate">Huajun Feng</span></a>
            </p>
            <p id="summary-2306.15870" class="summary">The large language model and high-level vision model have achieved impressive performance improvements with large datasets and model sizes. However, low-level computer vision tasks, such as image dehaze and blur removal, still rely on a small number of datasets and small-sized models, which generally leads to overfitting and local optima. Therefore, we propose a framework to integrate large-model prior into low-level computer vision tasks. Just as with the task of image segmentation, the degradation of haze is also texture-related. So we propose to detect gray-scale coding, network channel expansion, and pre-dehaze structures to integrate large-model prior knowledge into any low-level <mark data-markjs="true">dehazing</mark> network. We demonstrate the effectiveness and applicability of large models in guiding low-level visual tasks through different datasets and algorithms comparison experiments. Finally, we demonstrate the effect of grayscale coding, network channel expansion, and recurrent network structures through ablation experiments. Under the conditions where additional data and training resources are not required, we successfully prove that the integration of large-model prior knowledge will improve the dehaze performance and save training time for low-level visual tasks.</p>
            <p id="subjects-2306.15870" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2306.15870" class="metainfo date"><strong>Publish</strong>: 2023-06-28 02:02:19 UTC</p>
            <div id="pdf-container-2306.15870" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2306.15870" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2306.15870" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2306.13090" class="panel paper" keywords="promptir,degradation,restoration,image,levels,types,va1shn9v,restore,prompts,prompting">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2306.13090" target="_blank" title="86/298"><span class="index notranslate">#86</span></a>
                <a id="title-2306.13090" class="title-link" href="/arxiv/2306.13090" target="_blank">PromptIR: Prompting for All-in-One Blind Image Restoration</a>
                <a id="pdf-2306.13090" class="title-pdf notranslate" onclick="togglePdf('2306.13090', 'https://arxiv.org/pdf/2306.13090', this)">[PDF<sup id="pdf-stars-2306.13090">1</sup>]</a>
                <a id="copy-2306.13090" class="title-copy notranslate" onclick="copyToClipboard('2306.13090')">[Copy]</a>
                <a id="kimi-2306.13090" class="title-kimi notranslate" onclick="toggleKimi('2306.13090', this)">[Kimi<sup id="kimi-stars-2306.13090">3</sup>]</a>
                <a id="rel-2306.13090" class="title-rel notranslate" onclick="openRelatedPapers('2306.13090')">[REL]</a>
            </h2>
            <p id="authors-2306.13090" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vaishnav Potlapalli" target="_blank"><span class="author notranslate">Vaishnav Potlapalli</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Syed Waqas Zamir" target="_blank"><span class="author notranslate">Syed Waqas Zamir</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Salman Khan" target="_blank"><span class="author notranslate">Salman Khan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fahad Shahbaz Khan" target="_blank"><span class="author notranslate">Fahad Shahbaz Khan</span></a>
            </p>
            <p id="summary-2306.13090" class="summary">Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and <mark data-markjs="true">dehazing</mark>. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pretrained models are available here: https://github.com/va1shn9v/PromptIR</p>
            <p id="subjects-2306.13090" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2306.13090" class="metainfo date"><strong>Publish</strong>: 2023-06-22 17:59:52 UTC</p>
            <div id="pdf-container-2306.13090" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2306.13090" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2306.13090" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2306.07244" class="panel paper" keywords="dust,tractor,removal,tillage,dataset,vision,agricultural,agriscapes,based,bavaria">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2306.07244" target="_blank" title="87/298"><span class="index notranslate">#87</span></a>
                <a id="title-2306.07244" class="title-link" href="/arxiv/2306.07244" target="_blank">RB-Dust -- A Reference-based Dataset for Vision-based Dust Removal</a>
                <a id="pdf-2306.07244" class="title-pdf notranslate" onclick="togglePdf('2306.07244', 'https://arxiv.org/pdf/2306.07244', this)">[PDF<sup id="pdf-stars-2306.07244"></sup>]</a>
                <a id="copy-2306.07244" class="title-copy notranslate" onclick="copyToClipboard('2306.07244')">[Copy]</a>
                <a id="kimi-2306.07244" class="title-kimi notranslate" onclick="toggleKimi('2306.07244', this)">[Kimi<sup id="kimi-stars-2306.07244"></sup>]</a>
                <a id="rel-2306.07244" class="title-rel notranslate" onclick="openRelatedPapers('2306.07244')">[REL]</a>
            </h2>
            <p id="authors-2306.07244" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peter Buckel" target="_blank"><span class="author notranslate">Peter Buckel</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Timo Oksanen" target="_blank"><span class="author notranslate">Timo Oksanen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Thomas Dietmueller" target="_blank"><span class="author notranslate">Thomas Dietmueller</span></a>
            </p>
            <p id="summary-2306.07244" class="summary">Dust in the agricultural landscape is a significant challenge and influences, for example, the environmental perception of autonomous agricultural machines. Image enhancement algorithms can be used to reduce dust. However, these require dusty and dust-free images of the same environment for validation. In fact, to date, there is no dataset that we are aware of that addresses this issue. Therefore, we present the agriscapes RB-Dust dataset, which is named after its purpose of reference-based dust removal. It is not possible to take pictures from the cabin during tillage, as this would cause shifts in the images. Because of this, we built a setup from which it is possible to take images from a stationary position close to the passing tractor. The test setup was based on a half-sided gate through which the tractor could drive. The field tests were carried out on a farm in Bavaria, Germany, during tillage. During the field tests, other parameters such as soil moisture and wind speed were controlled, as these significantly affect dust development. We validated our dataset with contrast enhancement and image <mark data-markjs="true">dehazing</mark> algorithms and analyzed the generalizability from recordings from the moving tractor. Finally, we demonstrate the application of dust removal based on a high-level vision task, such as person classification. Our empirical study confirms the validity of RB-Dust for vision-based dust removal in agriculture.</p>
            <p id="subjects-2306.07244" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2306.07244" class="metainfo date"><strong>Publish</strong>: 2023-06-12 17:09:24 UTC</p>
            <div id="pdf-container-2306.07244" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2306.07244" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2306.07244" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2306.06288" class="panel paper" keywords="dehazing,ndvi,metric,stereotype,remote,ground,sage,image,sensing,phenology">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2306.06288" target="_blank" title="88/298"><span class="index notranslate">#88</span></a>
                <a id="title-2306.06288" class="title-link" href="/arxiv/2306.06288" target="_blank">SAGE-NDVI: A Stereotype-Breaking Evaluation Metric for Remote Sensing Image <mark data-markjs="true">Dehazing</mark> Using Satellite-to-Ground NDVI Knowledge</a>
                <a id="pdf-2306.06288" class="title-pdf notranslate" onclick="togglePdf('2306.06288', 'https://arxiv.org/pdf/2306.06288', this)" style="color: purple;">[PDF<sup id="pdf-stars-2306.06288"></sup>]</a>
                <a id="copy-2306.06288" class="title-copy notranslate" onclick="copyToClipboard('2306.06288')">[Copy]</a>
                <a id="kimi-2306.06288" class="title-kimi notranslate" onclick="toggleKimi('2306.06288', this)">[Kimi<sup id="kimi-stars-2306.06288"></sup>]</a>
                <a id="rel-2306.06288" class="title-rel notranslate" onclick="openRelatedPapers('2306.06288')">[REL]</a>
            </h2>
            <p id="authors-2306.06288" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zepeng Liu" target="_blank"><span class="author notranslate">Zepeng Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhicheng Yang" target="_blank"><span class="author notranslate">Zhicheng Yang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingye Zhu" target="_blank"><span class="author notranslate">Mingye Zhu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andy Wong" target="_blank"><span class="author notranslate">Andy Wong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yibing Wei" target="_blank"><span class="author notranslate">Yibing Wei</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mei Han" target="_blank"><span class="author notranslate">Mei Han</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Yu" target="_blank"><span class="author notranslate">Jun Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jui-Hsin Lai" target="_blank"><span class="author notranslate">Jui-Hsin Lai</span></a>
            </p>
            <p id="summary-2306.06288" class="summary">Image <mark data-markjs="true">dehazing</mark> is a meaningful low-level computer vision task and can be applied to a variety of contexts. In our industrial deployment scenario based on remote sensing (RS) images, the quality of image <mark data-markjs="true">dehazing</mark> directly affects the grade of our crop identification and growth monitoring products. However, the widely used peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) provide ambiguous visual interpretation. In this paper, we design a new objective metric for RS image <mark data-markjs="true">dehazing</mark> evaluation. Our proposed metric leverages a ground-based phenology observation resource to calculate the vegetation index error between RS and ground images at a hazy date. Extensive experiments validate that our metric appropriately evaluates different <mark data-markjs="true">dehazing</mark> models and is in line with human visual perception.</p>
            <p id="subjects-2306.06288" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2306.06288" class="metainfo date"><strong>Publish</strong>: 2023-06-09 22:29:42 UTC</p>
            <div id="pdf-container-2306.06288" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2306.06288" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2306.06288" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2306.05675" class="panel paper" keywords="dehazing,illumination,retinex,controllable,icdehazing,xiaofeng,unsupervised,illposedness,dehazed,network">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2306.05675" target="_blank" title="89/298"><span class="index notranslate">#89</span></a>
                <a id="title-2306.05675" class="title-link" href="/arxiv/2306.05675" target="_blank">Illumination Controllable <mark data-markjs="true">Dehazing</mark> Network based on Unsupervised Retinex Embedding</a>
                <a id="pdf-2306.05675" class="title-pdf notranslate" onclick="togglePdf('2306.05675', 'https://arxiv.org/pdf/2306.05675', this)" style="color: purple;">[PDF<sup id="pdf-stars-2306.05675"></sup>]</a>
                <a id="copy-2306.05675" class="title-copy notranslate" onclick="copyToClipboard('2306.05675')">[Copy]</a>
                <a id="kimi-2306.05675" class="title-kimi notranslate" onclick="toggleKimi('2306.05675', this)">[Kimi<sup id="kimi-stars-2306.05675"></sup>]</a>
                <a id="rel-2306.05675" class="title-rel notranslate" onclick="openRelatedPapers('2306.05675')">[REL]</a>
            </h2>
            <p id="authors-2306.05675" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie Gui" target="_blank"><span class="author notranslate">Jie Gui</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaofeng Cong" target="_blank"><span class="author notranslate">Xiaofeng Cong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lei He" target="_blank"><span class="author notranslate">Lei He</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuan Yan Tang" target="_blank"><span class="author notranslate">Yuan Yan Tang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=James Tin-Yau Kwok" target="_blank"><span class="author notranslate">James Tin-Yau Kwok</span></a>
            </p>
            <p id="summary-2306.05675" class="summary">On the one hand, the <mark data-markjs="true">dehazing</mark> task is an illposedness problem, which means that no unique solution exists. On the other hand, the <mark data-markjs="true">dehazing</mark> task should take into account the subjective factor, which is to give the user selectable dehazed images rather than a single result. Therefore, this paper proposes a multi-output <mark data-markjs="true">dehazing</mark> network by introducing illumination controllable ability, called IC-<mark data-markjs="true">Dehazing</mark>. The proposed IC-<mark data-markjs="true">Dehazing</mark> can change the illumination intensity by adjusting the factor of the illumination controllable module, which is realized based on the interpretable Retinex theory. Moreover, the backbone <mark data-markjs="true">dehazing</mark> network of IC-<mark data-markjs="true">Dehazing</mark> consists of a Transformer with double decoders for high-quality image restoration. Further, the prior-based loss function and unsupervised training strategy enable IC-<mark data-markjs="true">Dehazing</mark> to complete the parameter learning process without the need for paired data. To demonstrate the effectiveness of the proposed IC-<mark data-markjs="true">Dehazing</mark>, quantitative and qualitative experiments are conducted on image <mark data-markjs="true">dehazing</mark>, semantic segmentation, and object detection tasks. Code is available at https://github.com/Xiaofeng-life/ICDehazing.</p>
            <p id="subjects-2306.05675" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2306.05675" class="metainfo date"><strong>Publish</strong>: 2023-06-09 05:19:51 UTC</p>
            <div id="pdf-container-2306.05675" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2306.05675" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2306.05675" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.17863" class="panel paper" keywords="gridformer,restoration,weather,adverse,transformer,deraining,dehazing,residual,dense,image">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.17863" target="_blank" title="90/298"><span class="index notranslate">#90</span></a>
                <a id="title-2305.17863" class="title-link" href="/arxiv/2305.17863" target="_blank">GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions</a>
                <a id="pdf-2305.17863" class="title-pdf notranslate" onclick="togglePdf('2305.17863', 'https://arxiv.org/pdf/2305.17863', this)">[PDF<sup id="pdf-stars-2305.17863"></sup>]</a>
                <a id="copy-2305.17863" class="title-copy notranslate" onclick="copyToClipboard('2305.17863')">[Copy]</a>
                <a id="kimi-2305.17863" class="title-kimi notranslate" onclick="toggleKimi('2305.17863', this)">[Kimi<sup id="kimi-stars-2305.17863"></sup>]</a>
                <a id="rel-2305.17863" class="title-rel notranslate" onclick="openRelatedPapers('2305.17863')">[REL]</a>
            </h2>
            <p id="authors-2305.17863" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tao Wang" target="_blank"><span class="author notranslate">Tao Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaihao Zhang" target="_blank"><span class="author notranslate">Kaihao Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziqian Shao" target="_blank"><span class="author notranslate">Ziqian Shao</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenhan Luo" target="_blank"><span class="author notranslate">Wenhan Luo</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bjorn Stenger" target="_blank"><span class="author notranslate">Bjorn Stenger</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tong Lu" target="_blank"><span class="author notranslate">Tong Lu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tae-Kyun Kim" target="_blank"><span class="author notranslate">Tae-Kyun Kim</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Liu" target="_blank"><span class="author notranslate">Wei Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongdong Li" target="_blank"><span class="author notranslate">Hongdong Li</span></a>
            </p>
            <p id="summary-2305.17863" class="summary">Image restoration in adverse weather conditions is a difficult task in computer vision. In this paper, we propose a novel transformer-based framework called GridFormer which serves as a backbone for image restoration under adverse weather conditions. GridFormer is designed in a grid structure using a residual dense transformer block, and it introduces two core designs. First, it uses an enhanced attention mechanism in the transformer layer. The mechanism includes stages of the sampler and compact self-attention to improve efficiency, and a local enhancement stage to strengthen local information. Second, we introduce a residual dense transformer block (RDTB) as the final GridFormer layer. This design further improves the network's ability to learn effective features from both preceding and current local features. The GridFormer framework achieves state-of-the-art results on five diverse image restoration tasks in adverse weather conditions, including image deraining, <mark data-markjs="true">dehazing</mark>, deraining \&amp; <mark data-markjs="true">dehazing</mark>, desnowing, and multi-weather restoration. The source code and pre-trained models are available at https://github.com/TaoWangzj/GridFormer.</p>
            <p id="subjects-2305.17863" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.17863" class="metainfo date"><strong>Publish</strong>: 2023-05-29 03:03:53 UTC</p>
            <div id="pdf-container-2305.17863" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.17863" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.17863" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.17654" class="panel paper" keywords="mixdehazenet,dehazing,parallel,kernel,attention,module,hazy,mix,uneven,ameryxiong">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.17654" target="_blank" title="91/298"><span class="index notranslate">#91</span></a>
                <a id="title-2305.17654" class="title-link" href="/arxiv/2305.17654" target="_blank">MixDehazeNet : Mix Structure Block For Image <mark data-markjs="true">Dehazing</mark> Network</a>
                <a id="pdf-2305.17654" class="title-pdf notranslate" onclick="togglePdf('2305.17654', 'https://arxiv.org/pdf/2305.17654', this)" style="color: purple;">[PDF<sup id="pdf-stars-2305.17654"></sup>]</a>
                <a id="copy-2305.17654" class="title-copy notranslate" onclick="copyToClipboard('2305.17654')">[Copy]</a>
                <a id="kimi-2305.17654" class="title-kimi notranslate" onclick="toggleKimi('2305.17654', this)">[Kimi<sup id="kimi-stars-2305.17654"></sup>]</a>
                <a id="rel-2305.17654" class="title-rel notranslate" onclick="openRelatedPapers('2305.17654')">[REL]</a>
            </h2>
            <p id="authors-2305.17654" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=LiPing Lu" target="_blank"><span class="author notranslate">LiPing Lu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qian Xiong" target="_blank"><span class="author notranslate">Qian Xiong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=DuanFeng Chu" target="_blank"><span class="author notranslate">DuanFeng Chu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=BingRong Xu" target="_blank"><span class="author notranslate">BingRong Xu</span></a>
            </p>
            <p id="summary-2305.17654" class="summary">Image <mark data-markjs="true">dehazing</mark> is a typical task in the low-level vision field. Previous studies verified the effectiveness of the large convolutional kernel and attention mechanism in <mark data-markjs="true">dehazing</mark>. However, there are two drawbacks: the multi-scale properties of an image are readily ignored when a large convolutional kernel is introduced, and the standard series connection of an attention module does not sufficiently consider an uneven hazy distribution. In this paper, we propose a novel framework named Mix Structure Image <mark data-markjs="true">Dehazing</mark> Network (MixDehazeNet), which solves two issues mentioned above. Specifically, it mainly consists of two parts: the multi-scale parallel large convolution kernel module and the enhanced parallel attention module. Compared with a single large kernel, parallel large kernels with multi-scale are more capable of taking partial texture into account during the <mark data-markjs="true">dehazing</mark> phase. In addition, an enhanced parallel attention module is developed, in which parallel connections of attention perform better at <mark data-markjs="true">dehazing</mark> uneven hazy distribution. Extensive experiments on three benchmarks demonstrate the effectiveness of our proposed methods. For example, compared with the previous state-of-the-art methods, MixDehazeNet achieves a significant improvement (42.62dB PSNR) on the SOTS indoor dataset. The code is released in https://github.com/AmeryXiong/MixDehazeNet.</p>
            <p id="subjects-2305.17654" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.17654" class="metainfo date"><strong>Publish</strong>: 2023-05-28 07:41:10 UTC</p>
            <div id="pdf-container-2305.17654" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.17654" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.17654" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.16481" class="panel paper" keywords="simhaze,hazy,dehazing,clean,images,engine,depth,paired,game,synthesized">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.16481" target="_blank" title="92/298"><span class="index notranslate">#92</span></a>
                <a id="title-2305.16481" class="title-link" href="/arxiv/2305.16481" target="_blank">SimHaze: game engine simulated data for real-world <mark data-markjs="true">dehazing</mark></a>
                <a id="pdf-2305.16481" class="title-pdf notranslate" onclick="togglePdf('2305.16481', 'https://arxiv.org/pdf/2305.16481', this)">[PDF<sup id="pdf-stars-2305.16481"></sup>]</a>
                <a id="copy-2305.16481" class="title-copy notranslate" onclick="copyToClipboard('2305.16481')">[Copy]</a>
                <a id="kimi-2305.16481" class="title-kimi notranslate" onclick="toggleKimi('2305.16481', this)">[Kimi<sup id="kimi-stars-2305.16481"></sup>]</a>
                <a id="rel-2305.16481" class="title-rel notranslate" onclick="openRelatedPapers('2305.16481')">[REL]</a>
            </h2>
            <p id="authors-2305.16481" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhengyang Lou" target="_blank"><span class="author notranslate">Zhengyang Lou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huan Xu" target="_blank"><span class="author notranslate">Huan Xu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fangzhou Mu" target="_blank"><span class="author notranslate">Fangzhou Mu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yanli Liu" target="_blank"><span class="author notranslate">Yanli Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaoyu Zhang" target="_blank"><span class="author notranslate">Xiaoyu Zhang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liang Shang" target="_blank"><span class="author notranslate">Liang Shang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiang Li" target="_blank"><span class="author notranslate">Jiang Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bochen Guan" target="_blank"><span class="author notranslate">Bochen Guan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yin Li" target="_blank"><span class="author notranslate">Yin Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu Hen Hu" target="_blank"><span class="author notranslate">Yu Hen Hu</span></a>
            </p>
            <p id="summary-2305.16481" class="summary">Deep models have demonstrated recent success in single-image <mark data-markjs="true">dehazing</mark>. Most prior methods consider fully supervised training and learn from paired clean and hazy images, where a hazy image is synthesized based on a clean image and its estimated depth map. This paradigm, however, can produce low-quality hazy images due to inaccurate depth estimation, resulting in poor generalization of the trained models. In this paper, we explore an alternative approach for generating paired clean-hazy images by leveraging computer graphics. Using a modern game engine, our approach renders crisp clean images and their precise depth maps, based on which high-quality hazy images can be synthesized for training <mark data-markjs="true">dehazing</mark> models. To this end, we present SimHaze: a new synthetic haze dataset. More importantly, we show that training with SimHaze alone allows the latest <mark data-markjs="true">dehazing</mark> models to achieve significantly better performance in comparison to previous <mark data-markjs="true">dehazing</mark> datasets. Our dataset and code will be made publicly available.</p>
            <p id="subjects-2305.16481" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.16481" class="metainfo date"><strong>Publish</strong>: 2023-05-25 21:26:43 UTC</p>
            <div id="pdf-container-2305.16481" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.16481" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.16481" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.13819" class="panel paper" keywords="wavedm,wavelet,restoration,diffusion,image,steaks,sampling,removal,demoiring,raindrop">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.13819" target="_blank" title="93/298"><span class="index notranslate">#93</span></a>
                <a id="title-2305.13819" class="title-link" href="/arxiv/2305.13819" target="_blank">WaveDM: Wavelet-Based Diffusion Models for Image Restoration</a>
                <a id="pdf-2305.13819" class="title-pdf notranslate" onclick="togglePdf('2305.13819', 'https://arxiv.org/pdf/2305.13819', this)">[PDF<sup id="pdf-stars-2305.13819"></sup>]</a>
                <a id="copy-2305.13819" class="title-copy notranslate" onclick="copyToClipboard('2305.13819')">[Copy]</a>
                <a id="kimi-2305.13819" class="title-kimi notranslate" onclick="toggleKimi('2305.13819', this)">[Kimi<sup id="kimi-stars-2305.13819"></sup>]</a>
                <a id="rel-2305.13819" class="title-rel notranslate" onclick="openRelatedPapers('2305.13819')">[REL]</a>
            </h2>
            <p id="authors-2305.13819" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yi Huang" target="_blank"><span class="author notranslate">Yi Huang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiancheng Huang" target="_blank"><span class="author notranslate">Jiancheng Huang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianzhuang Liu" target="_blank"><span class="author notranslate">Jianzhuang Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingfu Yan" target="_blank"><span class="author notranslate">Mingfu Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu Dong" target="_blank"><span class="author notranslate">Yu Dong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiaxi Lv" target="_blank"><span class="author notranslate">Jiaxi Lv</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chaoqi Chen" target="_blank"><span class="author notranslate">Chaoqi Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shifeng Chen" target="_blank"><span class="author notranslate">Shifeng Chen</span></a>
            </p>
            <p id="summary-2305.13819" class="summary">Latest diffusion-based methods for many image restoration tasks outperform traditional models, but they encounter the long-time inference problem. To tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM). WaveDM learns the distribution of clean images in the wavelet domain conditioned on the wavelet spectrum of degraded images after wavelet transform, which is more time-saving in each step of sampling than modeling in the spatial domain. To ensure restoration performance, a unique training strategy is proposed where the low-frequency and high-frequency spectrums are learned using distinct modules. In addition, an Efficient Conditional Sampling (ECS) strategy is developed from experiments, which reduces the number of total sampling steps to around 5. Evaluations on twelve benchmark datasets including image raindrop removal, rain steaks removal, <mark data-markjs="true">dehazing</mark>, defocus deblurring, demoiring, and denoising demonstrate that WaveDM achieves state-of-the-art performance with the efficiency that is comparable to traditional one-pass methods and over 100<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x00D7;</mo></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-26" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.781em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.54em, 1000.63em, 2.335em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-27"><span class="mo" id="MathJax-Span-28" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.052em; border-left: 0px solid; width: 0px; height: 0.703em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-6">\times</script> faster than existing image restoration methods using vanilla diffusion models.</p>
            <p id="subjects-2305.13819" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.13819" class="metainfo date"><strong>Publish</strong>: 2023-05-23 08:41:04 UTC</p>
            <div id="pdf-container-2305.13819" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.13819" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.13819" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.09533" class="panel paper" keywords="nighttime,nighthazeformer,haze,removal,hazy,dehazing,degradations,transformer,fine,tuning">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.09533" target="_blank" title="94/298"><span class="index notranslate">#94</span></a>
                <a id="title-2305.09533" class="title-link" href="/arxiv/2305.09533" target="_blank">NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer</a>
                <a id="pdf-2305.09533" class="title-pdf notranslate" onclick="togglePdf('2305.09533', 'https://arxiv.org/pdf/2305.09533', this)">[PDF<sup id="pdf-stars-2305.09533"></sup>]</a>
                <a id="copy-2305.09533" class="title-copy notranslate" onclick="copyToClipboard('2305.09533')">[Copy]</a>
                <a id="kimi-2305.09533" class="title-kimi notranslate" onclick="toggleKimi('2305.09533', this)">[Kimi<sup id="kimi-stars-2305.09533"></sup>]</a>
                <a id="rel-2305.09533" class="title-rel notranslate" onclick="openRelatedPapers('2305.09533')">[REL]</a>
            </h2>
            <p id="authors-2305.09533" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yun Liu" target="_blank"><span class="author notranslate">Yun Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhongsheng Yan" target="_blank"><span class="author notranslate">Zhongsheng Yan</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sixiang Chen" target="_blank"><span class="author notranslate">Sixiang Chen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tian Ye" target="_blank"><span class="author notranslate">Tian Ye</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenqi Ren" target="_blank"><span class="author notranslate">Wenqi Ren</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Erkang Chen" target="_blank"><span class="author notranslate">Erkang Chen</span></a>
            </p>
            <p id="summary-2305.09533" class="summary">Nighttime image <mark data-markjs="true">dehazing</mark> is a challenging task due to the presence of multiple types of adverse degrading effects including glow, haze, blurry, noise, color distortion, and so on. However, most previous studies mainly focus on daytime image <mark data-markjs="true">dehazing</mark> or partial degradations presented in nighttime hazy scenes, which may lead to unsatisfactory restoration results. In this paper, we propose an end-to-end transformer-based framework for nighttime haze removal, called NightHazeFormer. Our proposed approach consists of two stages: supervised pre-training and semi-supervised fine-tuning. During the pre-training stage, we introduce two powerful priors into the transformer decoder to generate the non-learnable prior queries, which guide the model to extract specific degradations. For the fine-tuning, we combine the generated pseudo ground truths with input real-world nighttime hazy images as paired images and feed into the synthetic domain to fine-tune the pre-trained model. This semi-supervised fine-tuning paradigm helps improve the generalization to real domain. In addition, we also propose a large-scale synthetic dataset called UNREAL-NH, to simulate the real-world nighttime haze scenarios comprehensively. Extensive experiments on several synthetic and real-world datasets demonstrate the superiority of our NightHazeFormer over state-of-the-art nighttime haze removal methods in terms of both visually and quantitatively.</p>
            <p id="subjects-2305.09533" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.09533" class="metainfo date"><strong>Publish</strong>: 2023-05-16 15:26:09 UTC</p>
            <div id="pdf-container-2305.09533" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.09533" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.09533" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.04430" class="panel paper" keywords="haze,ffc,dwt,convnext,dehazing,homogeneous,branch,convolution,wavelete,zhouh115">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.04430" target="_blank" title="95/298"><span class="index notranslate">#95</span></a>
                <a id="title-2305.04430" class="title-link" href="/arxiv/2305.04430" target="_blank">Breaking Through the Haze: An Advanced Non-Homogeneous <mark data-markjs="true">Dehazing</mark> Method based on Fast Fourier Convolution and ConvNeXt</a>
                <a id="pdf-2305.04430" class="title-pdf notranslate" onclick="togglePdf('2305.04430', 'https://arxiv.org/pdf/2305.04430', this)">[PDF<sup id="pdf-stars-2305.04430"></sup>]</a>
                <a id="copy-2305.04430" class="title-copy notranslate" onclick="copyToClipboard('2305.04430')">[Copy]</a>
                <a id="kimi-2305.04430" class="title-kimi notranslate" onclick="toggleKimi('2305.04430', this)">[Kimi<sup id="kimi-stars-2305.04430"></sup>]</a>
                <a id="rel-2305.04430" class="title-rel notranslate" onclick="openRelatedPapers('2305.04430')">[REL]</a>
            </h2>
            <p id="authors-2305.04430" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Han Zhou" target="_blank"><span class="author notranslate">Han Zhou</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Dong" target="_blank"><span class="author notranslate">Wei Dong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yangyi Liu" target="_blank"><span class="author notranslate">Yangyi Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun Chen" target="_blank"><span class="author notranslate">Jun Chen</span></a>
            </p>
            <p id="summary-2305.04430" class="summary">Haze usually leads to deteriorated images with low contrast, color shift and structural distortion. We observe that many deep learning based models exhibit exceptional performance on removing homogeneous haze, but they usually fail to address the challenge of non-homogeneous <mark data-markjs="true">dehazing</mark>. Two main factors account for this situation. Firstly, due to the intricate and non uniform distribution of dense haze, the recovery of structural and chromatic features with high fidelity is challenging, particularly in regions with heavy haze. Secondly, the existing small scale datasets for non-homogeneous <mark data-markjs="true">dehazing</mark> are inadequate to support reliable learning of feature mappings between hazy images and their corresponding haze-free counterparts by convolutional neural network (CNN)-based models. To tackle these two challenges, we propose a novel two branch network that leverages 2D discrete wavelete transform (DWT), fast Fourier convolution (FFC) residual block and a pretrained ConvNeXt model. Specifically, in the DWT-FFC frequency branch, our model exploits DWT to capture more high-frequency features. Moreover, by taking advantage of the large receptive field provided by FFC residual blocks, our model is able to effectively explore global contextual information and produce images with better perceptual quality. In the prior knowledge branch, an ImageNet pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to learn more supplementary information and acquire a stronger generalization ability. The feasibility and effectiveness of the proposed method is demonstrated via extensive experiments and ablation studies. The code is available at https://github.com/zhouh115/DWT-FFC.</p>
            <p id="subjects-2305.04430" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/cs.AI" target="_blank"><span class="subject">Artificial Intelligence</span></a>
                ; <a href="/arxiv/cs.GR" target="_blank"><span class="subject">Graphics</span></a>
                ; <a href="/arxiv/cs.IR" target="_blank"><span class="subject">Information Retrieval</span></a>
                ; <a href="/arxiv/cs.LG" target="_blank"><span class="subject">Machine Learning</span></a>
            </p>
            <p id="date-2305.04430" class="metainfo date"><strong>Publish</strong>: 2023-05-08 02:59:02 UTC</p>
            <div id="pdf-container-2305.04430" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.04430" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.04430" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.02103" class="panel paper" keywords="fog,scatternerf,weather,rendering,inclement,scene,inverse,seeing,severly,autonomous">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.02103" target="_blank" title="96/298"><span class="index notranslate">#96</span></a>
                <a id="title-2305.02103" class="title-link" href="/arxiv/2305.02103" target="_blank">ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering</a>
                <a id="pdf-2305.02103" class="title-pdf notranslate" onclick="togglePdf('2305.02103', 'https://arxiv.org/pdf/2305.02103', this)">[PDF<sup id="pdf-stars-2305.02103"></sup>]</a>
                <a id="copy-2305.02103" class="title-copy notranslate" onclick="copyToClipboard('2305.02103')">[Copy]</a>
                <a id="kimi-2305.02103" class="title-kimi notranslate" onclick="toggleKimi('2305.02103', this)">[Kimi<sup id="kimi-stars-2305.02103"></sup>]</a>
                <a id="rel-2305.02103" class="title-rel notranslate" onclick="openRelatedPapers('2305.02103')">[REL]</a>
            </h2>
            <p id="authors-2305.02103" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrea Ramazzina" target="_blank"><span class="author notranslate">Andrea Ramazzina</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mario Bijelic" target="_blank"><span class="author notranslate">Mario Bijelic</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Stefanie Walz" target="_blank"><span class="author notranslate">Stefanie Walz</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alessandro Sanvito" target="_blank"><span class="author notranslate">Alessandro Sanvito</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dominik Scheuble" target="_blank"><span class="author notranslate">Dominik Scheuble</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Felix Heide" target="_blank"><span class="author notranslate">Felix Heide</span></a>
            </p>
            <p id="summary-2305.02103" class="summary">Vision in adverse weather conditions, whether it be snow, rain, or fog is challenging. In these scenarios, scattering and attenuation severly degrades image quality. Handling such inclement weather conditions, however, is essential to operate autonomous vehicles, drones and robotic applications where human performance is impeded the most. A large body of work explores removing weather-induced image degradations with <mark data-markjs="true">dehazing</mark> methods. Most methods rely on single images as input and struggle to generalize from synthetic fully-supervised training approaches or to generate high fidelity results from unpaired real-world datasets. With data as bottleneck and most of today's training data relying on good weather conditions with inclement weather as outlier, we rely on an inverse rendering approach to reconstruct the scene content. We introduce ScatterNeRF, a neural rendering method which adequately renders foggy scenes and decomposes the fog-free background from the participating media-exploiting the multiple views from a short automotive sequence without the need for a large training data corpus. Instead, the rendering approach is optimized on the multi-view scene itself, which can be typically captured by an autonomous vehicle, robot or drone during operation. Specifically, we propose a disentangled representation for the scattering volume and the scene objects, and learn the scene reconstruction with physics-inspired losses. We validate our method by capturing multi-view In-the-Wild data and controlled captures in a large-scale fog chamber.</p>
            <p id="subjects-2305.02103" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2305.02103" class="metainfo date"><strong>Publish</strong>: 2023-05-03 13:24:06 UTC</p>
            <div id="pdf-container-2305.02103" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.02103" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.02103" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2305.00273" class="panel paper" keywords="restoration,unsupervised,sparsity,sot,tasks,deraining,dehazing,supervised,methods,transport">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2305.00273" target="_blank" title="97/298"><span class="index notranslate">#97</span></a>
                <a id="title-2305.00273" class="title-link" href="/arxiv/2305.00273" target="_blank">Sparsity-Aware Optimal Transport for Unsupervised Restoration Learning</a>
                <a id="pdf-2305.00273" class="title-pdf notranslate" onclick="togglePdf('2305.00273', 'https://arxiv.org/pdf/2305.00273', this)">[PDF<sup id="pdf-stars-2305.00273"></sup>]</a>
                <a id="copy-2305.00273" class="title-copy notranslate" onclick="copyToClipboard('2305.00273')">[Copy]</a>
                <a id="kimi-2305.00273" class="title-kimi notranslate" onclick="toggleKimi('2305.00273', this)">[Kimi<sup id="kimi-stars-2305.00273"></sup>]</a>
                <a id="rel-2305.00273" class="title-rel notranslate" onclick="openRelatedPapers('2305.00273')">[REL]</a>
            </h2>
            <p id="authors-2305.00273" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fei Wen" target="_blank"><span class="author notranslate">Fei Wen</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Wang" target="_blank"><span class="author notranslate">Wei Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenxian Yu" target="_blank"><span class="author notranslate">Wenxian Yu</span></a>
            </p>
            <p id="summary-2305.00273" class="summary">Recent studies show that, without any prior model, the unsupervised restoration learning problem can be optimally formulated as an optimal transport (OT) problem, which has shown promising performance on denoising tasks to approach the performance of supervised methods. However, it still significantly lags behind state-of-the-art supervised methods on complex restoration tasks such as super-resolution, deraining, and <mark data-markjs="true">dehazing</mark>. In this paper, we exploit the sparsity of degradation in the OT framework to significantly boost its performance on these tasks. First, we disclose an observation that the degradation in these tasks is quite sparse in the frequency domain, and then propose a sparsity-aware optimal transport (SOT) criterion for unsupervised restoration learning. Further, we provide an analytic example to illustrate that exploiting the sparsity helps to reduce the ambiguity in finding an inverse map for restoration. Experiments on real-world super-resolution, deraining, and <mark data-markjs="true">dehazing</mark> demonstrate that SOT can improve the PSNR of OT by about 2.6 dB, 2.7 dB and 1.3 dB, respectively, while achieving the best perception scores among the compared supervised and unsupervised methods. Particularly, on the three tasks, SOT significantly outperforms existing unsupervised methods and approaches the performance of state-of-the-art supervised methods.</p>
            <p id="subjects-2305.00273" class="metainfo subjects"><strong>Subjects</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
                ; <a href="/arxiv/eess.IV" target="_blank"><span class="subject">Image and Video Processing</span></a>
            </p>
            <p id="date-2305.00273" class="metainfo date"><strong>Publish</strong>: 2023-04-29 15:09:48 UTC</p>
            <div id="pdf-container-2305.00273" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2305.00273" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2305.00273" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2304.13375" class="panel paper" keywords="sglc,dehazing,combinator,global,features,image,streamlined,local,resolution,gfg">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2304.13375" target="_blank" title="98/298"><span class="index notranslate">#98</span></a>
                <a id="title-2304.13375" class="title-link" href="/arxiv/2304.13375" target="_blank">Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image <mark data-markjs="true">Dehazing</mark></a>
                <a id="pdf-2304.13375" class="title-pdf notranslate" onclick="togglePdf('2304.13375', 'https://arxiv.org/pdf/2304.13375', this)">[PDF<sup id="pdf-stars-2304.13375"></sup>]</a>
                <a id="copy-2304.13375" class="title-copy notranslate" onclick="copyToClipboard('2304.13375')">[Copy]</a>
                <a id="kimi-2304.13375" class="title-kimi notranslate" onclick="toggleKimi('2304.13375', this)">[Kimi<sup id="kimi-stars-2304.13375"></sup>]</a>
                <a id="rel-2304.13375" class="title-rel notranslate" onclick="openRelatedPapers('2304.13375')">[REL]</a>
            </h2>
            <p id="authors-2304.13375" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bilel Benjdira" target="_blank"><span class="author notranslate">Bilel Benjdira</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anas M. Ali" target="_blank"><span class="author notranslate">Anas M. Ali</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anis Koubaa" target="_blank"><span class="author notranslate">Anis Koubaa</span></a>
            </p>
            <p id="summary-2304.13375" class="summary">Image <mark data-markjs="true">Dehazing</mark> aims to remove atmospheric fog or haze from an image. Although the <mark data-markjs="true">Dehazing</mark> models have evolved a lot in recent years, few have precisely tackled the problem of High-Resolution hazy images. For this kind of image, the model needs to work on a downscaled version of the image or on cropped patches from it. In both cases, the accuracy will drop. This is primarily due to the inherent failure to combine global and local features when the image size increases. The <mark data-markjs="true">Dehazing</mark> model requires global features to understand the general scene peculiarities and the local features to work better with fine and pixel details. In this study, we propose the Streamlined Global and Local Features Combinator (SGLC) to solve these issues and to optimize the application of any <mark data-markjs="true">Dehazing</mark> model to High-Resolution images. The SGLC contains two successive blocks. The first is the Global Features Generator (GFG) which generates the first version of the Dehazed image containing strong global features. The second block is the Local Features Enhancer (LFE) which improves the local feature details inside the previously generated image. When tested on the Uformer architecture for <mark data-markjs="true">Dehazing</mark>, SGLC increased the PSNR metric by a significant margin. Any other model can be incorporated inside the SGLC process to improve its efficiency on High-Resolution input data.</p>
            <p id="subjects-2304.13375" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2304.13375" class="metainfo date"><strong>Publish</strong>: 2023-04-26 08:34:00 UTC</p>
            <div id="pdf-container-2304.13375" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2304.13375" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2304.13375" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2304.11448" class="panel paper" keywords="dehazing,nerf,hazy,atmospheric,image,radiance,scattering,synthesis,consistency,images">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2304.11448" target="_blank" title="99/298"><span class="index notranslate">#99</span></a>
                <a id="title-2304.11448" class="title-link" href="/arxiv/2304.11448" target="_blank"><mark data-markjs="true">Dehazing</mark>-NeRF: Neural Radiance Fields from Hazy Images</a>
                <a id="pdf-2304.11448" class="title-pdf notranslate" onclick="togglePdf('2304.11448', 'https://arxiv.org/pdf/2304.11448', this)">[PDF<sup id="pdf-stars-2304.11448"></sup>]</a>
                <a id="copy-2304.11448" class="title-copy notranslate" onclick="copyToClipboard('2304.11448')">[Copy]</a>
                <a id="kimi-2304.11448" class="title-kimi notranslate" onclick="toggleKimi('2304.11448', this)">[Kimi<sup id="kimi-stars-2304.11448"></sup>]</a>
                <a id="rel-2304.11448" class="title-rel notranslate" onclick="openRelatedPapers('2304.11448')">[REL]</a>
            </h2>
            <p id="authors-2304.11448" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tian Li" target="_blank"><span class="author notranslate">Tian Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=LU Li" target="_blank"><span class="author notranslate">LU Li</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei Wang" target="_blank"><span class="author notranslate">Wei Wang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhangchi Feng" target="_blank"><span class="author notranslate">Zhangchi Feng</span></a>
            </p>
            <p id="summary-2304.11448" class="summary">Neural Radiance Field (NeRF) has received much attention in recent years due to the impressively high quality in 3D scene reconstruction and novel view synthesis. However, image degradation caused by the scattering of atmospheric light and object light by particles in the atmosphere can significantly decrease the reconstruction quality when shooting scenes in hazy conditions. To address this issue, we propose <mark data-markjs="true">Dehazing</mark>-NeRF, a method that can recover clear NeRF from hazy image inputs. Our method simulates the physical imaging process of hazy images using an atmospheric scattering model, and jointly learns the atmospheric scattering model and a clean NeRF model for both image <mark data-markjs="true">dehazing</mark> and novel view synthesis. Different from previous approaches, <mark data-markjs="true">Dehazing</mark>-NeRF is an unsupervised method with only hazy images as the input, and also does not rely on hand-designed <mark data-markjs="true">dehazing</mark> priors. By jointly combining the depth estimated from the NeRF 3D scene with the atmospheric scattering model, our proposed model breaks through the ill-posed problem of single-image <mark data-markjs="true">dehazing</mark> while maintaining geometric consistency. Besides, to alleviate the degradation of image quality caused by information loss, soft margin consistency regularization, as well as atmospheric consistency and contrast discriminative loss, are addressed during the model training process. Extensive experiments demonstrate that our method outperforms the simple combination of single-image <mark data-markjs="true">dehazing</mark> and NeRF on both image <mark data-markjs="true">dehazing</mark> and novel view image synthesis.</p>
            <p id="subjects-2304.11448" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2304.11448" class="metainfo date"><strong>Publish</strong>: 2023-04-22 17:09:05 UTC</p>
            <div id="pdf-container-2304.11448" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2304.11448" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2304.11448" class="kimi-container notranslate" style="display:none"></div>
        </div>
        <div id="2304.09588" class="panel paper" keywords="dadfnet,dual,dehazing,transportation,attention,frequency,haze,guided,intelligent,hlfn">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2304.09588" target="_blank" title="100/298"><span class="index notranslate">#100</span></a>
                <a id="title-2304.09588" class="title-link" href="/arxiv/2304.09588" target="_blank">DADFNet: Dual Attention and Dual Frequency-Guided <mark data-markjs="true">Dehazing</mark> Network for Video-Empowered Intelligent Transportation</a>
                <a id="pdf-2304.09588" class="title-pdf notranslate" onclick="togglePdf('2304.09588', 'https://arxiv.org/pdf/2304.09588', this)">[PDF<sup id="pdf-stars-2304.09588"></sup>]</a>
                <a id="copy-2304.09588" class="title-copy notranslate" onclick="copyToClipboard('2304.09588')">[Copy]</a>
                <a id="kimi-2304.09588" class="title-kimi notranslate" onclick="toggleKimi('2304.09588', this)">[Kimi<sup id="kimi-stars-2304.09588"></sup>]</a>
                <a id="rel-2304.09588" class="title-rel notranslate" onclick="openRelatedPapers('2304.09588')">[REL]</a>
            </h2>
            <p id="authors-2304.09588" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu Guo" target="_blank"><span class="author notranslate">Yu Guo</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ryan Wen Liu" target="_blank"><span class="author notranslate">Ryan Wen Liu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiangtian Nie" target="_blank"><span class="author notranslate">Jiangtian Nie</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lingjuan Lyu" target="_blank"><span class="author notranslate">Lingjuan Lyu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zehui Xiong" target="_blank"><span class="author notranslate">Zehui Xiong</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiawen Kang" target="_blank"><span class="author notranslate">Jiawen Kang</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Han Yu" target="_blank"><span class="author notranslate">Han Yu</span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dusit Niyato" target="_blank"><span class="author notranslate">Dusit Niyato</span></a>
            </p>
            <p id="summary-2304.09588" class="summary">Visual surveillance technology is an indispensable functional component of advanced traffic management systems. It has been applied to perform traffic supervision tasks, such as object detection, tracking and recognition. However, adverse weather conditions, e.g., fog, haze and mist, pose severe challenges for video-based transportation surveillance. To eliminate the influences of adverse weather conditions, we propose a dual attention and dual frequency-guided <mark data-markjs="true">dehazing</mark> network (termed DADFNet) for real-time visibility enhancement. It consists of a dual attention module (DAM) and a high-low frequency-guided sub-net (HLFN) to jointly consider the attention and frequency mapping to guide haze-free scene reconstruction. Extensive experiments on both synthetic and real-world images demonstrate the superiority of DADFNet over state-of-the-art methods in terms of visibility enhancement and improvement in detection accuracy. Furthermore, DADFNet only takes <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6.3</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-29" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.302em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.365em, 1001.26em, 2.366em, -1000em); top: -2.188em; left: 0em;"><span class="mrow" id="MathJax-Span-30"><span class="mn" id="MathJax-Span-31" style="font-family: MathJax_Main;">6.3</span></span><span style="display: inline-block; width: 0px; height: 2.188em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.089em; border-left: 0px solid; width: 0px; height: 0.951em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6.3</mn></math></span></span><script type="math/tex" id="MathJax-Element-7">6.3</script> ms to process a 1,920 * 1,080 image on the 2080 Ti GPU, making it highly efficient for deployment in intelligent transportation systems.</p>
            <p id="subjects-2304.09588" class="metainfo subjects"><strong>Subject</strong>:
                <a href="/arxiv/cs.CV" target="_blank"><span class="subject"><strong>Computer Vision and Pattern Recognition</strong></span></a>
            </p>
            <p id="date-2304.09588" class="metainfo date"><strong>Publish</strong>: 2023-04-19 11:55:30 UTC</p>
            <div id="pdf-container-2304.09588" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2304.09588" class="pdf-iframe" width="100%" height="800px"></iframe>
            </div>
            <div id="kimi-container-2304.09588" class="kimi-container notranslate" style="display:none"></div>
        </div>
    </div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include:</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-2410.16095" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#2410.16095">LMHaze: Intensity-aware Image Dehazing with a Large-scale Multi-intensity Real Haze Dataset</a>
                    <a class="i-star" onclick="toggleAppStar('2410.16095')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.16095')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2410.14595" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#2410.14595">DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail Recovery and a Novel Contrastive Learning Paradigm</a>
                    <a class="i-star" onclick="toggleAppStar('2410.14595')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.14595')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2410.10121" style="display: block;">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#2410.10121">Interaction-Guided Two-Branch Image Dehazing Network</a>
                    <a class="i-star" onclick="toggleAppStar('2410.10121')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.10121')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2410.04762" style="display: block;">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#2410.04762">WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform and Contrastive Learning</a>
                    <a class="i-star" onclick="toggleAppStar('2410.04762')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.04762')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2410.01395" style="display: block;">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#2410.01395">Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots</a>
                    <a class="i-star" onclick="toggleAppStar('2410.01395')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.01395')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2410.01225" style="display: block;">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#2410.01225">Perceptual Piercing: Human Visual Cue-based Object Detection in Low Visibility Conditions</a>
                    <a class="i-star" onclick="toggleAppStar('2410.01225')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2410.01225')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2409.17432" style="display: block;">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#2409.17432">HazeSpace2M: A Dataset for Haze Aware Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2409.17432')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2409.17432')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2409.10353" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#2409.10353">Taming Diffusion Models for Image Restoration: A Review</a>
                    <a class="i-star" onclick="toggleAppStar('2409.10353')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2409.10353')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2409.09779" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#2409.09779">Underwater Image Enhancement via Dehazing and Color Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2409.09779')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2409.09779')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2409.08510" style="display: block;">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#2409.08510">CasDyF-Net: Image Dehazing via Cascaded Dynamic Filters</a>
                    <a class="i-star" onclick="toggleAppStar('2409.08510')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2409.08510')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2409.04812" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#2409.04812">Power Line Aerial Image Restoration under dverse Weather: Datasets and Baselines</a>
                    <a class="i-star" onclick="toggleAppStar('2409.04812')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2409.04812')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2408.12317" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#2408.12317">Adapt CLIP as Aggregation Instructor for Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2408.12317')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2408.12317')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2408.10145" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#2408.10145">Multi-Scale Representation Learning for Image Restoration with State-Space Model</a>
                    <a class="i-star" onclick="toggleAppStar('2408.10145')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2408.10145')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2408.08149" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#2408.08149">Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('2408.08149')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2408.08149')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2408.05683" style="display: block;">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#2408.05683">Single Image Dehazing Using Scene Depth Ordering</a>
                    <a class="i-star" onclick="toggleAppStar('2408.05683')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2408.05683')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.14868" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#2407.14868">Dual High-Order Total Variation Model for Underwater Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2407.14868')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.14868')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.14823" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#2407.14823">CrossDehaze: Scaling Up Image Dehazing with Cross-Data Vision Alignment and Augmentation</a>
                    <a class="i-star" onclick="toggleAppStar('2407.14823')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.14823')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.13719" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#2407.13719">HazeCLIP: Towards Language Guided Real-World Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2407.13719')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.13719')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.11505" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#2407.11505">Haze-Aware Attention Network for Single-Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2407.11505')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.11505')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.10226" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#2407.10226">Addressing Domain Discrepancy: A Dual-branch Collaborative Model to Unsupervised Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2407.10226')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.10226')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.09768" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#2407.09768">Prototype Clustered Diffusion Models for Versatile Inverse Problems</a>
                    <a class="i-star" onclick="toggleAppStar('2407.09768')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.09768')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.08221" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#2407.08221">GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views</a>
                    <a class="i-star" onclick="toggleAppStar('2407.08221')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.08221')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.00972" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#2407.00972">FALCON: Frequency Adjoint Link with CONtinuous Density Mask for Fast Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2407.00972')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.00972')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.01636" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#2407.01636">Learning Frequency-Aware Dynamic Transformers for All-In-One Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2407.01636')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.01636')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.00676" style="display: block;">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#2407.00676">Instruct-IPT: All-in-One Image Processing Transformer via Weight Modulation</a>
                    <a class="i-star" onclick="toggleAppStar('2407.00676')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.00676')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2406.19703" style="display: block;">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#2406.19703">Vision Transformer with Key-select Routing Attention for Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2406.19703')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2406.19703')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2406.09627" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#2406.09627">RobustSAM: Segment Anything Robustly on Degraded Images</a>
                    <a class="i-star" onclick="toggleAppStar('2406.09627')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2406.09627')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2406.07966" style="display: block;">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#2406.07966">Real-world Image Dehazing with Coherence-based Label Generator and Cooperative Unfolding Network</a>
                    <a class="i-star" onclick="toggleAppStar('2406.07966')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2406.07966')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2406.05700" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#2406.05700">HDMba: Hyperspectral Remote Sensing Imagery Dehazing with State Space Model</a>
                    <a class="i-star" onclick="toggleAppStar('2406.05700')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2406.05700')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2406.00629" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#2406.00629">Correlation Matching Transformation Transformers for UHD Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2406.00629')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2406.00629')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2407.05169" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#2407.05169">DehazeDCT: Towards Effective Non-Homogeneous Dehazing via Deformable Convolutional Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('2407.05169')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2407.05169')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.15817" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#2405.15817">Rethinking the Elementary Function Fusion for Single-Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2405.15817')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.15817')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.12265" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#2405.12265">SEL-CIE: Knowledge-Guided Self-Supervised Learning Framework for CIE-XYZ Reconstruction from Non-Linear sRGB Images</a>
                    <a class="i-star" onclick="toggleAppStar('2405.12265')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.12265')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.10030" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#2405.10030">RSDehamba: Lightweight Vision Mamba for Remote Sensing Satellite Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2405.10030')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.10030')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.09996" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#2405.09996">Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance</a>
                    <a class="i-star" onclick="toggleAppStar('2405.09996')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.09996')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.09083" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#2405.09083">RSHazeDiff: A Unified Fourier-aware Diffusion Model for Remote Sensing Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2405.09083')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.09083')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.07520" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#2405.07520">Dehazing Remote Sensing and UAV Imagery: A Review of Deep Learning, Prior-based, and Hybrid Approaches</a>
                    <a class="i-star" onclick="toggleAppStar('2405.07520')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.07520')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2405.05811" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#2405.05811">Parallel Cross Strip Attention Network for Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2405.05811')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2405.05811')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.17825" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#2404.17825">ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2404.17825')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.17825')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.15638" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#2404.15638">PriorNet: A Novel Lightweight Network with Multidimensional Interactive Attention for Efficient Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2404.15638')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.15638')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.09269" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#2404.09269">PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing</a>
                    <a class="i-star" onclick="toggleAppStar('2404.09269')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.09269')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.07790" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#2404.07790">VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2404.07790')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.07790')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.02460" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#2404.02460">TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and Adaptive Learning</a>
                    <a class="i-star" onclick="toggleAppStar('2404.02460')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.02460')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.02154" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#2404.02154">Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2404.02154')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.02154')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.01998" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#2404.01998">Specularity Factorization for Low-Light Enhancement</a>
                    <a class="i-star" onclick="toggleAppStar('2404.01998')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.01998')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.01604" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#2404.01604">WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2404.01604')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.01604')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.00288" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#2404.00288">Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2404.00288')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2404.00288')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.18548" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#2403.18548">A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint</a>
                    <a class="i-star" onclick="toggleAppStar('2403.18548')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.18548')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.14614" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#2403.14614">AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation</a>
                    <a class="i-star" onclick="toggleAppStar('2403.14614')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.14614')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.09233" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#2403.09233">D-YOLO a robust framework for object detection in adverse weather conditions</a>
                    <a class="i-star" onclick="toggleAppStar('2403.09233')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.09233')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.07408" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#2403.07408">NightHaze: Nighttime Image Dehazing via Self-Prior Learning</a>
                    <a class="i-star" onclick="toggleAppStar('2403.07408')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.07408')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.04443" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#2403.04443">FriendNet: Detection-Friendly Dehazing Network</a>
                    <a class="i-star" onclick="toggleAppStar('2403.04443')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.04443')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.01105" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#2403.01105">Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2403.01105')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.01105')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.18181" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#2402.18181">CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation</a>
                    <a class="i-star" onclick="toggleAppStar('2402.18181')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.18181')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.18134" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#2402.18134">Learning to Deblur Polarized Images</a>
                    <a class="i-star" onclick="toggleAppStar('2402.18134')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.18134')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.15784" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#2402.15784">IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer</a>
                    <a class="i-star" onclick="toggleAppStar('2402.15784')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.15784')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.05281" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#2402.05281">Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning</a>
                    <a class="i-star" onclick="toggleAppStar('2402.05281')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.05281')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.04139" style="display: block;">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#2402.04139">U-shaped Vision Mamba for Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2402.04139')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.04139')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2402.01368" style="display: block;">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#2402.01368">LIR: A Lightweight Baseline for Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2402.01368')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2402.01368')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2401.16468" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#2401.16468">InstructIR: High-Quality Image Restoration Following Human Instructions</a>
                    <a class="i-star" onclick="toggleAppStar('2401.16468')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2401.16468')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2401.07213" style="display: block;">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#2401.07213">Depth-agnostic Single Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2401.07213')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2401.07213')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2403.12054" style="display: block;">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#2403.12054">Haze Removal via Regional Saturation-Value Translation and Soft Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('2403.12054')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2403.12054')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.17334" style="display: block;">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#2312.17334">Improving Image Restoration through Removing Degradations in Textual Representations</a>
                    <a class="i-star" onclick="toggleAppStar('2312.17334')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.17334')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.09955" style="display: block;">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#2312.09955">DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2312.09955')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.09955')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.07849" style="display: block;">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#2312.07849">Encoder-minimal and Decoder-minimal Framework for Remote Sensing Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2312.07849')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.07849')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.06850" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#2312.06850">NDELS: A Novel Approach for Nighttime Dehazing, Low-Light Enhancement, and Light Suppression</a>
                    <a class="i-star" onclick="toggleAppStar('2312.06850')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.06850')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.06162" style="display: block;">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#2312.06162">Textual Prompt Guided Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2312.06162')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.06162')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2312.05038" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#2312.05038">Prompt-In-Prompt Learning for Universal Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2312.05038')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2312.05038')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2309.17389" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#2309.17389">Prompt-based test-time real image dehazing: a novel pipeline</a>
                    <a class="i-star" onclick="toggleAppStar('2309.17389')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2309.17389')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2309.16494" style="display: block;">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#2309.16494">Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization</a>
                    <a class="i-star" onclick="toggleAppStar('2309.16494')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2309.16494')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2309.06023" style="display: block;">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#2309.06023">Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2309.06023')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2309.06023')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2309.00514" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#2309.00514">A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</a>
                    <a class="i-star" onclick="toggleAppStar('2309.00514')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2309.00514')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.14036" style="display: block;">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#2308.14036">MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2308.14036')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.14036')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.11949" style="display: block;">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#2308.11949">High-quality Image Dehazing with Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('2308.11949')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.11949')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.10510" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#2308.10510">Frequency Compensated Diffusion Model for Real-scene Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2308.10510')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.10510')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.06998" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#2308.06998">Mutual Information-driven Triple Interaction Network for Efficient Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2308.06998')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.06998')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.01738" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#2308.01738">Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution</a>
                    <a class="i-star" onclick="toggleAppStar('2308.01738')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.01738')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2308.00591" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#2308.00591">Visibility Enhancement for Low-light Hazy Scenarios</a>
                    <a class="i-star" onclick="toggleAppStar('2308.00591')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2308.00591')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.16050" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#2307.16050">A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods</a>
                    <a class="i-star" onclick="toggleAppStar('2307.16050')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.16050')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.13927" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#2307.13927">DFR-Net: Density Feature Refinement Network for Image Dehazing Utilizing Haze Density Difference</a>
                    <a class="i-star" onclick="toggleAppStar('2307.13927')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.13927')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.11204" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#2307.11204">Dehazing Ultrasound using Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('2307.11204')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.11204')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.05075" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#2307.05075">Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images</a>
                    <a class="i-star" onclick="toggleAppStar('2307.05075')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.05075')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.04149" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#2307.04149">Latent Graph Attention for Enhanced Spatial Context</a>
                    <a class="i-star" onclick="toggleAppStar('2307.04149')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.04149')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2307.04100" style="display: block;">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#2307.04100">Visible and infrared self-supervised fusion trained on a single example</a>
                    <a class="i-star" onclick="toggleAppStar('2307.04100')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2307.04100')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2306.15870" style="display: block;">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#2306.15870">Let Segment Anything Help Image Dehaze</a>
                    <a class="i-star" onclick="toggleAppStar('2306.15870')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2306.15870')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2306.13090" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#2306.13090">PromptIR: Prompting for All-in-One Blind Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2306.13090')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2306.13090')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2306.07244" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#2306.07244">RB-Dust -- A Reference-based Dataset for Vision-based Dust Removal</a>
                    <a class="i-star" onclick="toggleAppStar('2306.07244')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2306.07244')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2306.06288" style="display: block;">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#2306.06288">SAGE-NDVI: A Stereotype-Breaking Evaluation Metric for Remote Sensing Image Dehazing Using Satellite-to-Ground NDVI Knowledge</a>
                    <a class="i-star" onclick="toggleAppStar('2306.06288')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2306.06288')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2306.05675" style="display: block;">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#2306.05675">Illumination Controllable Dehazing Network based on Unsupervised Retinex Embedding</a>
                    <a class="i-star" onclick="toggleAppStar('2306.05675')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2306.05675')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.17863" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#2305.17863">GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions</a>
                    <a class="i-star" onclick="toggleAppStar('2305.17863')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.17863')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.17654" style="display: block;">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#2305.17654">MixDehazeNet : Mix Structure Block For Image Dehazing Network</a>
                    <a class="i-star" onclick="toggleAppStar('2305.17654')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.17654')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.16481" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#2305.16481">SimHaze: game engine simulated data for real-world dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2305.16481')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.16481')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.13819" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#2305.13819">WaveDM: Wavelet-Based Diffusion Models for Image Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('2305.13819')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.13819')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.09533" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#2305.09533">NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('2305.09533')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.09533')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.04430" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#2305.04430">Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method based on Fast Fourier Convolution and ConvNeXt</a>
                    <a class="i-star" onclick="toggleAppStar('2305.04430')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.04430')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.02103" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#2305.02103">ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering</a>
                    <a class="i-star" onclick="toggleAppStar('2305.02103')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.02103')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2305.00273" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#2305.00273">Sparsity-Aware Optimal Transport for Unsupervised Restoration Learning</a>
                    <a class="i-star" onclick="toggleAppStar('2305.00273')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2305.00273')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2304.13375" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#2304.13375">Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing</a>
                    <a class="i-star" onclick="toggleAppStar('2304.13375')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2304.13375')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2304.11448" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#2304.11448">Dehazing-NeRF: Neural Radiance Fields from Hazy Images</a>
                    <a class="i-star" onclick="toggleAppStar('2304.11448')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2304.11448')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2304.09588" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#2304.09588">DADFNet: Dual Attention and Dual Frequency-Guided Dehazing Network for Video-Empowered Intelligent Transportation</a>
                    <a class="i-star" onclick="toggleAppStar('2304.09588')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2304.09588')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            </div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/marked.min.js"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.4.9.1"></script>
    <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"]},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Main, sans-serif;"></div></div></body></html>